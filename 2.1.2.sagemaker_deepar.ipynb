{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cee7eaea",
   "metadata": {},
   "source": [
    "## 使用 DeepAR 进行股价预测\n",
    "\n",
    "像之前一样，我们将使用白酒行业的股票，将这些系列的收盘价作为将要预测的时间序列建立模型。但是，这里的区别在于，我们不必指定哪个是主序列，哪些是外生序列。DeepAR 算法致力于建立一个统一的模型，该模型将考虑所有提供的时间序列中的趋势，并可以针对任何这些趋势生成预测。这将避免需要针对不同的时间序列构建不同的模型。\n",
    "\n",
    "正如在自定义 RNN 示例中使用协变量时间序列一样，我们可以通过使用 Dynamic Feature 在 DeepAR 上起到类似的效果。DeepAR 还可以通过整数定义不同股票的类型。在股价预测的场景中，您可以将不同行业的股票都一同进行训练。假设这些行业板块之间确实存在某种可靠的正相关/负相关的关联性，DeepAR 就可以学习和捕捉这些关系，这往往可以提升模型的准确性。\n",
    "\n",
    "开始之前，清选择 conda_python3 内核。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1c9566b",
   "metadata": {},
   "source": [
    "### 定义参数\n",
    "\n",
    "在之前的自定义 RNN 示例中，我们实现了训练代码，因此可以使用超参数来自定义训练的某些方面。对于DeepAR，就像任何AWS提供的算法一样，您可以使用类似的hyparparameters配置来充分利用模型。\n",
    "\n",
    "DeepAR 允许您控制神经网络体系结构，例如网络的层数、每层中的循环单元数层、学习率，预测长度等。需要注意的是，更高的预测长度会消耗相当多的内存资源，因此建议您适当对超参进行选择。在以下的示例中，我们配置了一些超参，以便在合理的时间内完成训练。你可能注意到有些超参和之前的自定义 RNN 模型超参略有不同。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9e09201e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "aws_account_id = boto3.client('sts').get_caller_identity().get('Account')\n",
    "my_name = 'peng'\n",
    "aws_region = 'us-east-1'\n",
    "bucket = 'algo-trading-workshop-{}'.format(my_name)\n",
    "repository_name = 'sagemaker-deepar'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "17aee6ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics: ['open', 'low', 'high', 'close']\n"
     ]
    }
   ],
   "source": [
    "target_stock = \"600519\" #  贵州茅台\n",
    "covariate_stocks = \"000596,000568,000858,600779,002304\" # 古井贡酒、泸州老窖、五粮液、水井坊、洋河股份\n",
    "target_column = \"close\" # 需要预测的目标值\n",
    "covariate_columns = \"open,low,high\"\n",
    "covariates = covariate_stocks.split(',')\n",
    "covariate_columns = covariate_columns.split(',')\n",
    "\n",
    "interval = 'D' # 基于日线进行预测\n",
    "prediction_length = 91  \n",
    "context_length = 91\n",
    "train_test_split = 0.8\n",
    "num_test_windows = 4\n",
    "\n",
    "hyperparameters = {\n",
    "    \"prediction_length\": str(prediction_length), # number of time-steps model is trained to predict, always generates forecasts with this length\n",
    "    \"context_length\": str(context_length), # number of time-points that the model gets to see before making the prediction, should be about same as the prediction_length\n",
    "    \"time_freq\": interval, # granularity of the time series in the dataset\n",
    "    \"epochs\": \"200\", # maximum number of passes over the training data\n",
    "    \"early_stopping_patience\": \"40\", #training stops when no progress is made within the specified number of epochs\n",
    "    \"num_layers\": \"2\", #number of hidden layers in the RNN, typically range from 1 to 4    \n",
    "    \"num_cells\": \"40\", #number of cells to use in each hidden layer of the RNN, typically range from 30 to 100\n",
    "    \"mini_batch_size\": \"128\", #size of mini-batches used during training, typically values range from 32 to 512\n",
    "    \"learning_rate\": \"1e-3\", #learning rate used in training. Typical values range from 1e-4 to 1e-1\n",
    "    \"dropout_rate\": \"0.1\", # dropout rate to use for regularization, typically less than 0.2. \n",
    "    \"likelihood\": \"gaussian\" # noise model used for uncertainty estimates - gaussian/beta/negative-binomial/student-T/deterministic-L1\n",
    "}\n",
    "\n",
    "metrics=[]\n",
    "metrics.extend(covariate_columns)\n",
    "metrics.append(target_column)\n",
    "print('Metrics:', metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08617f20",
   "metadata": {},
   "source": [
    "### 获取数据\n",
    "\n",
    "与自定义 RNN 模型类似，我们先调取 1000 个交易日的数据进行训练："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "529d4d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install awswrangler\n",
    "\n",
    "import awswrangler as wr\n",
    "\n",
    "s3_output = wr.athena.create_athena_bucket()\n",
    "\n",
    "def execute_query(database, sql):\n",
    "    \n",
    "    query_execution_id = wr.athena.start_query_execution(database=database, sql=sql)\n",
    "    response = wr.athena.get_query_execution(query_execution_id=query_execution_id)\n",
    "    wr.athena.wait_query(query_execution_id=query_execution_id)\n",
    "    OutputLocation = response['ResultConfiguration']['OutputLocation']\n",
    "    \n",
    "    return OutputLocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "13c103a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "database = 'stock-data'\n",
    "table = 'stock_day'\n",
    "fields = '*'\n",
    "ticker = target_stock\n",
    "orderby = 'tradedate'\n",
    "sort = 'DESC'\n",
    "limit = '1000'\n",
    "\n",
    "sql = f'''\n",
    "SELECT {fields}\n",
    "FROM \"{database}\".\"{table}\"\n",
    "WHERE ticker='{ticker}'\n",
    "ORDER BY {orderby}\n",
    "{sort}\n",
    "LIMIT {limit}\n",
    "'''\n",
    "\n",
    "output_location = execute_query(database, sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b58458eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ticker</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tradedate</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2017-05-26</th>\n",
       "      <td>600519</td>\n",
       "      <td>450.82</td>\n",
       "      <td>455.60</td>\n",
       "      <td>448.00</td>\n",
       "      <td>451.92</td>\n",
       "      <td>2738664.0</td>\n",
       "      <td>1.237323e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-05-31</th>\n",
       "      <td>600519</td>\n",
       "      <td>450.00</td>\n",
       "      <td>450.50</td>\n",
       "      <td>440.11</td>\n",
       "      <td>442.94</td>\n",
       "      <td>4431225.0</td>\n",
       "      <td>1.968887e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-06-01</th>\n",
       "      <td>600519</td>\n",
       "      <td>442.50</td>\n",
       "      <td>449.95</td>\n",
       "      <td>441.01</td>\n",
       "      <td>449.28</td>\n",
       "      <td>4060478.0</td>\n",
       "      <td>1.808306e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-06-02</th>\n",
       "      <td>600519</td>\n",
       "      <td>450.00</td>\n",
       "      <td>450.95</td>\n",
       "      <td>445.60</td>\n",
       "      <td>447.31</td>\n",
       "      <td>2178526.0</td>\n",
       "      <td>9.748933e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-06-05</th>\n",
       "      <td>600519</td>\n",
       "      <td>448.04</td>\n",
       "      <td>449.00</td>\n",
       "      <td>442.35</td>\n",
       "      <td>444.41</td>\n",
       "      <td>1924120.0</td>\n",
       "      <td>8.548769e+08</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            ticker    open    high     low   close     volume         value\n",
       "tradedate                                                                  \n",
       "2017-05-26  600519  450.82  455.60  448.00  451.92  2738664.0  1.237323e+09\n",
       "2017-05-31  600519  450.00  450.50  440.11  442.94  4431225.0  1.968887e+09\n",
       "2017-06-01  600519  442.50  449.95  441.01  449.28  4060478.0  1.808306e+09\n",
       "2017-06-02  600519  450.00  450.95  445.60  447.31  2178526.0  9.748933e+08\n",
       "2017-06-05  600519  448.04  449.00  442.35  444.41  1924120.0  8.548769e+08"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = wr.s3.read_csv(path=[output_location])\n",
    "\n",
    "df['ticker'] = df['ticker'].apply(lambda x: str(x))\n",
    "df['ticker'] = df['ticker'].apply(lambda x: '0'*(6-len(x)) + x)\n",
    "df['openprice'] = df['openprice'] * df['accumadjfactor'] / df['accumadjfactor'].iloc[-1]\n",
    "df['closeprice'] = df['closeprice'] * df['accumadjfactor'] / df['accumadjfactor'].iloc[-1]\n",
    "df['highestprice'] = df['highestprice'] * df['accumadjfactor'] / df['accumadjfactor'].iloc[-1]\n",
    "df['lowestprice'] = df['lowestprice'] * df['accumadjfactor'] / df['accumadjfactor'].iloc[-1]\n",
    "df = df[df['isopen'] == True]\n",
    "df.drop('isopen', 1, inplace=True)\n",
    "df.drop('accumadjfactor', 1, inplace=True)\n",
    "df.drop('secid', 1, inplace=True)\n",
    "df.set_index('tradedate', inplace=True)\n",
    "df.sort_index(0, inplace=True)\n",
    "\n",
    "df.rename(columns={'openprice': 'open'}, inplace=True)\n",
    "df.rename(columns={'closeprice': 'close'}, inplace=True)\n",
    "df.rename(columns={'highestprice': 'high'}, inplace=True)\n",
    "df.rename(columns={'lowestprice': 'low'}, inplace=True)\n",
    "df.rename(columns={'turnovervol': 'volume'}, inplace=True)\n",
    "df.rename(columns={'turnovervalue': 'value'}, inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "599d5cf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target stock: 600519 2017-05-26 - 2021-07-05\n"
     ]
    }
   ],
   "source": [
    "start_date = df.index[0]\n",
    "end_date = df.index[-1]\n",
    "print('Target stock:', ticker, start_date, '-', end_date)\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "stock_data = pd.DataFrame([])\n",
    "stock_data = pd.concat([stock_data, df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b4421ef3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "000596 (999, 7) 2017-05-26 2021-07-05\n",
      "000568 (999, 7) 2017-05-26 2021-07-05\n",
      "000858 (1000, 7) 2017-05-26 2021-07-05\n",
      "600779 (997, 7) 2017-05-26 2021-07-05\n",
      "002304 (999, 7) 2017-05-26 2021-07-05\n"
     ]
    }
   ],
   "source": [
    "covariates = covariate_stocks.split(',')\n",
    "for ticker in covariates:\n",
    "    \n",
    "    sql = f'''\n",
    "    SELECT {fields}\n",
    "    FROM \"{database}\".\"{table}\"\n",
    "    WHERE ticker='{ticker}'\n",
    "    AND tradedate>='{start_date}'\n",
    "    ORDER BY {orderby}\n",
    "    {sort}\n",
    "    '''\n",
    "    \n",
    "    output_location = execute_query(database, sql)\n",
    "    df = wr.s3.read_csv(path=[output_location])\n",
    "    \n",
    "    df['ticker'] = df['ticker'].apply(lambda x: str(x))\n",
    "    df['ticker'] = df['ticker'].apply(lambda x: '0'*(6-len(x)) + x)\n",
    "    df['openprice'] = df['openprice'] * df['accumadjfactor'] / df['accumadjfactor'].iloc[-1]\n",
    "    df['closeprice'] = df['closeprice'] * df['accumadjfactor'] / df['accumadjfactor'].iloc[-1]\n",
    "    df['highestprice'] = df['highestprice'] * df['accumadjfactor'] / df['accumadjfactor'].iloc[-1]\n",
    "    df['lowestprice'] = df['lowestprice'] * df['accumadjfactor'] / df['accumadjfactor'].iloc[-1]\n",
    "    df = df[df['isopen'] == True]\n",
    "    df.drop('isopen', 1, inplace=True)\n",
    "    df.drop('accumadjfactor', 1, inplace=True)\n",
    "    df.drop('secid', 1, inplace=True)\n",
    "    df.set_index('tradedate', inplace=True)\n",
    "    df.sort_index(0, inplace=True)\n",
    "\n",
    "    df.rename(columns={'openprice': 'open'}, inplace=True)\n",
    "    df.rename(columns={'closeprice': 'close'}, inplace=True)\n",
    "    df.rename(columns={'highestprice': 'high'}, inplace=True)\n",
    "    df.rename(columns={'lowestprice': 'low'}, inplace=True)\n",
    "    df.rename(columns={'turnovervol': 'volume'}, inplace=True)\n",
    "    df.rename(columns={'turnovervalue': 'value'}, inplace=True)\n",
    "    \n",
    "    print(ticker, df.shape, df.index[0], df.index[-1])\n",
    "    \n",
    "    stock_data = pd.concat([stock_data, df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "de4920e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_data(data, target_column, covariate_columns):\n",
    "    \n",
    "    normalized_data = pd.DataFrame([])\n",
    "    for ticker in data['ticker'].unique():\n",
    "        df = data[data['ticker'] == ticker]\n",
    "        df.drop(['ticker'], 1, inplace=True)\n",
    "\n",
    "        for col in df.columns:\n",
    "            if (col != target_column) and (col not in covariate_columns):\n",
    "                df.drop(col, 1, inplace=True)\n",
    "            else:\n",
    "                df.rename(columns={col: ticker+'-'+col}, inplace=True)\n",
    "\n",
    "        normalized_data = normalized_data.combine_first(df)\n",
    "\n",
    "    normalized_data.fillna(method='ffill', inplace=True)\n",
    "    \n",
    "    return normalized_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b829ef36",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/pandas/core/frame.py:4174: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  errors=errors,\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/pandas/core/frame.py:4308: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  errors=errors,\n"
     ]
    }
   ],
   "source": [
    "df = normalize_data(stock_data, target_column, covariate_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6ab98559",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ticker</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tradedate</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2017-05-26</th>\n",
       "      <td>600519</td>\n",
       "      <td>450.82</td>\n",
       "      <td>455.60</td>\n",
       "      <td>448.00</td>\n",
       "      <td>451.92</td>\n",
       "      <td>2738664.0</td>\n",
       "      <td>1.237323e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-05-31</th>\n",
       "      <td>600519</td>\n",
       "      <td>450.00</td>\n",
       "      <td>450.50</td>\n",
       "      <td>440.11</td>\n",
       "      <td>442.94</td>\n",
       "      <td>4431225.0</td>\n",
       "      <td>1.968887e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-06-01</th>\n",
       "      <td>600519</td>\n",
       "      <td>442.50</td>\n",
       "      <td>449.95</td>\n",
       "      <td>441.01</td>\n",
       "      <td>449.28</td>\n",
       "      <td>4060478.0</td>\n",
       "      <td>1.808306e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-06-02</th>\n",
       "      <td>600519</td>\n",
       "      <td>450.00</td>\n",
       "      <td>450.95</td>\n",
       "      <td>445.60</td>\n",
       "      <td>447.31</td>\n",
       "      <td>2178526.0</td>\n",
       "      <td>9.748933e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-06-05</th>\n",
       "      <td>600519</td>\n",
       "      <td>448.04</td>\n",
       "      <td>449.00</td>\n",
       "      <td>442.35</td>\n",
       "      <td>444.41</td>\n",
       "      <td>1924120.0</td>\n",
       "      <td>8.548769e+08</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            ticker    open    high     low   close     volume         value\n",
       "tradedate                                                                  \n",
       "2017-05-26  600519  450.82  455.60  448.00  451.92  2738664.0  1.237323e+09\n",
       "2017-05-31  600519  450.00  450.50  440.11  442.94  4431225.0  1.968887e+09\n",
       "2017-06-01  600519  442.50  449.95  441.01  449.28  4060478.0  1.808306e+09\n",
       "2017-06-02  600519  450.00  450.95  445.60  447.31  2178526.0  9.748933e+08\n",
       "2017-06-05  600519  448.04  449.00  442.35  444.41  1924120.0  8.548769e+08"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stock_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d159499e",
   "metadata": {},
   "source": [
    "在开始训练之前，我们先把数据集按照日期进行简单拆分。这里大约 800 个交易日作为训练集，4 x 50 个交易日作为测试集。DeepAR 需要数据格式为 JSON："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fb993af7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>000568-close</th>\n",
       "      <th>000568-high</th>\n",
       "      <th>000568-low</th>\n",
       "      <th>000568-open</th>\n",
       "      <th>000596-close</th>\n",
       "      <th>000596-high</th>\n",
       "      <th>000596-low</th>\n",
       "      <th>000596-open</th>\n",
       "      <th>000858-close</th>\n",
       "      <th>000858-high</th>\n",
       "      <th>...</th>\n",
       "      <th>002304-low</th>\n",
       "      <th>002304-open</th>\n",
       "      <th>600519-close</th>\n",
       "      <th>600519-high</th>\n",
       "      <th>600519-low</th>\n",
       "      <th>600519-open</th>\n",
       "      <th>600779-close</th>\n",
       "      <th>600779-high</th>\n",
       "      <th>600779-low</th>\n",
       "      <th>600779-open</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tradedate</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2017-05-26</th>\n",
       "      <td>48.53</td>\n",
       "      <td>48.74</td>\n",
       "      <td>47.50</td>\n",
       "      <td>48.60</td>\n",
       "      <td>47.89</td>\n",
       "      <td>48.50</td>\n",
       "      <td>47.70</td>\n",
       "      <td>47.84</td>\n",
       "      <td>47.56</td>\n",
       "      <td>48.26</td>\n",
       "      <td>...</td>\n",
       "      <td>85.57</td>\n",
       "      <td>86.90</td>\n",
       "      <td>451.92</td>\n",
       "      <td>455.60</td>\n",
       "      <td>448.00</td>\n",
       "      <td>450.82</td>\n",
       "      <td>22.70</td>\n",
       "      <td>22.98</td>\n",
       "      <td>22.36</td>\n",
       "      <td>22.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-05-31</th>\n",
       "      <td>48.22</td>\n",
       "      <td>48.41</td>\n",
       "      <td>46.94</td>\n",
       "      <td>48.10</td>\n",
       "      <td>47.40</td>\n",
       "      <td>48.39</td>\n",
       "      <td>46.80</td>\n",
       "      <td>47.89</td>\n",
       "      <td>47.88</td>\n",
       "      <td>47.88</td>\n",
       "      <td>...</td>\n",
       "      <td>85.30</td>\n",
       "      <td>86.92</td>\n",
       "      <td>442.94</td>\n",
       "      <td>450.50</td>\n",
       "      <td>440.11</td>\n",
       "      <td>450.00</td>\n",
       "      <td>22.79</td>\n",
       "      <td>23.08</td>\n",
       "      <td>22.45</td>\n",
       "      <td>22.59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-06-01</th>\n",
       "      <td>48.01</td>\n",
       "      <td>48.48</td>\n",
       "      <td>47.46</td>\n",
       "      <td>47.74</td>\n",
       "      <td>47.21</td>\n",
       "      <td>47.68</td>\n",
       "      <td>46.99</td>\n",
       "      <td>47.00</td>\n",
       "      <td>48.34</td>\n",
       "      <td>48.54</td>\n",
       "      <td>...</td>\n",
       "      <td>86.00</td>\n",
       "      <td>86.68</td>\n",
       "      <td>449.28</td>\n",
       "      <td>449.95</td>\n",
       "      <td>441.01</td>\n",
       "      <td>442.50</td>\n",
       "      <td>23.69</td>\n",
       "      <td>24.05</td>\n",
       "      <td>22.65</td>\n",
       "      <td>22.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-06-02</th>\n",
       "      <td>47.46</td>\n",
       "      <td>48.24</td>\n",
       "      <td>47.10</td>\n",
       "      <td>48.02</td>\n",
       "      <td>46.97</td>\n",
       "      <td>47.57</td>\n",
       "      <td>46.29</td>\n",
       "      <td>47.21</td>\n",
       "      <td>47.96</td>\n",
       "      <td>48.74</td>\n",
       "      <td>...</td>\n",
       "      <td>85.55</td>\n",
       "      <td>87.30</td>\n",
       "      <td>447.31</td>\n",
       "      <td>450.95</td>\n",
       "      <td>445.60</td>\n",
       "      <td>450.00</td>\n",
       "      <td>23.40</td>\n",
       "      <td>24.00</td>\n",
       "      <td>23.21</td>\n",
       "      <td>23.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-06-05</th>\n",
       "      <td>46.69</td>\n",
       "      <td>47.63</td>\n",
       "      <td>46.47</td>\n",
       "      <td>47.37</td>\n",
       "      <td>46.15</td>\n",
       "      <td>47.20</td>\n",
       "      <td>45.97</td>\n",
       "      <td>46.97</td>\n",
       "      <td>47.09</td>\n",
       "      <td>48.50</td>\n",
       "      <td>...</td>\n",
       "      <td>84.20</td>\n",
       "      <td>86.59</td>\n",
       "      <td>444.41</td>\n",
       "      <td>449.00</td>\n",
       "      <td>442.35</td>\n",
       "      <td>448.04</td>\n",
       "      <td>23.14</td>\n",
       "      <td>23.64</td>\n",
       "      <td>23.03</td>\n",
       "      <td>23.47</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            000568-close  000568-high  000568-low  000568-open  000596-close  \\\n",
       "tradedate                                                                      \n",
       "2017-05-26         48.53        48.74       47.50        48.60         47.89   \n",
       "2017-05-31         48.22        48.41       46.94        48.10         47.40   \n",
       "2017-06-01         48.01        48.48       47.46        47.74         47.21   \n",
       "2017-06-02         47.46        48.24       47.10        48.02         46.97   \n",
       "2017-06-05         46.69        47.63       46.47        47.37         46.15   \n",
       "\n",
       "            000596-high  000596-low  000596-open  000858-close  000858-high  \\\n",
       "tradedate                                                                     \n",
       "2017-05-26        48.50       47.70        47.84         47.56        48.26   \n",
       "2017-05-31        48.39       46.80        47.89         47.88        47.88   \n",
       "2017-06-01        47.68       46.99        47.00         48.34        48.54   \n",
       "2017-06-02        47.57       46.29        47.21         47.96        48.74   \n",
       "2017-06-05        47.20       45.97        46.97         47.09        48.50   \n",
       "\n",
       "            ...  002304-low  002304-open  600519-close  600519-high  \\\n",
       "tradedate   ...                                                       \n",
       "2017-05-26  ...       85.57        86.90        451.92       455.60   \n",
       "2017-05-31  ...       85.30        86.92        442.94       450.50   \n",
       "2017-06-01  ...       86.00        86.68        449.28       449.95   \n",
       "2017-06-02  ...       85.55        87.30        447.31       450.95   \n",
       "2017-06-05  ...       84.20        86.59        444.41       449.00   \n",
       "\n",
       "            600519-low  600519-open  600779-close  600779-high  600779-low  \\\n",
       "tradedate                                                                    \n",
       "2017-05-26      448.00       450.82         22.70        22.98       22.36   \n",
       "2017-05-31      440.11       450.00         22.79        23.08       22.45   \n",
       "2017-06-01      441.01       442.50         23.69        24.05       22.65   \n",
       "2017-06-02      445.60       450.00         23.40        24.00       23.21   \n",
       "2017-06-05      442.35       448.04         23.14        23.64       23.03   \n",
       "\n",
       "            600779-open  \n",
       "tradedate                \n",
       "2017-05-26        22.49  \n",
       "2017-05-31        22.59  \n",
       "2017-06-01        22.84  \n",
       "2017-06-02        23.69  \n",
       "2017-06-05        23.47  \n",
       "\n",
       "[5 rows x 24 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a3f4980d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def json_serialize(data, start, end, target_column, covariate_columns, interval):\n",
    "    \n",
    "    timeseries = {}\n",
    "    for i, col in enumerate(data.columns):\n",
    "        metric = col[col.find('-')+1:]\n",
    "        ticker = col[:col.find('-')]\n",
    "        if metric == target_column:\n",
    "            if ticker in timeseries.keys():\n",
    "                timeseries[ticker][\"target\"] = data.iloc[:,i][start:end]\n",
    "            else:\n",
    "                timeseries[ticker] = {}\n",
    "                timeseries[ticker][\"start\"] = str(pd.Timestamp(start, freq = interval))\n",
    "                timeseries[ticker][\"target\"] = data.iloc[:,i][start:end]            \n",
    "            print(\"Time series for {} added\".format(ticker))\n",
    "        elif metric in covariate_columns:\n",
    "            if ticker in timeseries.keys():\n",
    "                if \"dynamic_feat\" in timeseries[ticker]:\n",
    "                    dynamic_feat = timeseries[ticker][\"dynamic_feat\"]\n",
    "                    dynamic_feat.append(data.iloc[:,i][start:end])\n",
    "                else:\n",
    "                    dynamic_feat = []\n",
    "                    dynamic_feat.append(data.iloc[:,i][start:end])\n",
    "                    timeseries[ticker][\"dynamic_feat\"] = dynamic_feat\n",
    "            else:\n",
    "                timeseries[ticker] = {}\n",
    "                dynamic_feat = []\n",
    "                dynamic_feat.append(data.iloc[:,i])\n",
    "                timeseries[ticker][\"dynamic_feat\"] = dynamic_feat            \n",
    "            print(\"Dynamic Feature - {} for {} added\".format(metric, ticker))\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "    json_data = [\n",
    "        {\n",
    "            \"start\": ts[\"start\"],\n",
    "            \"target\": ts[\"target\"].tolist(),  \n",
    "            \"dynamic_feat\": [feat.tolist() for feat in ts[\"dynamic_feat\"]]\n",
    "        }\n",
    "        for ts in timeseries.values()\n",
    "    ]\n",
    "    \n",
    "    return json_data\n",
    "\n",
    "\n",
    "def generate_train_test_set(data, target_column, covariate_columns, interval, train_test_split=0.9, num_test_windows=4):\n",
    "    \n",
    "    num_samples = len(data.index.values)\n",
    "    num_train = int(train_test_split * num_samples)\n",
    "    num_test = int((num_samples - num_train)/num_test_windows)\n",
    "    \n",
    "    print(\"Sample Size = {}, Training Set: {}, Test Set: {} * {}\".format(num_samples, num_train, num_test_windows, num_test))\n",
    "    train_start_date = data.index[0]\n",
    "    train_end_date = data.index[num_train - 1]   \n",
    "    print(\"Training Set: Starts at - {}, Ends at - {}\".format(train_start_date, train_end_date))\n",
    "    \n",
    "    print('Training Set:')\n",
    "    train_data = json_serialize(data, train_start_date, train_end_date, target_column, covariate_columns, interval)\n",
    "    \n",
    "    test_data = []\n",
    "    test_start_date = train_start_date\n",
    "    for i in range(num_test_windows):\n",
    "        test_end_date = data.index.values[num_train + i*num_test - 1]\n",
    "        print('Testing Set:', test_end_date)\n",
    "        test_data.extend(json_serialize(data, test_start_date, test_end_date, target_column, covariate_columns, interval))\n",
    "        \n",
    "    return train_data, test_data, train_start_date, train_end_date"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2d6d3da",
   "metadata": {},
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1a19cf63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Size = 1000, Training Set: 800, Test Set: 4 * 50\n",
      "Training Set: Starts at - 2017-05-26, Ends at - 2020-09-03\n",
      "Training Set:\n",
      "Time series for 000568 added\n",
      "Dynamic Feature - high for 000568 added\n",
      "Dynamic Feature - low for 000568 added\n",
      "Dynamic Feature - open for 000568 added\n",
      "Time series for 000596 added\n",
      "Dynamic Feature - high for 000596 added\n",
      "Dynamic Feature - low for 000596 added\n",
      "Dynamic Feature - open for 000596 added\n",
      "Time series for 000858 added\n",
      "Dynamic Feature - high for 000858 added\n",
      "Dynamic Feature - low for 000858 added\n",
      "Dynamic Feature - open for 000858 added\n",
      "Time series for 002304 added\n",
      "Dynamic Feature - high for 002304 added\n",
      "Dynamic Feature - low for 002304 added\n",
      "Dynamic Feature - open for 002304 added\n",
      "Time series for 600519 added\n",
      "Dynamic Feature - high for 600519 added\n",
      "Dynamic Feature - low for 600519 added\n",
      "Dynamic Feature - open for 600519 added\n",
      "Time series for 600779 added\n",
      "Dynamic Feature - high for 600779 added\n",
      "Dynamic Feature - low for 600779 added\n",
      "Dynamic Feature - open for 600779 added\n",
      "Testing Set: 2020-09-03\n",
      "Time series for 000568 added\n",
      "Dynamic Feature - high for 000568 added\n",
      "Dynamic Feature - low for 000568 added\n",
      "Dynamic Feature - open for 000568 added\n",
      "Time series for 000596 added\n",
      "Dynamic Feature - high for 000596 added\n",
      "Dynamic Feature - low for 000596 added\n",
      "Dynamic Feature - open for 000596 added\n",
      "Time series for 000858 added\n",
      "Dynamic Feature - high for 000858 added\n",
      "Dynamic Feature - low for 000858 added\n",
      "Dynamic Feature - open for 000858 added\n",
      "Time series for 002304 added\n",
      "Dynamic Feature - high for 002304 added\n",
      "Dynamic Feature - low for 002304 added\n",
      "Dynamic Feature - open for 002304 added\n",
      "Time series for 600519 added\n",
      "Dynamic Feature - high for 600519 added\n",
      "Dynamic Feature - low for 600519 added\n",
      "Dynamic Feature - open for 600519 added\n",
      "Time series for 600779 added\n",
      "Dynamic Feature - high for 600779 added\n",
      "Dynamic Feature - low for 600779 added\n",
      "Dynamic Feature - open for 600779 added\n",
      "Testing Set: 2020-11-20\n",
      "Time series for 000568 added\n",
      "Dynamic Feature - high for 000568 added\n",
      "Dynamic Feature - low for 000568 added\n",
      "Dynamic Feature - open for 000568 added\n",
      "Time series for 000596 added\n",
      "Dynamic Feature - high for 000596 added\n",
      "Dynamic Feature - low for 000596 added\n",
      "Dynamic Feature - open for 000596 added\n",
      "Time series for 000858 added\n",
      "Dynamic Feature - high for 000858 added\n",
      "Dynamic Feature - low for 000858 added\n",
      "Dynamic Feature - open for 000858 added\n",
      "Time series for 002304 added\n",
      "Dynamic Feature - high for 002304 added\n",
      "Dynamic Feature - low for 002304 added\n",
      "Dynamic Feature - open for 002304 added\n",
      "Time series for 600519 added\n",
      "Dynamic Feature - high for 600519 added\n",
      "Dynamic Feature - low for 600519 added\n",
      "Dynamic Feature - open for 600519 added\n",
      "Time series for 600779 added\n",
      "Dynamic Feature - high for 600779 added\n",
      "Dynamic Feature - low for 600779 added\n",
      "Dynamic Feature - open for 600779 added\n",
      "Testing Set: 2021-02-01\n",
      "Time series for 000568 added\n",
      "Dynamic Feature - high for 000568 added\n",
      "Dynamic Feature - low for 000568 added\n",
      "Dynamic Feature - open for 000568 added\n",
      "Time series for 000596 added\n",
      "Dynamic Feature - high for 000596 added\n",
      "Dynamic Feature - low for 000596 added\n",
      "Dynamic Feature - open for 000596 added\n",
      "Time series for 000858 added\n",
      "Dynamic Feature - high for 000858 added\n",
      "Dynamic Feature - low for 000858 added\n",
      "Dynamic Feature - open for 000858 added\n",
      "Time series for 002304 added\n",
      "Dynamic Feature - high for 002304 added\n",
      "Dynamic Feature - low for 002304 added\n",
      "Dynamic Feature - open for 002304 added\n",
      "Time series for 600519 added\n",
      "Dynamic Feature - high for 600519 added\n",
      "Dynamic Feature - low for 600519 added\n",
      "Dynamic Feature - open for 600519 added\n",
      "Time series for 600779 added\n",
      "Dynamic Feature - high for 600779 added\n",
      "Dynamic Feature - low for 600779 added\n",
      "Dynamic Feature - open for 600779 added\n",
      "Testing Set: 2021-04-20\n",
      "Time series for 000568 added\n",
      "Dynamic Feature - high for 000568 added\n",
      "Dynamic Feature - low for 000568 added\n",
      "Dynamic Feature - open for 000568 added\n",
      "Time series for 000596 added\n",
      "Dynamic Feature - high for 000596 added\n",
      "Dynamic Feature - low for 000596 added\n",
      "Dynamic Feature - open for 000596 added\n",
      "Time series for 000858 added\n",
      "Dynamic Feature - high for 000858 added\n",
      "Dynamic Feature - low for 000858 added\n",
      "Dynamic Feature - open for 000858 added\n",
      "Time series for 002304 added\n",
      "Dynamic Feature - high for 002304 added\n",
      "Dynamic Feature - low for 002304 added\n",
      "Dynamic Feature - open for 002304 added\n",
      "Time series for 600519 added\n",
      "Dynamic Feature - high for 600519 added\n",
      "Dynamic Feature - low for 600519 added\n",
      "Dynamic Feature - open for 600519 added\n",
      "Time series for 600779 added\n",
      "Dynamic Feature - high for 600779 added\n",
      "Dynamic Feature - low for 600779 added\n",
      "Dynamic Feature - open for 600779 added\n"
     ]
    }
   ],
   "source": [
    "train_data, test_data, train_start_date, train_end_date = generate_train_test_set(df, target_column, covariate_columns, interval, train_test_split, num_test_windows)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47e6be8a",
   "metadata": {},
   "source": [
    "接下来将数据写入到 S3 指定路径："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "709fa3b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train channel s3://algo-trading-workshop-peng/sagemaker-deepar/data/train.json\n",
      "Test channel s3://algo-trading-workshop-peng/sagemaker-deepar/data/test.json\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import json\n",
    "\n",
    "s3 = boto3.client('s3', region_name=aws_region)\n",
    "\n",
    "train_data_json = ''\n",
    "for r in train_data:\n",
    "    train_data_json += json.dumps(r)\n",
    "    train_data_json += \"\\n\"\n",
    "    \n",
    "test_data_json = ''\n",
    "for r in test_data:\n",
    "    test_data_json += json.dumps(r)\n",
    "    test_data_json += \"\\n\"\n",
    "    \n",
    "s3.put_object(Body=train_data_json.encode('utf-8'), Bucket=bucket, Key='{}/data/train.json'.format(repository_name))\n",
    "s3.put_object(Body=test_data_json.encode('utf-8'), Bucket=bucket, Key='{}/data/test.json'.format(repository_name))\n",
    "train_channel = 's3://{}/{}/data/train.json'.format(bucket, repository_name)\n",
    "test_channel = 's3://{}/{}/data/test.json'.format(bucket, repository_name)\n",
    "print(\"Train channel {}\".format(train_channel))\n",
    "print(\"Test channel {}\".format(test_channel))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc8629fd",
   "metadata": {},
   "source": [
    "### 模型训练\n",
    "\n",
    "与使用自定义 RNN 模型类似，接下来的代码将创建 estimator 类，并在一台新实例上开始模型的训练工作。根据使用的实例大小不同，这可能会花费10-15分钟左右的时间："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "062e3505",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'get_image_uri' method will be deprecated in favor of 'ImageURIProvider' class in SageMaker Python SDK v2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "镜像： 522234722520.dkr.ecr.us-east-1.amazonaws.com/forecasting-deepar:latest\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "from sagemaker.estimator import Estimator\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "session = sagemaker.Session()\n",
    "image_url = sagemaker.amazon.amazon_estimator.get_image_uri(aws_region, \"forecasting-deepar\", \"latest\")\n",
    "print('镜像：', image_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9e707166",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter image_name will be renamed to image_uri in SageMaker Python SDK v2.\n"
     ]
    }
   ],
   "source": [
    "estimator = Estimator(\n",
    "    image_url,\n",
    "    sagemaker_session=session,\n",
    "    role=role,\n",
    "    train_instance_count=1,\n",
    "    train_instance_type='ml.m5.2xlarge',\n",
    "    base_job_name=repository_name,\n",
    "    output_path='s3://{}/{}/output'.format(bucket, repository_name)\n",
    ")\n",
    "\n",
    "# Set the hyperparamters\n",
    "estimator.set_hyperparameters(**hyperparameters)\n",
    "\n",
    "# Specify data channels\n",
    "data_channels = {\n",
    "    \"train\": train_channel,\n",
    "    \"test\": test_channel\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "13d6a9ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'s3_input' class will be renamed to 'TrainingInput' in SageMaker Python SDK v2.\n",
      "'s3_input' class will be renamed to 'TrainingInput' in SageMaker Python SDK v2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-12-10 03:21:36 Starting - Starting the training job...\n",
      "2021-12-10 03:21:39 Starting - Launching requested ML instances......\n",
      "2021-12-10 03:22:40 Starting - Preparing the instances for training......\n",
      "2021-12-10 03:24:01 Downloading - Downloading input data...\n",
      "2021-12-10 03:24:18 Training - Downloading the training image.....\u001b[34mArguments: train\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:25:19 INFO 140644862207616] Reading default configuration from /opt/amazon/lib/python3.6/site-packages/algorithm/resources/default-input.json: {'_kvstore': 'auto', '_num_gpus': 'auto', '_num_kv_servers': 'auto', '_tuning_objective_metric': '', 'cardinality': 'auto', 'dropout_rate': '0.10', 'early_stopping_patience': '', 'embedding_dimension': '10', 'learning_rate': '0.001', 'likelihood': 'student-t', 'mini_batch_size': '128', 'num_cells': '40', 'num_dynamic_feat': 'auto', 'num_eval_samples': '100', 'num_layers': '2', 'test_quantiles': '[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]'}\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:25:19 INFO 140644862207616] Merging with provided configuration from /opt/ml/input/config/hyperparameters.json: {'prediction_length': '91', 'dropout_rate': '0.1', 'time_freq': 'D', 'likelihood': 'gaussian', 'context_length': '91', 'num_layers': '2', 'epochs': '200', 'learning_rate': '1e-3', 'early_stopping_patience': '40', 'mini_batch_size': '128', 'num_cells': '40'}\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:25:19 INFO 140644862207616] Final configuration: {'_kvstore': 'auto', '_num_gpus': 'auto', '_num_kv_servers': 'auto', '_tuning_objective_metric': '', 'cardinality': 'auto', 'dropout_rate': '0.1', 'early_stopping_patience': '40', 'embedding_dimension': '10', 'learning_rate': '1e-3', 'likelihood': 'gaussian', 'mini_batch_size': '128', 'num_cells': '40', 'num_dynamic_feat': 'auto', 'num_eval_samples': '100', 'num_layers': '2', 'test_quantiles': '[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]', 'prediction_length': '91', 'time_freq': 'D', 'context_length': '91', 'epochs': '200'}\u001b[0m\n",
      "\u001b[34mProcess 1 is a worker.\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:25:19 INFO 140644862207616] Detected entry point for worker worker\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:25:19 INFO 140644862207616] Using early stopping with patience 40\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:25:19 INFO 140644862207616] random_seed is None\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:25:19 INFO 140644862207616] [cardinality=auto] `cat` field was NOT found in the file `/opt/ml/input/data/train/train.json` and will NOT be used for training.\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:25:19 INFO 140644862207616] [num_dynamic_feat=auto] `dynamic_feat` field was found in the file `/opt/ml/input/data/train/train.json` and will be used for training.\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:25:19 INFO 140644862207616] [num_dynamic_feat=auto] Inferred value of num_dynamic_feat=3 from dataset.\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:25:19 INFO 140644862207616] Training set statistics:\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:25:19 INFO 140644862207616] Real time series\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:25:19 INFO 140644862207616] number of time series: 6\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:25:19 INFO 140644862207616] number of observations: 4800\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:25:19 INFO 140644862207616] mean target length: 800.0\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:25:19 INFO 140644862207616] min/mean/max target: 22.700000762939453/223.70556477864582/1904.6365966796875\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:25:19 INFO 140644862207616] mean abs(target): 223.70556477864582\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:25:19 INFO 140644862207616] contains missing values: no\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:25:19 INFO 140644862207616] Small number of time series. Doing 214 passes over dataset with prob 0.9968847352024922 per epoch.\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:25:19 INFO 140644862207616] Test set statistics:\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:25:19 INFO 140644862207616] Real time series\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:25:19 INFO 140644862207616] number of time series: 24\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:25:19 INFO 140644862207616] number of observations: 21000\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:25:19 INFO 140644862207616] mean target length: 875.0\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:25:19 INFO 140644862207616] min/mean/max target: 22.700000762939453/246.69826041666667/2749.17578125\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:25:19 INFO 140644862207616] mean abs(target): 246.69826041666667\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:25:19 INFO 140644862207616] contains missing values: no\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:25:19 INFO 140644862207616] #memory_usage::<batchbuffer> = 49.627723693847656 mb\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:25:19 INFO 140644862207616] nvidia-smi took: 0.025169849395751953 secs to identify 0 gpus\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:25:19 INFO 140644862207616] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:25:19 INFO 140644862207616] Create Store: local\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1639106719.2736397, \"EndTime\": 1639106719.6914515, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"get_graph.time\": {\"sum\": 416.0623550415039, \"count\": 1, \"min\": 416.0623550415039, \"max\": 416.0623550415039}}}\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:25:19 INFO 140644862207616] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:25:20 INFO 140644862207616] #memory_usage::<model> = 115 mb\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1639106719.691539, \"EndTime\": 1639106720.2337399, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"initialize.time\": {\"sum\": 959.9778652191162, \"count\": 1, \"min\": 959.9778652191162, \"max\": 959.9778652191162}}}\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:25:21 INFO 140644862207616] Epoch[0] Batch[0] avg_epoch_loss=6.216046\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:25:21 INFO 140644862207616] #quality_metric: host=algo-1, epoch=0, batch=0 train loss <loss>=6.216046333312988\u001b[0m\n",
      "\n",
      "2021-12-10 03:25:17 Training - Training image download completed. Training in progress.\u001b[34m[12/10/2021 03:25:23 INFO 140644862207616] Epoch[0] Batch[5] avg_epoch_loss=5.699763\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:25:23 INFO 140644862207616] #quality_metric: host=algo-1, epoch=0, batch=5 train loss <loss>=5.6997629801432295\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:25:23 INFO 140644862207616] Epoch[0] Batch [5]#011Speed: 289.18 samples/sec#011loss=5.699763\u001b[0m\n",
      "\u001b[34m/opt/amazon/python3.6/lib/python3.6/contextlib.py:99: DeprecationWarning: generator 'local_timer' raised StopIteration\n",
      "  self.gen.throw(type, value, traceback)\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:25:25 INFO 140644862207616] processed a total of 1256 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1639106720.2338324, \"EndTime\": 1639106725.5025766, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"epochs\": {\"sum\": 200.0, \"count\": 1, \"min\": 200, \"max\": 200}, \"update.time\": {\"sum\": 5268.653392791748, \"count\": 1, \"min\": 5268.653392791748, \"max\": 5268.653392791748}}}\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:25:25 INFO 140644862207616] #throughput_metric: host=algo-1, train throughput=238.38549730999603 records/second\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:25:25 INFO 140644862207616] #progress_metric: host=algo-1, completed 0.5 % of epochs\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:25:25 INFO 140644862207616] #quality_metric: host=algo-1, epoch=0, train loss <loss>=5.591681671142578\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:25:25 INFO 140644862207616] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:25:25 INFO 140644862207616] Saved checkpoint to \"/opt/ml/model/state_fe3394f3-8ab6-425a-9b37-9d1beca9a123-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1639106725.5026627, \"EndTime\": 1639106725.557148, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 54.03399467468262, \"count\": 1, \"min\": 54.03399467468262, \"max\": 54.03399467468262}}}\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:25:26 INFO 140644862207616] Epoch[1] Batch[0] avg_epoch_loss=5.138002\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:25:26 INFO 140644862207616] #quality_metric: host=algo-1, epoch=1, batch=0 train loss <loss>=5.138002395629883\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:25:28 INFO 140644862207616] Epoch[1] Batch[5] avg_epoch_loss=4.968291\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:25:28 INFO 140644862207616] #quality_metric: host=algo-1, epoch=1, batch=5 train loss <loss>=4.968291123708089\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:25:28 INFO 140644862207616] Epoch[1] Batch [5]#011Speed: 313.97 samples/sec#011loss=4.968291\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:25:30 INFO 140644862207616] processed a total of 1275 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1639106725.5572188, \"EndTime\": 1639106730.4620683, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4904.787302017212, \"count\": 1, \"min\": 4904.787302017212, \"max\": 4904.787302017212}}}\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:25:30 INFO 140644862207616] #throughput_metric: host=algo-1, train throughput=259.9442599274609 records/second\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:25:30 INFO 140644862207616] #progress_metric: host=algo-1, completed 1.0 % of epochs\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:25:30 INFO 140644862207616] #quality_metric: host=algo-1, epoch=1, train loss <loss>=4.863806056976318\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:25:30 INFO 140644862207616] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:25:30 INFO 140644862207616] Saved checkpoint to \"/opt/ml/model/state_3167f523-da3d-44a3-b883-844fef33f720-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1639106730.462146, \"EndTime\": 1639106730.5228188, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 60.26649475097656, \"count\": 1, \"min\": 60.26649475097656, \"max\": 60.26649475097656}}}\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:25:31 INFO 140644862207616] Epoch[2] Batch[0] avg_epoch_loss=4.577507\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:25:31 INFO 140644862207616] #quality_metric: host=algo-1, epoch=2, batch=0 train loss <loss>=4.5775065422058105\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:25:33 INFO 140644862207616] Epoch[2] Batch[5] avg_epoch_loss=4.499873\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:25:33 INFO 140644862207616] #quality_metric: host=algo-1, epoch=2, batch=5 train loss <loss>=4.499873240788777\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:25:33 INFO 140644862207616] Epoch[2] Batch [5]#011Speed: 312.37 samples/sec#011loss=4.499873\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:25:35 INFO 140644862207616] Epoch[2] Batch[10] avg_epoch_loss=4.423357\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:25:35 INFO 140644862207616] #quality_metric: host=algo-1, epoch=2, batch=10 train loss <loss>=4.331538248062134\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:25:35 INFO 140644862207616] Epoch[2] Batch [10]#011Speed: 314.26 samples/sec#011loss=4.331538\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:25:35 INFO 140644862207616] processed a total of 1308 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1639106730.5228775, \"EndTime\": 1639106735.832271, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5309.33141708374, \"count\": 1, \"min\": 5309.33141708374, \"max\": 5309.33141708374}}}\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:25:35 INFO 140644862207616] #throughput_metric: host=algo-1, train throughput=246.35459841815563 records/second\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:25:35 INFO 140644862207616] #progress_metric: host=algo-1, completed 1.5 % of epochs\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:25:35 INFO 140644862207616] #quality_metric: host=algo-1, epoch=2, train loss <loss>=4.4233573350039395\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:25:35 INFO 140644862207616] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:25:35 INFO 140644862207616] Saved checkpoint to \"/opt/ml/model/state_d571f475-daaa-4622-bf36-66fbdd76c3ac-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1639106735.8323293, \"EndTime\": 1639106735.8733766, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 40.62032699584961, \"count\": 1, \"min\": 40.62032699584961, \"max\": 40.62032699584961}}}\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:25:37 INFO 140644862207616] Epoch[3] Batch[0] avg_epoch_loss=4.250520\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:25:37 INFO 140644862207616] #quality_metric: host=algo-1, epoch=3, batch=0 train loss <loss>=4.2505202293396\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:25:39 INFO 140644862207616] Epoch[3] Batch[5] avg_epoch_loss=4.255737\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:25:39 INFO 140644862207616] #quality_metric: host=algo-1, epoch=3, batch=5 train loss <loss>=4.25573746363322\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:25:39 INFO 140644862207616] Epoch[3] Batch [5]#011Speed: 314.78 samples/sec#011loss=4.255737\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:25:41 INFO 140644862207616] Epoch[3] Batch[10] avg_epoch_loss=4.089434\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:25:41 INFO 140644862207616] #quality_metric: host=algo-1, epoch=3, batch=10 train loss <loss>=3.8898691654205324\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:25:41 INFO 140644862207616] Epoch[3] Batch [10]#011Speed: 306.67 samples/sec#011loss=3.889869\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:25:41 INFO 140644862207616] processed a total of 1282 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1639106735.8734467, \"EndTime\": 1639106741.2202322, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5346.720933914185, \"count\": 1, \"min\": 5346.720933914185, \"max\": 5346.720933914185}}}\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:25:41 INFO 140644862207616] #throughput_metric: host=algo-1, train throughput=239.76884010620805 records/second\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:25:41 INFO 140644862207616] #progress_metric: host=algo-1, completed 2.0 % of epochs\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:25:41 INFO 140644862207616] #quality_metric: host=algo-1, epoch=3, train loss <loss>=4.089433691718361\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:25:41 INFO 140644862207616] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:25:41 INFO 140644862207616] Saved checkpoint to \"/opt/ml/model/state_1d6308a3-9ade-4762-92a6-3a48664ac645-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1639106741.2202966, \"EndTime\": 1639106741.2597537, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 39.05057907104492, \"count\": 1, \"min\": 39.05057907104492, \"max\": 39.05057907104492}}}\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:25:42 INFO 140644862207616] Epoch[4] Batch[0] avg_epoch_loss=4.297471\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:25:42 INFO 140644862207616] #quality_metric: host=algo-1, epoch=4, batch=0 train loss <loss>=4.297470569610596\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:25:44 INFO 140644862207616] Epoch[4] Batch[5] avg_epoch_loss=4.078135\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:25:44 INFO 140644862207616] #quality_metric: host=algo-1, epoch=4, batch=5 train loss <loss>=4.078135172526042\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:25:44 INFO 140644862207616] Epoch[4] Batch [5]#011Speed: 313.92 samples/sec#011loss=4.078135\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:25:46 INFO 140644862207616] processed a total of 1187 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1639106741.2598188, \"EndTime\": 1639106746.1166954, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4856.814384460449, \"count\": 1, \"min\": 4856.814384460449, \"max\": 4856.814384460449}}}\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:25:46 INFO 140644862207616] #throughput_metric: host=algo-1, train throughput=244.39251037491763 records/second\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:25:46 INFO 140644862207616] #progress_metric: host=algo-1, completed 2.5 % of epochs\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:25:46 INFO 140644862207616] #quality_metric: host=algo-1, epoch=4, train loss <loss>=3.97738516330719\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:25:46 INFO 140644862207616] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:25:46 INFO 140644862207616] Saved checkpoint to \"/opt/ml/model/state_bdf6b87e-59b0-48a3-b0c6-74e3b6bec34e-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1639106746.1167848, \"EndTime\": 1639106746.1775022, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 60.19186973571777, \"count\": 1, \"min\": 60.19186973571777, \"max\": 60.19186973571777}}}\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:25:47 INFO 140644862207616] Epoch[5] Batch[0] avg_epoch_loss=4.043955\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:25:47 INFO 140644862207616] #quality_metric: host=algo-1, epoch=5, batch=0 train loss <loss>=4.043954849243164\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:25:49 INFO 140644862207616] Epoch[5] Batch[5] avg_epoch_loss=4.020898\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:25:49 INFO 140644862207616] #quality_metric: host=algo-1, epoch=5, batch=5 train loss <loss>=4.020898222923279\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:25:49 INFO 140644862207616] Epoch[5] Batch [5]#011Speed: 317.28 samples/sec#011loss=4.020898\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:25:51 INFO 140644862207616] Epoch[5] Batch[10] avg_epoch_loss=4.238394\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:25:51 INFO 140644862207616] #quality_metric: host=algo-1, epoch=5, batch=10 train loss <loss>=4.499387931823731\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:25:51 INFO 140644862207616] Epoch[5] Batch [10]#011Speed: 317.67 samples/sec#011loss=4.499388\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:25:51 INFO 140644862207616] processed a total of 1297 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1639106746.1775713, \"EndTime\": 1639106751.4375362, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5259.901285171509, \"count\": 1, \"min\": 5259.901285171509, \"max\": 5259.901285171509}}}\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:25:51 INFO 140644862207616] #throughput_metric: host=algo-1, train throughput=246.5778578025874 records/second\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:25:51 INFO 140644862207616] #progress_metric: host=algo-1, completed 3.0 % of epochs\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:25:51 INFO 140644862207616] #quality_metric: host=algo-1, epoch=5, train loss <loss>=4.238393545150757\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:25:51 INFO 140644862207616] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:25:52 INFO 140644862207616] Epoch[6] Batch[0] avg_epoch_loss=3.879805\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:25:52 INFO 140644862207616] #quality_metric: host=algo-1, epoch=6, batch=0 train loss <loss>=3.8798046112060547\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:25:54 INFO 140644862207616] Epoch[6] Batch[5] avg_epoch_loss=3.889064\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:25:54 INFO 140644862207616] #quality_metric: host=algo-1, epoch=6, batch=5 train loss <loss>=3.889063835144043\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:25:54 INFO 140644862207616] Epoch[6] Batch [5]#011Speed: 314.37 samples/sec#011loss=3.889064\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:25:56 INFO 140644862207616] processed a total of 1255 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1639106751.4376063, \"EndTime\": 1639106756.313893, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4875.953912734985, \"count\": 1, \"min\": 4875.953912734985, \"max\": 4875.953912734985}}}\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:25:56 INFO 140644862207616] #throughput_metric: host=algo-1, train throughput=257.37984523035635 records/second\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:25:56 INFO 140644862207616] #progress_metric: host=algo-1, completed 3.5 % of epochs\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:25:56 INFO 140644862207616] #quality_metric: host=algo-1, epoch=6, train loss <loss>=3.8774065017700194\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:25:56 INFO 140644862207616] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:25:56 INFO 140644862207616] Saved checkpoint to \"/opt/ml/model/state_704079b6-fca6-4932-9c8b-ba35b8288211-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1639106756.3139694, \"EndTime\": 1639106756.3761897, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 61.789751052856445, \"count\": 1, \"min\": 61.789751052856445, \"max\": 61.789751052856445}}}\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:25:57 INFO 140644862207616] Epoch[7] Batch[0] avg_epoch_loss=3.883501\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:25:57 INFO 140644862207616] #quality_metric: host=algo-1, epoch=7, batch=0 train loss <loss>=3.8835010528564453\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:25:59 INFO 140644862207616] Epoch[7] Batch[5] avg_epoch_loss=3.889255\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:25:59 INFO 140644862207616] #quality_metric: host=algo-1, epoch=7, batch=5 train loss <loss>=3.889255166053772\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:25:59 INFO 140644862207616] Epoch[7] Batch [5]#011Speed: 316.75 samples/sec#011loss=3.889255\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:26:01 INFO 140644862207616] processed a total of 1274 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1639106756.3762589, \"EndTime\": 1639106761.2648318, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4888.505697250366, \"count\": 1, \"min\": 4888.505697250366, \"max\": 4888.505697250366}}}\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:26:01 INFO 140644862207616] #throughput_metric: host=algo-1, train throughput=260.60555275777307 records/second\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:26:01 INFO 140644862207616] #progress_metric: host=algo-1, completed 4.0 % of epochs\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:26:01 INFO 140644862207616] #quality_metric: host=algo-1, epoch=7, train loss <loss>=3.915016436576843\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:26:01 INFO 140644862207616] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:26:02 INFO 140644862207616] Epoch[8] Batch[0] avg_epoch_loss=3.790419\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:26:02 INFO 140644862207616] #quality_metric: host=algo-1, epoch=8, batch=0 train loss <loss>=3.7904186248779297\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:26:04 INFO 140644862207616] Epoch[8] Batch[5] avg_epoch_loss=3.749526\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:26:04 INFO 140644862207616] #quality_metric: host=algo-1, epoch=8, batch=5 train loss <loss>=3.7495259443918862\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:26:04 INFO 140644862207616] Epoch[8] Batch [5]#011Speed: 289.44 samples/sec#011loss=3.749526\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:26:06 INFO 140644862207616] Epoch[8] Batch[10] avg_epoch_loss=3.938008\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:26:06 INFO 140644862207616] #quality_metric: host=algo-1, epoch=8, batch=10 train loss <loss>=4.1641875267028805\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:26:06 INFO 140644862207616] Epoch[8] Batch [10]#011Speed: 286.99 samples/sec#011loss=4.164188\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:26:06 INFO 140644862207616] processed a total of 1292 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1639106761.2649064, \"EndTime\": 1639106766.9695725, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5704.229116439819, \"count\": 1, \"min\": 5704.229116439819, \"max\": 5704.229116439819}}}\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:26:06 INFO 140644862207616] #throughput_metric: host=algo-1, train throughput=226.49490498967572 records/second\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:26:06 INFO 140644862207616] #progress_metric: host=algo-1, completed 4.5 % of epochs\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:26:06 INFO 140644862207616] #quality_metric: host=algo-1, epoch=8, train loss <loss>=3.9380084818059746\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:26:06 INFO 140644862207616] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:26:08 INFO 140644862207616] Epoch[9] Batch[0] avg_epoch_loss=3.704995\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:26:08 INFO 140644862207616] #quality_metric: host=algo-1, epoch=9, batch=0 train loss <loss>=3.7049951553344727\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:26:10 INFO 140644862207616] Epoch[9] Batch[5] avg_epoch_loss=3.761115\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:26:10 INFO 140644862207616] #quality_metric: host=algo-1, epoch=9, batch=5 train loss <loss>=3.7611146767934165\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:26:10 INFO 140644862207616] Epoch[9] Batch [5]#011Speed: 306.83 samples/sec#011loss=3.761115\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:26:11 INFO 140644862207616] processed a total of 1280 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1639106766.9696372, \"EndTime\": 1639106771.908383, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4938.408136367798, \"count\": 1, \"min\": 4938.408136367798, \"max\": 4938.408136367798}}}\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:26:11 INFO 140644862207616] #throughput_metric: host=algo-1, train throughput=259.18717799293285 records/second\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:26:11 INFO 140644862207616] #progress_metric: host=algo-1, completed 5.0 % of epochs\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:26:11 INFO 140644862207616] #quality_metric: host=algo-1, epoch=9, train loss <loss>=3.6356317043304442\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:26:11 INFO 140644862207616] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:26:11 INFO 140644862207616] Saved checkpoint to \"/opt/ml/model/state_157a7403-9f8b-4d9f-921d-fe14b31cf5c8-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1639106771.9084587, \"EndTime\": 1639106771.9595394, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 50.681352615356445, \"count\": 1, \"min\": 50.681352615356445, \"max\": 50.681352615356445}}}\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:26:13 INFO 140644862207616] Epoch[10] Batch[0] avg_epoch_loss=3.890059\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:26:13 INFO 140644862207616] #quality_metric: host=algo-1, epoch=10, batch=0 train loss <loss>=3.890059471130371\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:26:15 INFO 140644862207616] Epoch[10] Batch[5] avg_epoch_loss=3.667193\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:26:15 INFO 140644862207616] #quality_metric: host=algo-1, epoch=10, batch=5 train loss <loss>=3.667193373044332\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:26:15 INFO 140644862207616] Epoch[10] Batch [5]#011Speed: 317.37 samples/sec#011loss=3.667193\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:26:17 INFO 140644862207616] Epoch[10] Batch[10] avg_epoch_loss=3.859573\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:26:17 INFO 140644862207616] #quality_metric: host=algo-1, epoch=10, batch=10 train loss <loss>=4.090429258346558\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:26:17 INFO 140644862207616] Epoch[10] Batch [10]#011Speed: 318.08 samples/sec#011loss=4.090429\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:26:17 INFO 140644862207616] processed a total of 1283 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1639106771.9595978, \"EndTime\": 1639106777.2429972, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5283.337831497192, \"count\": 1, \"min\": 5283.337831497192, \"max\": 5283.337831497192}}}\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:26:17 INFO 140644862207616] #throughput_metric: host=algo-1, train throughput=242.83428558226097 records/second\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:26:17 INFO 140644862207616] #progress_metric: host=algo-1, completed 5.5 % of epochs\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:26:17 INFO 140644862207616] #quality_metric: host=algo-1, epoch=10, train loss <loss>=3.85957332090898\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:26:17 INFO 140644862207616] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:26:18 INFO 140644862207616] Epoch[11] Batch[0] avg_epoch_loss=3.805005\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:26:18 INFO 140644862207616] #quality_metric: host=algo-1, epoch=11, batch=0 train loss <loss>=3.8050053119659424\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:26:20 INFO 140644862207616] Epoch[11] Batch[5] avg_epoch_loss=3.664631\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:26:20 INFO 140644862207616] #quality_metric: host=algo-1, epoch=11, batch=5 train loss <loss>=3.664631485939026\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:26:20 INFO 140644862207616] Epoch[11] Batch [5]#011Speed: 315.52 samples/sec#011loss=3.664631\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:26:22 INFO 140644862207616] Epoch[11] Batch[10] avg_epoch_loss=3.541035\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:26:22 INFO 140644862207616] #quality_metric: host=algo-1, epoch=11, batch=10 train loss <loss>=3.3927189350128173\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:26:22 INFO 140644862207616] Epoch[11] Batch [10]#011Speed: 320.57 samples/sec#011loss=3.392719\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:26:22 INFO 140644862207616] processed a total of 1296 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1639106777.2430675, \"EndTime\": 1639106782.5604677, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5317.0576095581055, \"count\": 1, \"min\": 5317.0576095581055, \"max\": 5317.0576095581055}}}\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:26:22 INFO 140644862207616] #throughput_metric: host=algo-1, train throughput=243.7391548586427 records/second\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:26:22 INFO 140644862207616] #progress_metric: host=algo-1, completed 6.0 % of epochs\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:26:22 INFO 140644862207616] #quality_metric: host=algo-1, epoch=11, train loss <loss>=3.541034871881658\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:26:22 INFO 140644862207616] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:26:22 INFO 140644862207616] Saved checkpoint to \"/opt/ml/model/state_95b05beb-4d1e-44f7-bf5c-e93030dc9d85-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1639106782.56054, \"EndTime\": 1639106782.607959, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 47.05548286437988, \"count\": 1, \"min\": 47.05548286437988, \"max\": 47.05548286437988}}}\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:26:23 INFO 140644862207616] Epoch[12] Batch[0] avg_epoch_loss=3.479453\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:26:23 INFO 140644862207616] #quality_metric: host=algo-1, epoch=12, batch=0 train loss <loss>=3.479452610015869\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:26:25 INFO 140644862207616] Epoch[12] Batch[5] avg_epoch_loss=3.364581\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:26:25 INFO 140644862207616] #quality_metric: host=algo-1, epoch=12, batch=5 train loss <loss>=3.3645805517832437\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:26:25 INFO 140644862207616] Epoch[12] Batch [5]#011Speed: 319.14 samples/sec#011loss=3.364581\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:26:27 INFO 140644862207616] processed a total of 1183 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1639106782.6080308, \"EndTime\": 1639106787.4605246, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4852.420330047607, \"count\": 1, \"min\": 4852.420330047607, \"max\": 4852.420330047607}}}\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:26:27 INFO 140644862207616] #throughput_metric: host=algo-1, train throughput=243.7899812101567 records/second\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:26:27 INFO 140644862207616] #progress_metric: host=algo-1, completed 6.5 % of epochs\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:26:27 INFO 140644862207616] #quality_metric: host=algo-1, epoch=12, train loss <loss>=3.639940118789673\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:26:27 INFO 140644862207616] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:26:28 INFO 140644862207616] Epoch[13] Batch[0] avg_epoch_loss=3.833747\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:26:28 INFO 140644862207616] #quality_metric: host=algo-1, epoch=13, batch=0 train loss <loss>=3.833746910095215\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:26:30 INFO 140644862207616] Epoch[13] Batch[5] avg_epoch_loss=3.534070\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:26:30 INFO 140644862207616] #quality_metric: host=algo-1, epoch=13, batch=5 train loss <loss>=3.534070452054342\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:26:30 INFO 140644862207616] Epoch[13] Batch [5]#011Speed: 319.42 samples/sec#011loss=3.534070\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:26:32 INFO 140644862207616] processed a total of 1249 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1639106787.4606009, \"EndTime\": 1639106792.2907448, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4829.754829406738, \"count\": 1, \"min\": 4829.754829406738, \"max\": 4829.754829406738}}}\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:26:32 INFO 140644862207616] #throughput_metric: host=algo-1, train throughput=258.599425399956 records/second\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:26:32 INFO 140644862207616] #progress_metric: host=algo-1, completed 7.0 % of epochs\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:26:32 INFO 140644862207616] #quality_metric: host=algo-1, epoch=13, train loss <loss>=3.538996148109436\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:26:32 INFO 140644862207616] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:26:32 INFO 140644862207616] Saved checkpoint to \"/opt/ml/model/state_5ee73c5d-5163-48ad-a302-0365ed433f6c-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1639106792.2908218, \"EndTime\": 1639106792.352273, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 61.037540435791016, \"count\": 1, \"min\": 61.037540435791016, \"max\": 61.037540435791016}}}\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:26:33 INFO 140644862207616] Epoch[14] Batch[0] avg_epoch_loss=3.845946\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:26:33 INFO 140644862207616] #quality_metric: host=algo-1, epoch=14, batch=0 train loss <loss>=3.8459455966949463\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:26:35 INFO 140644862207616] Epoch[14] Batch[5] avg_epoch_loss=3.634121\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:26:35 INFO 140644862207616] #quality_metric: host=algo-1, epoch=14, batch=5 train loss <loss>=3.6341205835342407\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:26:35 INFO 140644862207616] Epoch[14] Batch [5]#011Speed: 308.54 samples/sec#011loss=3.634121\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:26:37 INFO 140644862207616] processed a total of 1243 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1639106792.3523457, \"EndTime\": 1639106797.2942135, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4941.805601119995, \"count\": 1, \"min\": 4941.805601119995, \"max\": 4941.805601119995}}}\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:26:37 INFO 140644862207616] #throughput_metric: host=algo-1, train throughput=251.52191633545036 records/second\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:26:37 INFO 140644862207616] #progress_metric: host=algo-1, completed 7.5 % of epochs\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:26:37 INFO 140644862207616] #quality_metric: host=algo-1, epoch=14, train loss <loss>=3.6975754737854003\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:26:37 INFO 140644862207616] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:26:38 INFO 140644862207616] Epoch[15] Batch[0] avg_epoch_loss=3.538387\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:26:38 INFO 140644862207616] #quality_metric: host=algo-1, epoch=15, batch=0 train loss <loss>=3.5383872985839844\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:26:40 INFO 140644862207616] Epoch[15] Batch[5] avg_epoch_loss=3.457384\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:26:40 INFO 140644862207616] #quality_metric: host=algo-1, epoch=15, batch=5 train loss <loss>=3.45738418896993\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:26:40 INFO 140644862207616] Epoch[15] Batch [5]#011Speed: 302.49 samples/sec#011loss=3.457384\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:26:42 INFO 140644862207616] processed a total of 1266 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1639106797.294291, \"EndTime\": 1639106802.3018742, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5007.134914398193, \"count\": 1, \"min\": 5007.134914398193, \"max\": 5007.134914398193}}}\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:26:42 INFO 140644862207616] #throughput_metric: host=algo-1, train throughput=252.83373713806847 records/second\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:26:42 INFO 140644862207616] #progress_metric: host=algo-1, completed 8.0 % of epochs\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:26:42 INFO 140644862207616] #quality_metric: host=algo-1, epoch=15, train loss <loss>=3.4622400522232057\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:26:42 INFO 140644862207616] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:26:42 INFO 140644862207616] Saved checkpoint to \"/opt/ml/model/state_b9ecaf0b-c80d-417d-9ef7-df9cf3667557-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1639106802.301951, \"EndTime\": 1639106802.3532712, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 50.91595649719238, \"count\": 1, \"min\": 50.91595649719238, \"max\": 50.91595649719238}}}\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:26:43 INFO 140644862207616] Epoch[16] Batch[0] avg_epoch_loss=3.358185\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:26:43 INFO 140644862207616] #quality_metric: host=algo-1, epoch=16, batch=0 train loss <loss>=3.358185291290283\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:26:45 INFO 140644862207616] Epoch[16] Batch[5] avg_epoch_loss=3.465692\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:26:45 INFO 140644862207616] #quality_metric: host=algo-1, epoch=16, batch=5 train loss <loss>=3.4656918048858643\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:26:45 INFO 140644862207616] Epoch[16] Batch [5]#011Speed: 316.02 samples/sec#011loss=3.465692\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:26:47 INFO 140644862207616] Epoch[16] Batch[10] avg_epoch_loss=3.657742\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:26:47 INFO 140644862207616] #quality_metric: host=algo-1, epoch=16, batch=10 train loss <loss>=3.888201951980591\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:26:47 INFO 140644862207616] Epoch[16] Batch [10]#011Speed: 319.13 samples/sec#011loss=3.888202\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:26:47 INFO 140644862207616] processed a total of 1284 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1639106802.3533428, \"EndTime\": 1639106807.6176496, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5264.192342758179, \"count\": 1, \"min\": 5264.192342758179, \"max\": 5264.192342758179}}}\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:26:47 INFO 140644862207616] #throughput_metric: host=algo-1, train throughput=243.90765262824468 records/second\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:26:47 INFO 140644862207616] #progress_metric: host=algo-1, completed 8.5 % of epochs\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:26:47 INFO 140644862207616] #quality_metric: host=algo-1, epoch=16, train loss <loss>=3.6577418717471035\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:26:47 INFO 140644862207616] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:26:48 INFO 140644862207616] Epoch[17] Batch[0] avg_epoch_loss=3.819834\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:26:48 INFO 140644862207616] #quality_metric: host=algo-1, epoch=17, batch=0 train loss <loss>=3.8198342323303223\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:26:50 INFO 140644862207616] Epoch[17] Batch[5] avg_epoch_loss=3.452714\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:26:50 INFO 140644862207616] #quality_metric: host=algo-1, epoch=17, batch=5 train loss <loss>=3.4527142445246377\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:26:50 INFO 140644862207616] Epoch[17] Batch [5]#011Speed: 302.26 samples/sec#011loss=3.452714\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:26:52 INFO 140644862207616] Epoch[17] Batch[10] avg_epoch_loss=3.298752\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:26:52 INFO 140644862207616] #quality_metric: host=algo-1, epoch=17, batch=10 train loss <loss>=3.1139971733093263\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:26:52 INFO 140644862207616] Epoch[17] Batch [10]#011Speed: 315.80 samples/sec#011loss=3.113997\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:26:52 INFO 140644862207616] processed a total of 1311 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1639106807.617713, \"EndTime\": 1639106812.986725, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5368.624925613403, \"count\": 1, \"min\": 5368.624925613403, \"max\": 5368.624925613403}}}\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:26:52 INFO 140644862207616] #throughput_metric: host=algo-1, train throughput=244.19205397977134 records/second\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:26:52 INFO 140644862207616] #progress_metric: host=algo-1, completed 9.0 % of epochs\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:26:52 INFO 140644862207616] #quality_metric: host=algo-1, epoch=17, train loss <loss>=3.298751939426769\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:26:52 INFO 140644862207616] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:26:53 INFO 140644862207616] Saved checkpoint to \"/opt/ml/model/state_a9410655-92e0-48d5-b873-b94ac172bee3-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1639106812.9867942, \"EndTime\": 1639106813.0370796, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 49.910783767700195, \"count\": 1, \"min\": 49.910783767700195, \"max\": 49.910783767700195}}}\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:26:54 INFO 140644862207616] Epoch[18] Batch[0] avg_epoch_loss=3.592799\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:26:54 INFO 140644862207616] #quality_metric: host=algo-1, epoch=18, batch=0 train loss <loss>=3.592799186706543\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:26:56 INFO 140644862207616] Epoch[18] Batch[5] avg_epoch_loss=3.351807\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:26:56 INFO 140644862207616] #quality_metric: host=algo-1, epoch=18, batch=5 train loss <loss>=3.351807395617167\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:26:56 INFO 140644862207616] Epoch[18] Batch [5]#011Speed: 315.06 samples/sec#011loss=3.351807\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:26:57 INFO 140644862207616] processed a total of 1246 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1639106813.0371706, \"EndTime\": 1639106817.9381967, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4900.9599685668945, \"count\": 1, \"min\": 4900.9599685668945, \"max\": 4900.9599685668945}}}\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:26:57 INFO 140644862207616] #throughput_metric: host=algo-1, train throughput=254.2301678586613 records/second\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:26:57 INFO 140644862207616] #progress_metric: host=algo-1, completed 9.5 % of epochs\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:26:57 INFO 140644862207616] #quality_metric: host=algo-1, epoch=18, train loss <loss>=3.2736857175827025\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:26:57 INFO 140644862207616] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:26:57 INFO 140644862207616] Saved checkpoint to \"/opt/ml/model/state_69fa1592-e797-47df-9e1a-cf799e0bbdb4-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1639106817.9382734, \"EndTime\": 1639106817.979035, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 40.30203819274902, \"count\": 1, \"min\": 40.30203819274902, \"max\": 40.30203819274902}}}\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:26:59 INFO 140644862207616] Epoch[19] Batch[0] avg_epoch_loss=3.366686\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:26:59 INFO 140644862207616] #quality_metric: host=algo-1, epoch=19, batch=0 train loss <loss>=3.366685628890991\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:27:01 INFO 140644862207616] Epoch[19] Batch[5] avg_epoch_loss=3.284644\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:27:01 INFO 140644862207616] #quality_metric: host=algo-1, epoch=19, batch=5 train loss <loss>=3.2846439282099404\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:27:01 INFO 140644862207616] Epoch[19] Batch [5]#011Speed: 319.36 samples/sec#011loss=3.284644\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:27:02 INFO 140644862207616] processed a total of 1217 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1639106817.9790878, \"EndTime\": 1639106822.9437273, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4964.578866958618, \"count\": 1, \"min\": 4964.578866958618, \"max\": 4964.578866958618}}}\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:27:02 INFO 140644862207616] #throughput_metric: host=algo-1, train throughput=245.13064654628224 records/second\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:27:02 INFO 140644862207616] #progress_metric: host=algo-1, completed 10.0 % of epochs\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:27:02 INFO 140644862207616] #quality_metric: host=algo-1, epoch=19, train loss <loss>=3.352967548370361\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:27:02 INFO 140644862207616] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:27:04 INFO 140644862207616] Epoch[20] Batch[0] avg_epoch_loss=3.196907\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:27:04 INFO 140644862207616] #quality_metric: host=algo-1, epoch=20, batch=0 train loss <loss>=3.196906566619873\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:27:06 INFO 140644862207616] Epoch[20] Batch[5] avg_epoch_loss=3.272509\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:27:06 INFO 140644862207616] #quality_metric: host=algo-1, epoch=20, batch=5 train loss <loss>=3.2725090980529785\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:27:06 INFO 140644862207616] Epoch[20] Batch [5]#011Speed: 286.53 samples/sec#011loss=3.272509\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:27:08 INFO 140644862207616] processed a total of 1262 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1639106822.9438176, \"EndTime\": 1639106828.0204566, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5076.23291015625, \"count\": 1, \"min\": 5076.23291015625, \"max\": 5076.23291015625}}}\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:27:08 INFO 140644862207616] #throughput_metric: host=algo-1, train throughput=248.60481343173174 records/second\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:27:08 INFO 140644862207616] #progress_metric: host=algo-1, completed 10.5 % of epochs\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:27:08 INFO 140644862207616] #quality_metric: host=algo-1, epoch=20, train loss <loss>=3.2517146348953245\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:27:08 INFO 140644862207616] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:27:08 INFO 140644862207616] Saved checkpoint to \"/opt/ml/model/state_2d0f171d-8660-4fd6-85ce-71614836c94c-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1639106828.0205235, \"EndTime\": 1639106828.0609498, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 40.007591247558594, \"count\": 1, \"min\": 40.007591247558594, \"max\": 40.007591247558594}}}\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:27:09 INFO 140644862207616] Epoch[21] Batch[0] avg_epoch_loss=3.437036\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:27:09 INFO 140644862207616] #quality_metric: host=algo-1, epoch=21, batch=0 train loss <loss>=3.43703556060791\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:27:11 INFO 140644862207616] Epoch[21] Batch[5] avg_epoch_loss=3.261966\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:27:11 INFO 140644862207616] #quality_metric: host=algo-1, epoch=21, batch=5 train loss <loss>=3.2619659900665283\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:27:11 INFO 140644862207616] Epoch[21] Batch [5]#011Speed: 304.76 samples/sec#011loss=3.261966\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:27:13 INFO 140644862207616] Epoch[21] Batch[10] avg_epoch_loss=3.455627\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:27:13 INFO 140644862207616] #quality_metric: host=algo-1, epoch=21, batch=10 train loss <loss>=3.6880197525024414\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:27:13 INFO 140644862207616] Epoch[21] Batch [10]#011Speed: 322.31 samples/sec#011loss=3.688020\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:27:13 INFO 140644862207616] processed a total of 1307 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1639106828.0610044, \"EndTime\": 1639106833.35335, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5292.2844886779785, \"count\": 1, \"min\": 5292.2844886779785, \"max\": 5292.2844886779785}}}\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:27:13 INFO 140644862207616] #throughput_metric: host=algo-1, train throughput=246.95853036042453 records/second\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:27:13 INFO 140644862207616] #progress_metric: host=algo-1, completed 11.0 % of epochs\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:27:13 INFO 140644862207616] #quality_metric: host=algo-1, epoch=21, train loss <loss>=3.4556267911737617\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:27:13 INFO 140644862207616] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:27:14 INFO 140644862207616] Epoch[22] Batch[0] avg_epoch_loss=3.346833\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:27:14 INFO 140644862207616] #quality_metric: host=algo-1, epoch=22, batch=0 train loss <loss>=3.3468332290649414\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:27:16 INFO 140644862207616] Epoch[22] Batch[5] avg_epoch_loss=3.380473\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:27:16 INFO 140644862207616] #quality_metric: host=algo-1, epoch=22, batch=5 train loss <loss>=3.380472739537557\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:27:16 INFO 140644862207616] Epoch[22] Batch [5]#011Speed: 325.07 samples/sec#011loss=3.380473\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:27:18 INFO 140644862207616] Epoch[22] Batch[10] avg_epoch_loss=3.411615\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:27:18 INFO 140644862207616] #quality_metric: host=algo-1, epoch=22, batch=10 train loss <loss>=3.4489859104156495\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:27:18 INFO 140644862207616] Epoch[22] Batch [10]#011Speed: 319.79 samples/sec#011loss=3.448986\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:27:18 INFO 140644862207616] processed a total of 1293 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1639106833.353421, \"EndTime\": 1639106838.63068, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5276.853322982788, \"count\": 1, \"min\": 5276.853322982788, \"max\": 5276.853322982788}}}\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:27:18 INFO 140644862207616] #throughput_metric: host=algo-1, train throughput=245.02786554964467 records/second\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:27:18 INFO 140644862207616] #progress_metric: host=algo-1, completed 11.5 % of epochs\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:27:18 INFO 140644862207616] #quality_metric: host=algo-1, epoch=22, train loss <loss>=3.41161508993669\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:27:18 INFO 140644862207616] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:27:19 INFO 140644862207616] Epoch[23] Batch[0] avg_epoch_loss=3.010439\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:27:19 INFO 140644862207616] #quality_metric: host=algo-1, epoch=23, batch=0 train loss <loss>=3.010438919067383\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:27:21 INFO 140644862207616] Epoch[23] Batch[5] avg_epoch_loss=3.254382\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:27:21 INFO 140644862207616] #quality_metric: host=algo-1, epoch=23, batch=5 train loss <loss>=3.2543818950653076\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:27:21 INFO 140644862207616] Epoch[23] Batch [5]#011Speed: 325.78 samples/sec#011loss=3.254382\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:27:23 INFO 140644862207616] processed a total of 1257 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1639106838.6307483, \"EndTime\": 1639106843.4258852, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4794.7962284088135, \"count\": 1, \"min\": 4794.7962284088135, \"max\": 4794.7962284088135}}}\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:27:23 INFO 140644862207616] #throughput_metric: host=algo-1, train throughput=262.153163368827 records/second\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:27:23 INFO 140644862207616] #progress_metric: host=algo-1, completed 12.0 % of epochs\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:27:23 INFO 140644862207616] #quality_metric: host=algo-1, epoch=23, train loss <loss>=3.2387619256973266\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:27:23 INFO 140644862207616] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:27:23 INFO 140644862207616] Saved checkpoint to \"/opt/ml/model/state_539e36a6-0035-4879-9b6d-3b04824b0d89-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1639106843.4259624, \"EndTime\": 1639106843.4795673, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 53.20334434509277, \"count\": 1, \"min\": 53.20334434509277, \"max\": 53.20334434509277}}}\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:27:24 INFO 140644862207616] Epoch[24] Batch[0] avg_epoch_loss=3.495284\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:27:24 INFO 140644862207616] #quality_metric: host=algo-1, epoch=24, batch=0 train loss <loss>=3.49528431892395\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:27:26 INFO 140644862207616] Epoch[24] Batch[5] avg_epoch_loss=3.342714\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:27:26 INFO 140644862207616] #quality_metric: host=algo-1, epoch=24, batch=5 train loss <loss>=3.3427135149637857\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:27:26 INFO 140644862207616] Epoch[24] Batch [5]#011Speed: 328.72 samples/sec#011loss=3.342714\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:27:28 INFO 140644862207616] processed a total of 1222 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1639106843.479627, \"EndTime\": 1639106848.1751142, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4695.423364639282, \"count\": 1, \"min\": 4695.423364639282, \"max\": 4695.423364639282}}}\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:27:28 INFO 140644862207616] #throughput_metric: host=algo-1, train throughput=260.2472379472828 records/second\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:27:28 INFO 140644862207616] #progress_metric: host=algo-1, completed 12.5 % of epochs\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:27:28 INFO 140644862207616] #quality_metric: host=algo-1, epoch=24, train loss <loss>=3.3524336338043215\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:27:28 INFO 140644862207616] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:27:29 INFO 140644862207616] Epoch[25] Batch[0] avg_epoch_loss=3.134145\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:27:29 INFO 140644862207616] #quality_metric: host=algo-1, epoch=25, batch=0 train loss <loss>=3.134145498275757\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:27:31 INFO 140644862207616] Epoch[25] Batch[5] avg_epoch_loss=3.114972\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:27:31 INFO 140644862207616] #quality_metric: host=algo-1, epoch=25, batch=5 train loss <loss>=3.1149718364079795\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:27:31 INFO 140644862207616] Epoch[25] Batch [5]#011Speed: 327.68 samples/sec#011loss=3.114972\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:27:33 INFO 140644862207616] Epoch[25] Batch[10] avg_epoch_loss=3.054010\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:27:33 INFO 140644862207616] #quality_metric: host=algo-1, epoch=25, batch=10 train loss <loss>=2.9808566570281982\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:27:33 INFO 140644862207616] Epoch[25] Batch [10]#011Speed: 328.90 samples/sec#011loss=2.980857\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:27:33 INFO 140644862207616] processed a total of 1328 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1639106848.1751926, \"EndTime\": 1639106853.301043, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5125.403881072998, \"count\": 1, \"min\": 5125.403881072998, \"max\": 5125.403881072998}}}\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:27:33 INFO 140644862207616] #throughput_metric: host=algo-1, train throughput=259.09691637107557 records/second\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:27:33 INFO 140644862207616] #progress_metric: host=algo-1, completed 13.0 % of epochs\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:27:33 INFO 140644862207616] #quality_metric: host=algo-1, epoch=25, train loss <loss>=3.0540103912353516\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:27:33 INFO 140644862207616] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:27:33 INFO 140644862207616] Saved checkpoint to \"/opt/ml/model/state_724e8cbd-f722-4e70-9207-3515aac676a0-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1639106853.301106, \"EndTime\": 1639106853.3410294, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 39.49284553527832, \"count\": 1, \"min\": 39.49284553527832, \"max\": 39.49284553527832}}}\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:27:34 INFO 140644862207616] Epoch[26] Batch[0] avg_epoch_loss=3.069937\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:27:34 INFO 140644862207616] #quality_metric: host=algo-1, epoch=26, batch=0 train loss <loss>=3.069936513900757\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:27:36 INFO 140644862207616] Epoch[26] Batch[5] avg_epoch_loss=3.103718\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:27:36 INFO 140644862207616] #quality_metric: host=algo-1, epoch=26, batch=5 train loss <loss>=3.1037180026372275\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:27:36 INFO 140644862207616] Epoch[26] Batch [5]#011Speed: 332.07 samples/sec#011loss=3.103718\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:27:38 INFO 140644862207616] processed a total of 1230 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1639106853.3410845, \"EndTime\": 1639106858.0073812, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4666.234493255615, \"count\": 1, \"min\": 4666.234493255615, \"max\": 4666.234493255615}}}\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:27:38 INFO 140644862207616] #throughput_metric: host=algo-1, train throughput=263.5901967221072 records/second\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:27:38 INFO 140644862207616] #progress_metric: host=algo-1, completed 13.5 % of epochs\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:27:38 INFO 140644862207616] #quality_metric: host=algo-1, epoch=26, train loss <loss>=3.1701058387756347\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:27:38 INFO 140644862207616] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:27:39 INFO 140644862207616] Epoch[27] Batch[0] avg_epoch_loss=2.971170\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:27:39 INFO 140644862207616] #quality_metric: host=algo-1, epoch=27, batch=0 train loss <loss>=2.971169948577881\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:27:41 INFO 140644862207616] Epoch[27] Batch[5] avg_epoch_loss=3.103615\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:27:41 INFO 140644862207616] #quality_metric: host=algo-1, epoch=27, batch=5 train loss <loss>=3.1036145289738974\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:27:41 INFO 140644862207616] Epoch[27] Batch [5]#011Speed: 330.21 samples/sec#011loss=3.103615\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:27:42 INFO 140644862207616] processed a total of 1246 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1639106858.0074494, \"EndTime\": 1639106862.7817843, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4773.879289627075, \"count\": 1, \"min\": 4773.879289627075, \"max\": 4773.879289627075}}}\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:27:42 INFO 140644862207616] #throughput_metric: host=algo-1, train throughput=260.99768392833244 records/second\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:27:42 INFO 140644862207616] #progress_metric: host=algo-1, completed 14.0 % of epochs\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:27:42 INFO 140644862207616] #quality_metric: host=algo-1, epoch=27, train loss <loss>=3.0572289705276487\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:27:42 INFO 140644862207616] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:27:44 INFO 140644862207616] Epoch[28] Batch[0] avg_epoch_loss=3.046463\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:27:44 INFO 140644862207616] #quality_metric: host=algo-1, epoch=28, batch=0 train loss <loss>=3.0464630126953125\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:27:45 INFO 140644862207616] Epoch[28] Batch[5] avg_epoch_loss=3.018856\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:27:45 INFO 140644862207616] #quality_metric: host=algo-1, epoch=28, batch=5 train loss <loss>=3.018856485684713\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:27:45 INFO 140644862207616] Epoch[28] Batch [5]#011Speed: 324.58 samples/sec#011loss=3.018856\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:27:47 INFO 140644862207616] processed a total of 1219 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1639106862.7818613, \"EndTime\": 1639106867.5313866, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4749.1443157196045, \"count\": 1, \"min\": 4749.1443157196045, \"max\": 4749.1443157196045}}}\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:27:47 INFO 140644862207616] #throughput_metric: host=algo-1, train throughput=256.6729214018158 records/second\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:27:47 INFO 140644862207616] #progress_metric: host=algo-1, completed 14.5 % of epochs\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:27:47 INFO 140644862207616] #quality_metric: host=algo-1, epoch=28, train loss <loss>=3.1958560705184937\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:27:47 INFO 140644862207616] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:27:48 INFO 140644862207616] Epoch[29] Batch[0] avg_epoch_loss=3.075866\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:27:48 INFO 140644862207616] #quality_metric: host=algo-1, epoch=29, batch=0 train loss <loss>=3.0758659839630127\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:27:50 INFO 140644862207616] Epoch[29] Batch[5] avg_epoch_loss=3.125293\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:27:50 INFO 140644862207616] #quality_metric: host=algo-1, epoch=29, batch=5 train loss <loss>=3.125293016433716\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:27:50 INFO 140644862207616] Epoch[29] Batch [5]#011Speed: 332.18 samples/sec#011loss=3.125293\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:27:52 INFO 140644862207616] Epoch[29] Batch[10] avg_epoch_loss=3.085544\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:27:52 INFO 140644862207616] #quality_metric: host=algo-1, epoch=29, batch=10 train loss <loss>=3.0378457069396974\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:27:52 INFO 140644862207616] Epoch[29] Batch [10]#011Speed: 328.18 samples/sec#011loss=3.037846\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:27:52 INFO 140644862207616] processed a total of 1290 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1639106867.5314503, \"EndTime\": 1639106872.6198845, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5087.958574295044, \"count\": 1, \"min\": 5087.958574295044, \"max\": 5087.958574295044}}}\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:27:52 INFO 140644862207616] #throughput_metric: host=algo-1, train throughput=253.5350954538296 records/second\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:27:52 INFO 140644862207616] #progress_metric: host=algo-1, completed 15.0 % of epochs\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:27:52 INFO 140644862207616] #quality_metric: host=algo-1, epoch=29, train loss <loss>=3.08554423939098\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:27:52 INFO 140644862207616] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:27:53 INFO 140644862207616] Epoch[30] Batch[0] avg_epoch_loss=3.025394\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:27:53 INFO 140644862207616] #quality_metric: host=algo-1, epoch=30, batch=0 train loss <loss>=3.0253939628601074\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:27:55 INFO 140644862207616] Epoch[30] Batch[5] avg_epoch_loss=3.112132\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:27:55 INFO 140644862207616] #quality_metric: host=algo-1, epoch=30, batch=5 train loss <loss>=3.112131953239441\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:27:55 INFO 140644862207616] Epoch[30] Batch [5]#011Speed: 333.57 samples/sec#011loss=3.112132\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:27:57 INFO 140644862207616] Epoch[30] Batch[10] avg_epoch_loss=3.106438\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:27:57 INFO 140644862207616] #quality_metric: host=algo-1, epoch=30, batch=10 train loss <loss>=3.0996047496795653\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:27:57 INFO 140644862207616] Epoch[30] Batch [10]#011Speed: 318.96 samples/sec#011loss=3.099605\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:27:57 INFO 140644862207616] processed a total of 1311 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1639106872.6199472, \"EndTime\": 1639106877.7512348, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5130.876541137695, \"count\": 1, \"min\": 5130.876541137695, \"max\": 5130.876541137695}}}\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:27:57 INFO 140644862207616] #throughput_metric: host=algo-1, train throughput=255.50661396565135 records/second\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:27:57 INFO 140644862207616] #progress_metric: host=algo-1, completed 15.5 % of epochs\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:27:57 INFO 140644862207616] #quality_metric: host=algo-1, epoch=30, train loss <loss>=3.106437769803134\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:27:57 INFO 140644862207616] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:27:58 INFO 140644862207616] Epoch[31] Batch[0] avg_epoch_loss=3.834320\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:27:58 INFO 140644862207616] #quality_metric: host=algo-1, epoch=31, batch=0 train loss <loss>=3.834320068359375\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:28:00 INFO 140644862207616] Epoch[31] Batch[5] avg_epoch_loss=3.421638\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:28:00 INFO 140644862207616] #quality_metric: host=algo-1, epoch=31, batch=5 train loss <loss>=3.4216377337773642\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:28:00 INFO 140644862207616] Epoch[31] Batch [5]#011Speed: 333.17 samples/sec#011loss=3.421638\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:28:02 INFO 140644862207616] processed a total of 1253 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1639106877.7512958, \"EndTime\": 1639106882.5594614, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4807.820796966553, \"count\": 1, \"min\": 4807.820796966553, \"max\": 4807.820796966553}}}\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:28:02 INFO 140644862207616] #throughput_metric: host=algo-1, train throughput=260.6111545534974 records/second\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:28:02 INFO 140644862207616] #progress_metric: host=algo-1, completed 16.0 % of epochs\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:28:02 INFO 140644862207616] #quality_metric: host=algo-1, epoch=31, train loss <loss>=3.4029382467269897\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:28:02 INFO 140644862207616] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:28:03 INFO 140644862207616] Epoch[32] Batch[0] avg_epoch_loss=3.323075\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:28:03 INFO 140644862207616] #quality_metric: host=algo-1, epoch=32, batch=0 train loss <loss>=3.323075294494629\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:28:06 INFO 140644862207616] Epoch[32] Batch[5] avg_epoch_loss=3.159954\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:28:06 INFO 140644862207616] #quality_metric: host=algo-1, epoch=32, batch=5 train loss <loss>=3.1599535942077637\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:28:06 INFO 140644862207616] Epoch[32] Batch [5]#011Speed: 282.48 samples/sec#011loss=3.159954\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:28:08 INFO 140644862207616] Epoch[32] Batch[10] avg_epoch_loss=3.347586\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:28:08 INFO 140644862207616] #quality_metric: host=algo-1, epoch=32, batch=10 train loss <loss>=3.5727454662322997\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:28:08 INFO 140644862207616] Epoch[32] Batch [10]#011Speed: 326.49 samples/sec#011loss=3.572745\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:28:08 INFO 140644862207616] processed a total of 1303 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1639106882.5595376, \"EndTime\": 1639106888.1103635, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5550.3973960876465, \"count\": 1, \"min\": 5550.3973960876465, \"max\": 5550.3973960876465}}}\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:28:08 INFO 140644862207616] #throughput_metric: host=algo-1, train throughput=234.75339742386194 records/second\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:28:08 INFO 140644862207616] #progress_metric: host=algo-1, completed 16.5 % of epochs\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:28:08 INFO 140644862207616] #quality_metric: host=algo-1, epoch=32, train loss <loss>=3.3475862633098257\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:28:08 INFO 140644862207616] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:28:09 INFO 140644862207616] Epoch[33] Batch[0] avg_epoch_loss=3.248964\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:28:09 INFO 140644862207616] #quality_metric: host=algo-1, epoch=33, batch=0 train loss <loss>=3.2489638328552246\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:28:11 INFO 140644862207616] Epoch[33] Batch[5] avg_epoch_loss=3.225772\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:28:11 INFO 140644862207616] #quality_metric: host=algo-1, epoch=33, batch=5 train loss <loss>=3.22577166557312\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:28:11 INFO 140644862207616] Epoch[33] Batch [5]#011Speed: 332.98 samples/sec#011loss=3.225772\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:28:13 INFO 140644862207616] Epoch[33] Batch[10] avg_epoch_loss=3.065320\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:28:13 INFO 140644862207616] #quality_metric: host=algo-1, epoch=33, batch=10 train loss <loss>=2.8727773666381835\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:28:13 INFO 140644862207616] Epoch[33] Batch [10]#011Speed: 312.61 samples/sec#011loss=2.872777\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:28:13 INFO 140644862207616] processed a total of 1307 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1639106888.110437, \"EndTime\": 1639106893.289486, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5178.685426712036, \"count\": 1, \"min\": 5178.685426712036, \"max\": 5178.685426712036}}}\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:28:13 INFO 140644862207616] #throughput_metric: host=algo-1, train throughput=252.37589896410864 records/second\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:28:13 INFO 140644862207616] #progress_metric: host=algo-1, completed 17.0 % of epochs\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:28:13 INFO 140644862207616] #quality_metric: host=algo-1, epoch=33, train loss <loss>=3.065319711511785\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:28:13 INFO 140644862207616] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:28:14 INFO 140644862207616] Epoch[34] Batch[0] avg_epoch_loss=2.856102\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:28:14 INFO 140644862207616] #quality_metric: host=algo-1, epoch=34, batch=0 train loss <loss>=2.8561015129089355\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:28:16 INFO 140644862207616] Epoch[34] Batch[5] avg_epoch_loss=3.106218\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:28:16 INFO 140644862207616] #quality_metric: host=algo-1, epoch=34, batch=5 train loss <loss>=3.1062177419662476\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:28:16 INFO 140644862207616] Epoch[34] Batch [5]#011Speed: 327.97 samples/sec#011loss=3.106218\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:28:18 INFO 140644862207616] Epoch[34] Batch[10] avg_epoch_loss=3.114936\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:28:18 INFO 140644862207616] #quality_metric: host=algo-1, epoch=34, batch=10 train loss <loss>=3.125398302078247\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:28:18 INFO 140644862207616] Epoch[34] Batch [10]#011Speed: 323.65 samples/sec#011loss=3.125398\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:28:18 INFO 140644862207616] processed a total of 1333 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1639106893.2895546, \"EndTime\": 1639106898.424301, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5134.403467178345, \"count\": 1, \"min\": 5134.403467178345, \"max\": 5134.403467178345}}}\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:28:18 INFO 140644862207616] #throughput_metric: host=algo-1, train throughput=259.6165486645739 records/second\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:28:18 INFO 140644862207616] #progress_metric: host=algo-1, completed 17.5 % of epochs\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:28:18 INFO 140644862207616] #quality_metric: host=algo-1, epoch=34, train loss <loss>=3.114936178380793\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:28:18 INFO 140644862207616] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:28:19 INFO 140644862207616] Epoch[35] Batch[0] avg_epoch_loss=2.921260\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:28:19 INFO 140644862207616] #quality_metric: host=algo-1, epoch=35, batch=0 train loss <loss>=2.921259880065918\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:28:21 INFO 140644862207616] Epoch[35] Batch[5] avg_epoch_loss=3.013497\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:28:21 INFO 140644862207616] #quality_metric: host=algo-1, epoch=35, batch=5 train loss <loss>=3.01349675655365\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:28:21 INFO 140644862207616] Epoch[35] Batch [5]#011Speed: 334.31 samples/sec#011loss=3.013497\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:28:23 INFO 140644862207616] Epoch[35] Batch[10] avg_epoch_loss=3.031568\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:28:23 INFO 140644862207616] #quality_metric: host=algo-1, epoch=35, batch=10 train loss <loss>=3.0532537937164306\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:28:23 INFO 140644862207616] Epoch[35] Batch [10]#011Speed: 326.68 samples/sec#011loss=3.053254\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:28:23 INFO 140644862207616] processed a total of 1303 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1639106898.4243612, \"EndTime\": 1639106903.497969, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5073.24481010437, \"count\": 1, \"min\": 5073.24481010437, \"max\": 5073.24481010437}}}\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:28:23 INFO 140644862207616] #throughput_metric: host=algo-1, train throughput=256.83294888753835 records/second\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:28:23 INFO 140644862207616] #progress_metric: host=algo-1, completed 18.0 % of epochs\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:28:23 INFO 140644862207616] #quality_metric: host=algo-1, epoch=35, train loss <loss>=3.0315681370821865\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:28:23 INFO 140644862207616] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:28:23 INFO 140644862207616] Saved checkpoint to \"/opt/ml/model/state_aa49077d-5c72-40b9-a4dd-578f86362f15-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1639106903.498032, \"EndTime\": 1639106903.5386055, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 40.148019790649414, \"count\": 1, \"min\": 40.148019790649414, \"max\": 40.148019790649414}}}\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:28:24 INFO 140644862207616] Epoch[36] Batch[0] avg_epoch_loss=2.948022\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:28:24 INFO 140644862207616] #quality_metric: host=algo-1, epoch=36, batch=0 train loss <loss>=2.9480221271514893\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:28:26 INFO 140644862207616] Epoch[36] Batch[5] avg_epoch_loss=2.985357\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:28:26 INFO 140644862207616] #quality_metric: host=algo-1, epoch=36, batch=5 train loss <loss>=2.9853570063908896\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:28:26 INFO 140644862207616] Epoch[36] Batch [5]#011Speed: 334.67 samples/sec#011loss=2.985357\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:28:28 INFO 140644862207616] Epoch[36] Batch[10] avg_epoch_loss=3.277900\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:28:28 INFO 140644862207616] #quality_metric: host=algo-1, epoch=36, batch=10 train loss <loss>=3.62895245552063\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:28:28 INFO 140644862207616] Epoch[36] Batch [10]#011Speed: 332.51 samples/sec#011loss=3.628952\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:28:28 INFO 140644862207616] processed a total of 1300 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1639106903.5386758, \"EndTime\": 1639106908.576163, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5037.426948547363, \"count\": 1, \"min\": 5037.426948547363, \"max\": 5037.426948547363}}}\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:28:28 INFO 140644862207616] #throughput_metric: host=algo-1, train throughput=258.0630065034055 records/second\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:28:28 INFO 140644862207616] #progress_metric: host=algo-1, completed 18.5 % of epochs\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:28:28 INFO 140644862207616] #quality_metric: host=algo-1, epoch=36, train loss <loss>=3.277900392358953\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:28:28 INFO 140644862207616] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:28:29 INFO 140644862207616] Epoch[37] Batch[0] avg_epoch_loss=2.944392\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:28:29 INFO 140644862207616] #quality_metric: host=algo-1, epoch=37, batch=0 train loss <loss>=2.944392442703247\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:28:31 INFO 140644862207616] Epoch[37] Batch[5] avg_epoch_loss=3.137207\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:28:31 INFO 140644862207616] #quality_metric: host=algo-1, epoch=37, batch=5 train loss <loss>=3.137206792831421\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:28:31 INFO 140644862207616] Epoch[37] Batch [5]#011Speed: 327.43 samples/sec#011loss=3.137207\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:28:33 INFO 140644862207616] processed a total of 1245 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1639106908.5762346, \"EndTime\": 1639106913.2598712, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4683.288097381592, \"count\": 1, \"min\": 4683.288097381592, \"max\": 4683.288097381592}}}\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:28:33 INFO 140644862207616] #throughput_metric: host=algo-1, train throughput=265.8323707643614 records/second\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:28:33 INFO 140644862207616] #progress_metric: host=algo-1, completed 19.0 % of epochs\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:28:33 INFO 140644862207616] #quality_metric: host=algo-1, epoch=37, train loss <loss>=3.013427710533142\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:28:33 INFO 140644862207616] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:28:33 INFO 140644862207616] Saved checkpoint to \"/opt/ml/model/state_19efa462-9c1e-4846-a672-52c8b3535d3f-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1639106913.259952, \"EndTime\": 1639106913.300381, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 39.98279571533203, \"count\": 1, \"min\": 39.98279571533203, \"max\": 39.98279571533203}}}\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:28:34 INFO 140644862207616] Epoch[38] Batch[0] avg_epoch_loss=2.983293\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:28:34 INFO 140644862207616] #quality_metric: host=algo-1, epoch=38, batch=0 train loss <loss>=2.983292818069458\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:28:36 INFO 140644862207616] Epoch[38] Batch[5] avg_epoch_loss=2.945152\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:28:36 INFO 140644862207616] #quality_metric: host=algo-1, epoch=38, batch=5 train loss <loss>=2.9451520442962646\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:28:36 INFO 140644862207616] Epoch[38] Batch [5]#011Speed: 338.99 samples/sec#011loss=2.945152\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:28:38 INFO 140644862207616] Epoch[38] Batch[10] avg_epoch_loss=2.934774\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:28:38 INFO 140644862207616] #quality_metric: host=algo-1, epoch=38, batch=10 train loss <loss>=2.922319746017456\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:28:38 INFO 140644862207616] Epoch[38] Batch [10]#011Speed: 326.67 samples/sec#011loss=2.922320\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:28:38 INFO 140644862207616] processed a total of 1303 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1639106913.3004417, \"EndTime\": 1639106918.345688, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5045.180797576904, \"count\": 1, \"min\": 5045.180797576904, \"max\": 5045.180797576904}}}\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:28:38 INFO 140644862207616] #throughput_metric: host=algo-1, train throughput=258.26122434513536 records/second\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:28:38 INFO 140644862207616] #progress_metric: host=algo-1, completed 19.5 % of epochs\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:28:38 INFO 140644862207616] #quality_metric: host=algo-1, epoch=38, train loss <loss>=2.934773726896806\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:28:38 INFO 140644862207616] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:28:38 INFO 140644862207616] Saved checkpoint to \"/opt/ml/model/state_9b2593a6-2395-49d0-ab55-cda2a8122dde-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1639106918.345754, \"EndTime\": 1639106918.3858914, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 39.69264030456543, \"count\": 1, \"min\": 39.69264030456543, \"max\": 39.69264030456543}}}\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:28:39 INFO 140644862207616] Epoch[39] Batch[0] avg_epoch_loss=3.310331\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:28:39 INFO 140644862207616] #quality_metric: host=algo-1, epoch=39, batch=0 train loss <loss>=3.310331106185913\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:28:41 INFO 140644862207616] Epoch[39] Batch[5] avg_epoch_loss=3.160887\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:28:41 INFO 140644862207616] #quality_metric: host=algo-1, epoch=39, batch=5 train loss <loss>=3.1608874003092446\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:28:41 INFO 140644862207616] Epoch[39] Batch [5]#011Speed: 338.24 samples/sec#011loss=3.160887\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:28:43 INFO 140644862207616] processed a total of 1265 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1639106918.385955, \"EndTime\": 1639106923.087369, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4701.348304748535, \"count\": 1, \"min\": 4701.348304748535, \"max\": 4701.348304748535}}}\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:28:43 INFO 140644862207616] #throughput_metric: host=algo-1, train throughput=269.0647876907374 records/second\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:28:43 INFO 140644862207616] #progress_metric: host=algo-1, completed 20.0 % of epochs\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:28:43 INFO 140644862207616] #quality_metric: host=algo-1, epoch=39, train loss <loss>=3.041519355773926\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:28:43 INFO 140644862207616] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:28:44 INFO 140644862207616] Epoch[40] Batch[0] avg_epoch_loss=3.148831\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:28:44 INFO 140644862207616] #quality_metric: host=algo-1, epoch=40, batch=0 train loss <loss>=3.1488311290740967\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:28:46 INFO 140644862207616] Epoch[40] Batch[5] avg_epoch_loss=3.075141\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:28:46 INFO 140644862207616] #quality_metric: host=algo-1, epoch=40, batch=5 train loss <loss>=3.0751412312189736\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:28:46 INFO 140644862207616] Epoch[40] Batch [5]#011Speed: 339.13 samples/sec#011loss=3.075141\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:28:48 INFO 140644862207616] Epoch[40] Batch[10] avg_epoch_loss=2.927035\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:28:48 INFO 140644862207616] #quality_metric: host=algo-1, epoch=40, batch=10 train loss <loss>=2.7493074417114256\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:28:48 INFO 140644862207616] Epoch[40] Batch [10]#011Speed: 332.86 samples/sec#011loss=2.749307\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:28:48 INFO 140644862207616] processed a total of 1283 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1639106923.0874536, \"EndTime\": 1639106928.1036274, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5015.6090259552, \"count\": 1, \"min\": 5015.6090259552, \"max\": 5015.6090259552}}}\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:28:48 INFO 140644862207616] #throughput_metric: host=algo-1, train throughput=255.79684149637168 records/second\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:28:48 INFO 140644862207616] #progress_metric: host=algo-1, completed 20.5 % of epochs\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:28:48 INFO 140644862207616] #quality_metric: host=algo-1, epoch=40, train loss <loss>=2.9270349632609975\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:28:48 INFO 140644862207616] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:28:48 INFO 140644862207616] Saved checkpoint to \"/opt/ml/model/state_1081bdee-7ec0-4e3e-934f-f01271455b77-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1639106928.1036842, \"EndTime\": 1639106928.1439416, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 39.88051414489746, \"count\": 1, \"min\": 39.88051414489746, \"max\": 39.88051414489746}}}\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:28:49 INFO 140644862207616] Epoch[41] Batch[0] avg_epoch_loss=2.794633\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:28:49 INFO 140644862207616] #quality_metric: host=algo-1, epoch=41, batch=0 train loss <loss>=2.794633150100708\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:28:51 INFO 140644862207616] Epoch[41] Batch[5] avg_epoch_loss=3.098235\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:28:51 INFO 140644862207616] #quality_metric: host=algo-1, epoch=41, batch=5 train loss <loss>=3.09823477268219\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:28:51 INFO 140644862207616] Epoch[41] Batch [5]#011Speed: 338.07 samples/sec#011loss=3.098235\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:28:53 INFO 140644862207616] Epoch[41] Batch[10] avg_epoch_loss=3.046211\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:28:53 INFO 140644862207616] #quality_metric: host=algo-1, epoch=41, batch=10 train loss <loss>=2.9837828159332274\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:28:53 INFO 140644862207616] Epoch[41] Batch [10]#011Speed: 329.45 samples/sec#011loss=2.983783\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:28:53 INFO 140644862207616] processed a total of 1300 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1639106928.1440125, \"EndTime\": 1639106933.1704214, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5026.3426303863525, \"count\": 1, \"min\": 5026.3426303863525, \"max\": 5026.3426303863525}}}\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:28:53 INFO 140644862207616] #throughput_metric: host=algo-1, train throughput=258.6324674107436 records/second\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:28:53 INFO 140644862207616] #progress_metric: host=algo-1, completed 21.0 % of epochs\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:28:53 INFO 140644862207616] #quality_metric: host=algo-1, epoch=41, train loss <loss>=3.046211155978116\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:28:53 INFO 140644862207616] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:28:54 INFO 140644862207616] Epoch[42] Batch[0] avg_epoch_loss=2.722338\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:28:54 INFO 140644862207616] #quality_metric: host=algo-1, epoch=42, batch=0 train loss <loss>=2.7223381996154785\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:28:56 INFO 140644862207616] Epoch[42] Batch[5] avg_epoch_loss=2.924553\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:28:56 INFO 140644862207616] #quality_metric: host=algo-1, epoch=42, batch=5 train loss <loss>=2.92455259958903\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:28:56 INFO 140644862207616] Epoch[42] Batch [5]#011Speed: 338.77 samples/sec#011loss=2.924553\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:28:57 INFO 140644862207616] processed a total of 1269 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1639106933.1704853, \"EndTime\": 1639106937.7931995, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4622.355937957764, \"count\": 1, \"min\": 4622.355937957764, \"max\": 4622.355937957764}}}\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:28:57 INFO 140644862207616] #throughput_metric: host=algo-1, train throughput=274.52831802004334 records/second\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:28:57 INFO 140644862207616] #progress_metric: host=algo-1, completed 21.5 % of epochs\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:28:57 INFO 140644862207616] #quality_metric: host=algo-1, epoch=42, train loss <loss>=2.9994357347488405\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:28:57 INFO 140644862207616] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:28:59 INFO 140644862207616] Epoch[43] Batch[0] avg_epoch_loss=3.180218\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:28:59 INFO 140644862207616] #quality_metric: host=algo-1, epoch=43, batch=0 train loss <loss>=3.180217981338501\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:29:00 INFO 140644862207616] Epoch[43] Batch[5] avg_epoch_loss=3.037245\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:29:00 INFO 140644862207616] #quality_metric: host=algo-1, epoch=43, batch=5 train loss <loss>=3.0372445980707803\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:29:00 INFO 140644862207616] Epoch[43] Batch [5]#011Speed: 338.98 samples/sec#011loss=3.037245\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:29:02 INFO 140644862207616] Epoch[43] Batch[10] avg_epoch_loss=2.894368\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:29:02 INFO 140644862207616] #quality_metric: host=algo-1, epoch=43, batch=10 train loss <loss>=2.722915840148926\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:29:02 INFO 140644862207616] Epoch[43] Batch [10]#011Speed: 308.52 samples/sec#011loss=2.722916\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:29:02 INFO 140644862207616] processed a total of 1282 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1639106937.793282, \"EndTime\": 1639106942.976014, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5182.323932647705, \"count\": 1, \"min\": 5182.323932647705, \"max\": 5182.323932647705}}}\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:29:02 INFO 140644862207616] #throughput_metric: host=algo-1, train throughput=247.37484612961552 records/second\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:29:02 INFO 140644862207616] #progress_metric: host=algo-1, completed 22.0 % of epochs\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:29:02 INFO 140644862207616] #quality_metric: host=algo-1, epoch=43, train loss <loss>=2.894367889924483\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:29:02 INFO 140644862207616] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:29:03 INFO 140644862207616] Saved checkpoint to \"/opt/ml/model/state_4f55ee47-9fa8-4ad1-9c85-11e17e3b0179-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1639106942.976077, \"EndTime\": 1639106943.016258, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 39.73793983459473, \"count\": 1, \"min\": 39.73793983459473, \"max\": 39.73793983459473}}}\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:29:04 INFO 140644862207616] Epoch[44] Batch[0] avg_epoch_loss=2.933905\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:29:04 INFO 140644862207616] #quality_metric: host=algo-1, epoch=44, batch=0 train loss <loss>=2.9339051246643066\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:29:06 INFO 140644862207616] Epoch[44] Batch[5] avg_epoch_loss=3.050537\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:29:06 INFO 140644862207616] #quality_metric: host=algo-1, epoch=44, batch=5 train loss <loss>=3.0505365133285522\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:29:06 INFO 140644862207616] Epoch[44] Batch [5]#011Speed: 290.82 samples/sec#011loss=3.050537\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:29:07 INFO 140644862207616] processed a total of 1256 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1639106943.0163162, \"EndTime\": 1639106948.0001206, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4983.748197555542, \"count\": 1, \"min\": 4983.748197555542, \"max\": 4983.748197555542}}}\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:29:08 INFO 140644862207616] #throughput_metric: host=algo-1, train throughput=252.01376400331384 records/second\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:29:08 INFO 140644862207616] #progress_metric: host=algo-1, completed 22.5 % of epochs\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:29:08 INFO 140644862207616] #quality_metric: host=algo-1, epoch=44, train loss <loss>=3.0544371128082277\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:29:08 INFO 140644862207616] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:29:09 INFO 140644862207616] Epoch[45] Batch[0] avg_epoch_loss=2.841993\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:29:09 INFO 140644862207616] #quality_metric: host=algo-1, epoch=45, batch=0 train loss <loss>=2.8419930934906006\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:29:11 INFO 140644862207616] Epoch[45] Batch[5] avg_epoch_loss=2.831617\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:29:11 INFO 140644862207616] #quality_metric: host=algo-1, epoch=45, batch=5 train loss <loss>=2.8316171566645303\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:29:11 INFO 140644862207616] Epoch[45] Batch [5]#011Speed: 331.54 samples/sec#011loss=2.831617\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:29:13 INFO 140644862207616] Epoch[45] Batch[10] avg_epoch_loss=3.067161\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:29:13 INFO 140644862207616] #quality_metric: host=algo-1, epoch=45, batch=10 train loss <loss>=3.349813938140869\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:29:13 INFO 140644862207616] Epoch[45] Batch [10]#011Speed: 322.28 samples/sec#011loss=3.349814\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:29:13 INFO 140644862207616] processed a total of 1315 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1639106948.000197, \"EndTime\": 1639106953.1095307, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5108.814001083374, \"count\": 1, \"min\": 5108.814001083374, \"max\": 5108.814001083374}}}\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:29:13 INFO 140644862207616] #throughput_metric: host=algo-1, train throughput=257.39315123093724 records/second\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:29:13 INFO 140644862207616] #progress_metric: host=algo-1, completed 23.0 % of epochs\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:29:13 INFO 140644862207616] #quality_metric: host=algo-1, epoch=45, train loss <loss>=3.0671611482446846\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:29:13 INFO 140644862207616] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:29:14 INFO 140644862207616] Epoch[46] Batch[0] avg_epoch_loss=2.769230\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:29:14 INFO 140644862207616] #quality_metric: host=algo-1, epoch=46, batch=0 train loss <loss>=2.769230365753174\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:29:16 INFO 140644862207616] Epoch[46] Batch[5] avg_epoch_loss=3.056748\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:29:16 INFO 140644862207616] #quality_metric: host=algo-1, epoch=46, batch=5 train loss <loss>=3.0567477544148765\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:29:16 INFO 140644862207616] Epoch[46] Batch [5]#011Speed: 335.71 samples/sec#011loss=3.056748\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:29:17 INFO 140644862207616] processed a total of 1244 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1639106953.1096015, \"EndTime\": 1639106957.7580254, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4648.070335388184, \"count\": 1, \"min\": 4648.070335388184, \"max\": 4648.070335388184}}}\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:29:17 INFO 140644862207616] #throughput_metric: host=algo-1, train throughput=267.63174152969407 records/second\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:29:17 INFO 140644862207616] #progress_metric: host=algo-1, completed 23.5 % of epochs\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:29:17 INFO 140644862207616] #quality_metric: host=algo-1, epoch=46, train loss <loss>=3.102249765396118\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:29:17 INFO 140644862207616] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:29:18 INFO 140644862207616] Epoch[47] Batch[0] avg_epoch_loss=2.720572\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:29:18 INFO 140644862207616] #quality_metric: host=algo-1, epoch=47, batch=0 train loss <loss>=2.7205724716186523\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:29:20 INFO 140644862207616] Epoch[47] Batch[5] avg_epoch_loss=2.921117\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:29:20 INFO 140644862207616] #quality_metric: host=algo-1, epoch=47, batch=5 train loss <loss>=2.9211167891820273\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:29:20 INFO 140644862207616] Epoch[47] Batch [5]#011Speed: 335.83 samples/sec#011loss=2.921117\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:29:22 INFO 140644862207616] processed a total of 1280 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1639106957.7581005, \"EndTime\": 1639106962.3909783, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4632.420539855957, \"count\": 1, \"min\": 4632.420539855957, \"max\": 4632.420539855957}}}\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:29:22 INFO 140644862207616] #throughput_metric: host=algo-1, train throughput=276.30682970321936 records/second\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:29:22 INFO 140644862207616] #progress_metric: host=algo-1, completed 24.0 % of epochs\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:29:22 INFO 140644862207616] #quality_metric: host=algo-1, epoch=47, train loss <loss>=2.9506286859512327\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:29:22 INFO 140644862207616] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:29:23 INFO 140644862207616] Epoch[48] Batch[0] avg_epoch_loss=2.776744\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:29:23 INFO 140644862207616] #quality_metric: host=algo-1, epoch=48, batch=0 train loss <loss>=2.7767443656921387\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:29:25 INFO 140644862207616] Epoch[48] Batch[5] avg_epoch_loss=2.837340\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:29:25 INFO 140644862207616] #quality_metric: host=algo-1, epoch=48, batch=5 train loss <loss>=2.837340235710144\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:29:25 INFO 140644862207616] Epoch[48] Batch [5]#011Speed: 336.02 samples/sec#011loss=2.837340\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:29:27 INFO 140644862207616] Epoch[48] Batch[10] avg_epoch_loss=2.945632\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:29:27 INFO 140644862207616] #quality_metric: host=algo-1, epoch=48, batch=10 train loss <loss>=3.0755826950073244\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:29:27 INFO 140644862207616] Epoch[48] Batch [10]#011Speed: 329.08 samples/sec#011loss=3.075583\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:29:27 INFO 140644862207616] processed a total of 1322 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1639106962.3910556, \"EndTime\": 1639106967.461659, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5070.2223777771, \"count\": 1, \"min\": 5070.2223777771, \"max\": 5070.2223777771}}}\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:29:27 INFO 140644862207616] #throughput_metric: host=algo-1, train throughput=260.73366900191394 records/second\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:29:27 INFO 140644862207616] #progress_metric: host=algo-1, completed 24.5 % of epochs\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:29:27 INFO 140644862207616] #quality_metric: host=algo-1, epoch=48, train loss <loss>=2.9456322626634077\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:29:27 INFO 140644862207616] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:29:28 INFO 140644862207616] Epoch[49] Batch[0] avg_epoch_loss=3.116330\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:29:28 INFO 140644862207616] #quality_metric: host=algo-1, epoch=49, batch=0 train loss <loss>=3.1163299083709717\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:29:30 INFO 140644862207616] Epoch[49] Batch[5] avg_epoch_loss=2.960311\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:29:30 INFO 140644862207616] #quality_metric: host=algo-1, epoch=49, batch=5 train loss <loss>=2.9603105783462524\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:29:30 INFO 140644862207616] Epoch[49] Batch [5]#011Speed: 337.35 samples/sec#011loss=2.960311\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:29:32 INFO 140644862207616] Epoch[49] Batch[10] avg_epoch_loss=2.884524\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:29:32 INFO 140644862207616] #quality_metric: host=algo-1, epoch=49, batch=10 train loss <loss>=2.793579339981079\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:29:32 INFO 140644862207616] Epoch[49] Batch [10]#011Speed: 333.98 samples/sec#011loss=2.793579\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:29:32 INFO 140644862207616] processed a total of 1294 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1639106967.4617167, \"EndTime\": 1639106972.4723806, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5010.300874710083, \"count\": 1, \"min\": 5010.300874710083, \"max\": 5010.300874710083}}}\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:29:32 INFO 140644862207616] #throughput_metric: host=algo-1, train throughput=258.26297018033864 records/second\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:29:32 INFO 140644862207616] #progress_metric: host=algo-1, completed 25.0 % of epochs\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:29:32 INFO 140644862207616] #quality_metric: host=algo-1, epoch=49, train loss <loss>=2.8845236518166284\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:29:32 INFO 140644862207616] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:29:32 INFO 140644862207616] Saved checkpoint to \"/opt/ml/model/state_5556a3a7-855d-4b27-b222-476ccee99fb1-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1639106972.4724445, \"EndTime\": 1639106972.513377, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 40.58551788330078, \"count\": 1, \"min\": 40.58551788330078, \"max\": 40.58551788330078}}}\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:29:33 INFO 140644862207616] Epoch[50] Batch[0] avg_epoch_loss=2.786537\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:29:33 INFO 140644862207616] #quality_metric: host=algo-1, epoch=50, batch=0 train loss <loss>=2.786536931991577\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:29:35 INFO 140644862207616] Epoch[50] Batch[5] avg_epoch_loss=3.108726\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:29:35 INFO 140644862207616] #quality_metric: host=algo-1, epoch=50, batch=5 train loss <loss>=3.1087258656819663\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:29:35 INFO 140644862207616] Epoch[50] Batch [5]#011Speed: 337.92 samples/sec#011loss=3.108726\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:29:37 INFO 140644862207616] processed a total of 1257 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1639106972.5134358, \"EndTime\": 1639106977.1214855, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4607.987403869629, \"count\": 1, \"min\": 4607.987403869629, \"max\": 4607.987403869629}}}\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:29:37 INFO 140644862207616] #throughput_metric: host=algo-1, train throughput=272.78073990785686 records/second\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:29:37 INFO 140644862207616] #progress_metric: host=algo-1, completed 25.5 % of epochs\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:29:37 INFO 140644862207616] #quality_metric: host=algo-1, epoch=50, train loss <loss>=3.0066377401351927\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:29:37 INFO 140644862207616] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:29:38 INFO 140644862207616] Epoch[51] Batch[0] avg_epoch_loss=2.830272\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:29:38 INFO 140644862207616] #quality_metric: host=algo-1, epoch=51, batch=0 train loss <loss>=2.8302721977233887\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:29:40 INFO 140644862207616] Epoch[51] Batch[5] avg_epoch_loss=2.861555\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:29:40 INFO 140644862207616] #quality_metric: host=algo-1, epoch=51, batch=5 train loss <loss>=2.861555298169454\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:29:40 INFO 140644862207616] Epoch[51] Batch [5]#011Speed: 337.90 samples/sec#011loss=2.861555\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:29:42 INFO 140644862207616] Epoch[51] Batch[10] avg_epoch_loss=2.951683\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:29:42 INFO 140644862207616] #quality_metric: host=algo-1, epoch=51, batch=10 train loss <loss>=3.0598356246948244\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:29:42 INFO 140644862207616] Epoch[51] Batch [10]#011Speed: 326.07 samples/sec#011loss=3.059836\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:29:42 INFO 140644862207616] processed a total of 1329 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1639106977.1215634, \"EndTime\": 1639106982.2183642, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5096.348285675049, \"count\": 1, \"min\": 5096.348285675049, \"max\": 5096.348285675049}}}\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:29:42 INFO 140644862207616] #throughput_metric: host=algo-1, train throughput=260.7701738386733 records/second\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:29:42 INFO 140644862207616] #progress_metric: host=algo-1, completed 26.0 % of epochs\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:29:42 INFO 140644862207616] #quality_metric: host=algo-1, epoch=51, train loss <loss>=2.9516827193173496\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:29:42 INFO 140644862207616] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:29:43 INFO 140644862207616] Epoch[52] Batch[0] avg_epoch_loss=3.038534\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:29:43 INFO 140644862207616] #quality_metric: host=algo-1, epoch=52, batch=0 train loss <loss>=3.0385336875915527\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:29:45 INFO 140644862207616] Epoch[52] Batch[5] avg_epoch_loss=2.866793\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:29:45 INFO 140644862207616] #quality_metric: host=algo-1, epoch=52, batch=5 train loss <loss>=2.866792837778727\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:29:45 INFO 140644862207616] Epoch[52] Batch [5]#011Speed: 324.13 samples/sec#011loss=2.866793\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:29:47 INFO 140644862207616] Epoch[52] Batch[10] avg_epoch_loss=2.745090\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:29:47 INFO 140644862207616] #quality_metric: host=algo-1, epoch=52, batch=10 train loss <loss>=2.5990458011627195\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:29:47 INFO 140644862207616] Epoch[52] Batch [10]#011Speed: 327.12 samples/sec#011loss=2.599046\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:29:47 INFO 140644862207616] processed a total of 1351 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1639106982.2184262, \"EndTime\": 1639106987.312692, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5093.886137008667, \"count\": 1, \"min\": 5093.886137008667, \"max\": 5093.886137008667}}}\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:29:47 INFO 140644862207616] #throughput_metric: host=algo-1, train throughput=265.2144684868232 records/second\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:29:47 INFO 140644862207616] #progress_metric: host=algo-1, completed 26.5 % of epochs\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:29:47 INFO 140644862207616] #quality_metric: host=algo-1, epoch=52, train loss <loss>=2.7450896393169057\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:29:47 INFO 140644862207616] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:29:47 INFO 140644862207616] Saved checkpoint to \"/opt/ml/model/state_cde3509e-7299-407c-9d1f-55d7e61cf0f0-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1639106987.3127666, \"EndTime\": 1639106987.3534129, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 40.22407531738281, \"count\": 1, \"min\": 40.22407531738281, \"max\": 40.22407531738281}}}\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:29:48 INFO 140644862207616] Epoch[53] Batch[0] avg_epoch_loss=2.765671\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:29:48 INFO 140644862207616] #quality_metric: host=algo-1, epoch=53, batch=0 train loss <loss>=2.7656710147857666\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:29:50 INFO 140644862207616] Epoch[53] Batch[5] avg_epoch_loss=2.841456\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:29:50 INFO 140644862207616] #quality_metric: host=algo-1, epoch=53, batch=5 train loss <loss>=2.841455856959025\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:29:50 INFO 140644862207616] Epoch[53] Batch [5]#011Speed: 336.00 samples/sec#011loss=2.841456\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:29:51 INFO 140644862207616] processed a total of 1262 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1639106987.353469, \"EndTime\": 1639106991.9955661, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4642.038822174072, \"count\": 1, \"min\": 4642.038822174072, \"max\": 4642.038822174072}}}\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:29:51 INFO 140644862207616] #throughput_metric: host=algo-1, train throughput=271.8574511103723 records/second\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:29:51 INFO 140644862207616] #progress_metric: host=algo-1, completed 27.0 % of epochs\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:29:51 INFO 140644862207616] #quality_metric: host=algo-1, epoch=53, train loss <loss>=2.800404977798462\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:29:51 INFO 140644862207616] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:29:53 INFO 140644862207616] Epoch[54] Batch[0] avg_epoch_loss=3.163952\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:29:53 INFO 140644862207616] #quality_metric: host=algo-1, epoch=54, batch=0 train loss <loss>=3.163952112197876\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:29:55 INFO 140644862207616] Epoch[54] Batch[5] avg_epoch_loss=2.785885\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:29:55 INFO 140644862207616] #quality_metric: host=algo-1, epoch=54, batch=5 train loss <loss>=2.7858851750691733\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:29:55 INFO 140644862207616] Epoch[54] Batch [5]#011Speed: 335.40 samples/sec#011loss=2.785885\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:29:56 INFO 140644862207616] processed a total of 1241 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1639106991.9956348, \"EndTime\": 1639106996.6440692, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4647.938251495361, \"count\": 1, \"min\": 4647.938251495361, \"max\": 4647.938251495361}}}\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:29:56 INFO 140644862207616] #throughput_metric: host=algo-1, train throughput=266.99365412354757 records/second\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:29:56 INFO 140644862207616] #progress_metric: host=algo-1, completed 27.5 % of epochs\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:29:56 INFO 140644862207616] #quality_metric: host=algo-1, epoch=54, train loss <loss>=2.6993921756744386\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:29:56 INFO 140644862207616] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:29:56 INFO 140644862207616] Saved checkpoint to \"/opt/ml/model/state_282f8ced-2ecd-4400-b252-d4e5105a176f-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1639106996.6441464, \"EndTime\": 1639106996.683981, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 39.36886787414551, \"count\": 1, \"min\": 39.36886787414551, \"max\": 39.36886787414551}}}\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:29:57 INFO 140644862207616] Epoch[55] Batch[0] avg_epoch_loss=2.457088\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:29:57 INFO 140644862207616] #quality_metric: host=algo-1, epoch=55, batch=0 train loss <loss>=2.4570884704589844\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:29:59 INFO 140644862207616] Epoch[55] Batch[5] avg_epoch_loss=2.737096\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:29:59 INFO 140644862207616] #quality_metric: host=algo-1, epoch=55, batch=5 train loss <loss>=2.737095673878988\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:29:59 INFO 140644862207616] Epoch[55] Batch [5]#011Speed: 335.99 samples/sec#011loss=2.737096\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:30:01 INFO 140644862207616] processed a total of 1249 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1639106996.6840403, \"EndTime\": 1639107001.3498392, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4665.736436843872, \"count\": 1, \"min\": 4665.736436843872, \"max\": 4665.736436843872}}}\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:30:01 INFO 140644862207616] #throughput_metric: host=algo-1, train throughput=267.6892831608178 records/second\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:30:01 INFO 140644862207616] #progress_metric: host=algo-1, completed 28.0 % of epochs\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:30:01 INFO 140644862207616] #quality_metric: host=algo-1, epoch=55, train loss <loss>=2.8209531545639037\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:30:01 INFO 140644862207616] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:30:02 INFO 140644862207616] Epoch[56] Batch[0] avg_epoch_loss=2.849060\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:30:02 INFO 140644862207616] #quality_metric: host=algo-1, epoch=56, batch=0 train loss <loss>=2.849060297012329\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:30:04 INFO 140644862207616] Epoch[56] Batch[5] avg_epoch_loss=2.747863\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:30:04 INFO 140644862207616] #quality_metric: host=algo-1, epoch=56, batch=5 train loss <loss>=2.747863292694092\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:30:04 INFO 140644862207616] Epoch[56] Batch [5]#011Speed: 315.05 samples/sec#011loss=2.747863\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:30:06 INFO 140644862207616] Epoch[56] Batch[10] avg_epoch_loss=2.709395\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:30:06 INFO 140644862207616] #quality_metric: host=algo-1, epoch=56, batch=10 train loss <loss>=2.6632323265075684\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:30:06 INFO 140644862207616] Epoch[56] Batch [10]#011Speed: 295.14 samples/sec#011loss=2.663232\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:30:06 INFO 140644862207616] processed a total of 1283 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1639107001.349926, \"EndTime\": 1639107006.7848046, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5434.457302093506, \"count\": 1, \"min\": 5434.457302093506, \"max\": 5434.457302093506}}}\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:30:06 INFO 140644862207616] #throughput_metric: host=algo-1, train throughput=236.08193656176397 records/second\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:30:06 INFO 140644862207616] #progress_metric: host=algo-1, completed 28.5 % of epochs\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:30:06 INFO 140644862207616] #quality_metric: host=algo-1, epoch=56, train loss <loss>=2.7093946717002173\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:30:06 INFO 140644862207616] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:30:07 INFO 140644862207616] Epoch[57] Batch[0] avg_epoch_loss=4.689209\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:30:07 INFO 140644862207616] #quality_metric: host=algo-1, epoch=57, batch=0 train loss <loss>=4.689208984375\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:30:09 INFO 140644862207616] Epoch[57] Batch[5] avg_epoch_loss=3.895056\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:30:09 INFO 140644862207616] #quality_metric: host=algo-1, epoch=57, batch=5 train loss <loss>=3.895055890083313\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:30:09 INFO 140644862207616] Epoch[57] Batch [5]#011Speed: 337.17 samples/sec#011loss=3.895056\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:30:11 INFO 140644862207616] Epoch[57] Batch[10] avg_epoch_loss=3.554381\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:30:11 INFO 140644862207616] #quality_metric: host=algo-1, epoch=57, batch=10 train loss <loss>=3.1455714225769045\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:30:11 INFO 140644862207616] Epoch[57] Batch [10]#011Speed: 333.13 samples/sec#011loss=3.145571\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:30:11 INFO 140644862207616] processed a total of 1294 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1639107006.7848713, \"EndTime\": 1639107011.819161, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5033.93292427063, \"count\": 1, \"min\": 5033.93292427063, \"max\": 5033.93292427063}}}\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:30:11 INFO 140644862207616] #throughput_metric: host=algo-1, train throughput=257.05040664222156 records/second\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:30:11 INFO 140644862207616] #progress_metric: host=algo-1, completed 29.0 % of epochs\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:30:11 INFO 140644862207616] #quality_metric: host=algo-1, epoch=57, train loss <loss>=3.5543811321258545\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:30:11 INFO 140644862207616] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:30:13 INFO 140644862207616] Epoch[58] Batch[0] avg_epoch_loss=3.499859\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:30:13 INFO 140644862207616] #quality_metric: host=algo-1, epoch=58, batch=0 train loss <loss>=3.499858856201172\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:30:15 INFO 140644862207616] Epoch[58] Batch[5] avg_epoch_loss=3.481779\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:30:15 INFO 140644862207616] #quality_metric: host=algo-1, epoch=58, batch=5 train loss <loss>=3.4817787806193032\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:30:15 INFO 140644862207616] Epoch[58] Batch [5]#011Speed: 316.55 samples/sec#011loss=3.481779\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:30:16 INFO 140644862207616] Epoch[58] Batch[10] avg_epoch_loss=3.391056\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:30:16 INFO 140644862207616] #quality_metric: host=algo-1, epoch=58, batch=10 train loss <loss>=3.2821887969970702\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:30:16 INFO 140644862207616] Epoch[58] Batch [10]#011Speed: 330.24 samples/sec#011loss=3.282189\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:30:16 INFO 140644862207616] processed a total of 1313 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1639107011.8192294, \"EndTime\": 1639107016.9844081, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5164.84522819519, \"count\": 1, \"min\": 5164.84522819519, \"max\": 5164.84522819519}}}\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:30:16 INFO 140644862207616] #throughput_metric: host=algo-1, train throughput=254.2141473731052 records/second\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:30:16 INFO 140644862207616] #progress_metric: host=algo-1, completed 29.5 % of epochs\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:30:16 INFO 140644862207616] #quality_metric: host=algo-1, epoch=58, train loss <loss>=3.3910560607910156\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:30:16 INFO 140644862207616] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:30:18 INFO 140644862207616] Epoch[59] Batch[0] avg_epoch_loss=3.391236\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:30:18 INFO 140644862207616] #quality_metric: host=algo-1, epoch=59, batch=0 train loss <loss>=3.391235828399658\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:30:20 INFO 140644862207616] Epoch[59] Batch[5] avg_epoch_loss=3.313993\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:30:20 INFO 140644862207616] #quality_metric: host=algo-1, epoch=59, batch=5 train loss <loss>=3.313993056615194\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:30:20 INFO 140644862207616] Epoch[59] Batch [5]#011Speed: 336.07 samples/sec#011loss=3.313993\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:30:22 INFO 140644862207616] Epoch[59] Batch[10] avg_epoch_loss=3.139604\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:30:22 INFO 140644862207616] #quality_metric: host=algo-1, epoch=59, batch=10 train loss <loss>=2.93033652305603\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:30:22 INFO 140644862207616] Epoch[59] Batch [10]#011Speed: 334.30 samples/sec#011loss=2.930337\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:30:22 INFO 140644862207616] processed a total of 1287 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1639107016.984471, \"EndTime\": 1639107022.005889, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5021.026134490967, \"count\": 1, \"min\": 5021.026134490967, \"max\": 5021.026134490967}}}\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:30:22 INFO 140644862207616] #throughput_metric: host=algo-1, train throughput=256.316910379073 records/second\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:30:22 INFO 140644862207616] #progress_metric: host=algo-1, completed 30.0 % of epochs\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:30:22 INFO 140644862207616] #quality_metric: host=algo-1, epoch=59, train loss <loss>=3.1396037231792104\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:30:22 INFO 140644862207616] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:30:23 INFO 140644862207616] Epoch[60] Batch[0] avg_epoch_loss=2.935441\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:30:23 INFO 140644862207616] #quality_metric: host=algo-1, epoch=60, batch=0 train loss <loss>=2.935441255569458\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:30:25 INFO 140644862207616] Epoch[60] Batch[5] avg_epoch_loss=2.907671\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:30:25 INFO 140644862207616] #quality_metric: host=algo-1, epoch=60, batch=5 train loss <loss>=2.9076706965764365\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:30:25 INFO 140644862207616] Epoch[60] Batch [5]#011Speed: 335.17 samples/sec#011loss=2.907671\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:30:27 INFO 140644862207616] Epoch[60] Batch[10] avg_epoch_loss=2.864301\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:30:27 INFO 140644862207616] #quality_metric: host=algo-1, epoch=60, batch=10 train loss <loss>=2.8122573852539063\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:30:27 INFO 140644862207616] Epoch[60] Batch [10]#011Speed: 328.23 samples/sec#011loss=2.812257\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:30:27 INFO 140644862207616] processed a total of 1300 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1639107022.0059593, \"EndTime\": 1639107027.1007173, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5094.414472579956, \"count\": 1, \"min\": 5094.414472579956, \"max\": 5094.414472579956}}}\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:30:27 INFO 140644862207616] #throughput_metric: host=algo-1, train throughput=255.17620516061407 records/second\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:30:27 INFO 140644862207616] #progress_metric: host=algo-1, completed 30.5 % of epochs\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:30:27 INFO 140644862207616] #quality_metric: host=algo-1, epoch=60, train loss <loss>=2.86430100961165\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:30:27 INFO 140644862207616] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:30:28 INFO 140644862207616] Epoch[61] Batch[0] avg_epoch_loss=2.964468\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:30:28 INFO 140644862207616] #quality_metric: host=algo-1, epoch=61, batch=0 train loss <loss>=2.964467763900757\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:30:30 INFO 140644862207616] Epoch[61] Batch[5] avg_epoch_loss=2.987281\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:30:30 INFO 140644862207616] #quality_metric: host=algo-1, epoch=61, batch=5 train loss <loss>=2.9872809648513794\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:30:30 INFO 140644862207616] Epoch[61] Batch [5]#011Speed: 331.65 samples/sec#011loss=2.987281\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:30:32 INFO 140644862207616] Epoch[61] Batch[10] avg_epoch_loss=2.891921\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:30:32 INFO 140644862207616] #quality_metric: host=algo-1, epoch=61, batch=10 train loss <loss>=2.7774892330169676\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:30:32 INFO 140644862207616] Epoch[61] Batch [10]#011Speed: 324.03 samples/sec#011loss=2.777489\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:30:32 INFO 140644862207616] processed a total of 1335 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1639107027.100791, \"EndTime\": 1639107032.2114172, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5110.201597213745, \"count\": 1, \"min\": 5110.201597213745, \"max\": 5110.201597213745}}}\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:30:32 INFO 140644862207616] #throughput_metric: host=algo-1, train throughput=261.2365820129937 records/second\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:30:32 INFO 140644862207616] #progress_metric: host=algo-1, completed 31.0 % of epochs\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:30:32 INFO 140644862207616] #quality_metric: host=algo-1, epoch=61, train loss <loss>=2.8919210867448286\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:30:32 INFO 140644862207616] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:30:33 INFO 140644862207616] Epoch[62] Batch[0] avg_epoch_loss=2.498414\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:30:33 INFO 140644862207616] #quality_metric: host=algo-1, epoch=62, batch=0 train loss <loss>=2.4984142780303955\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:30:35 INFO 140644862207616] Epoch[62] Batch[5] avg_epoch_loss=2.675337\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:30:35 INFO 140644862207616] #quality_metric: host=algo-1, epoch=62, batch=5 train loss <loss>=2.675336758295695\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:30:35 INFO 140644862207616] Epoch[62] Batch [5]#011Speed: 329.56 samples/sec#011loss=2.675337\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:30:36 INFO 140644862207616] processed a total of 1239 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1639107032.211494, \"EndTime\": 1639107036.9386063, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4726.722955703735, \"count\": 1, \"min\": 4726.722955703735, \"max\": 4726.722955703735}}}\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:30:36 INFO 140644862207616] #throughput_metric: host=algo-1, train throughput=262.1199220955339 records/second\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:30:36 INFO 140644862207616] #progress_metric: host=algo-1, completed 31.5 % of epochs\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:30:36 INFO 140644862207616] #quality_metric: host=algo-1, epoch=62, train loss <loss>=2.885801410675049\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:30:36 INFO 140644862207616] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:30:38 INFO 140644862207616] Epoch[63] Batch[0] avg_epoch_loss=2.867795\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:30:38 INFO 140644862207616] #quality_metric: host=algo-1, epoch=63, batch=0 train loss <loss>=2.8677945137023926\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:30:40 INFO 140644862207616] Epoch[63] Batch[5] avg_epoch_loss=2.892329\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:30:40 INFO 140644862207616] #quality_metric: host=algo-1, epoch=63, batch=5 train loss <loss>=2.8923285802205405\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:30:40 INFO 140644862207616] Epoch[63] Batch [5]#011Speed: 327.69 samples/sec#011loss=2.892329\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:30:41 INFO 140644862207616] processed a total of 1247 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1639107036.9386845, \"EndTime\": 1639107041.6827605, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4743.616580963135, \"count\": 1, \"min\": 4743.616580963135, \"max\": 4743.616580963135}}}\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:30:41 INFO 140644862207616] #throughput_metric: host=algo-1, train throughput=262.8733444129243 records/second\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:30:41 INFO 140644862207616] #progress_metric: host=algo-1, completed 32.0 % of epochs\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:30:41 INFO 140644862207616] #quality_metric: host=algo-1, epoch=63, train loss <loss>=2.8896103620529177\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:30:41 INFO 140644862207616] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:30:42 INFO 140644862207616] Epoch[64] Batch[0] avg_epoch_loss=2.945284\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:30:42 INFO 140644862207616] #quality_metric: host=algo-1, epoch=64, batch=0 train loss <loss>=2.945283889770508\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:30:44 INFO 140644862207616] Epoch[64] Batch[5] avg_epoch_loss=2.894343\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:30:44 INFO 140644862207616] #quality_metric: host=algo-1, epoch=64, batch=5 train loss <loss>=2.8943425019582114\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:30:44 INFO 140644862207616] Epoch[64] Batch [5]#011Speed: 318.04 samples/sec#011loss=2.894343\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:30:46 INFO 140644862207616] processed a total of 1227 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1639107041.6828399, \"EndTime\": 1639107046.5731132, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4889.862060546875, \"count\": 1, \"min\": 4889.862060546875, \"max\": 4889.862060546875}}}\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:30:46 INFO 140644862207616] #throughput_metric: host=algo-1, train throughput=250.91987299768306 records/second\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:30:46 INFO 140644862207616] #progress_metric: host=algo-1, completed 32.5 % of epochs\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:30:46 INFO 140644862207616] #quality_metric: host=algo-1, epoch=64, train loss <loss>=2.8196144819259645\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:30:46 INFO 140644862207616] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:30:47 INFO 140644862207616] Epoch[65] Batch[0] avg_epoch_loss=3.091646\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:30:47 INFO 140644862207616] #quality_metric: host=algo-1, epoch=65, batch=0 train loss <loss>=3.0916457176208496\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:30:49 INFO 140644862207616] Epoch[65] Batch[5] avg_epoch_loss=2.884668\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:30:49 INFO 140644862207616] #quality_metric: host=algo-1, epoch=65, batch=5 train loss <loss>=2.8846676349639893\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:30:49 INFO 140644862207616] Epoch[65] Batch [5]#011Speed: 329.57 samples/sec#011loss=2.884668\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:30:51 INFO 140644862207616] processed a total of 1269 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1639107046.5732245, \"EndTime\": 1639107051.3063831, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4732.748031616211, \"count\": 1, \"min\": 4732.748031616211, \"max\": 4732.748031616211}}}\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:30:51 INFO 140644862207616] #throughput_metric: host=algo-1, train throughput=268.1253855777438 records/second\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:30:51 INFO 140644862207616] #progress_metric: host=algo-1, completed 33.0 % of epochs\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:30:51 INFO 140644862207616] #quality_metric: host=algo-1, epoch=65, train loss <loss>=2.8263205766677855\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:30:51 INFO 140644862207616] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:30:52 INFO 140644862207616] Epoch[66] Batch[0] avg_epoch_loss=2.816180\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:30:52 INFO 140644862207616] #quality_metric: host=algo-1, epoch=66, batch=0 train loss <loss>=2.816180467605591\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:30:54 INFO 140644862207616] Epoch[66] Batch[5] avg_epoch_loss=2.818702\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:30:54 INFO 140644862207616] #quality_metric: host=algo-1, epoch=66, batch=5 train loss <loss>=2.8187015056610107\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:30:54 INFO 140644862207616] Epoch[66] Batch [5]#011Speed: 328.20 samples/sec#011loss=2.818702\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:30:56 INFO 140644862207616] Epoch[66] Batch[10] avg_epoch_loss=2.823756\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:30:56 INFO 140644862207616] #quality_metric: host=algo-1, epoch=66, batch=10 train loss <loss>=2.829820442199707\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:30:56 INFO 140644862207616] Epoch[66] Batch [10]#011Speed: 322.82 samples/sec#011loss=2.829820\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:30:56 INFO 140644862207616] processed a total of 1294 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1639107051.3064613, \"EndTime\": 1639107056.478524, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5171.676874160767, \"count\": 1, \"min\": 5171.676874160767, \"max\": 5171.676874160767}}}\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:30:56 INFO 140644862207616] #throughput_metric: host=algo-1, train throughput=250.20420552296724 records/second\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:30:56 INFO 140644862207616] #progress_metric: host=algo-1, completed 33.5 % of epochs\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:30:56 INFO 140644862207616] #quality_metric: host=algo-1, epoch=66, train loss <loss>=2.8237555677240547\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:30:56 INFO 140644862207616] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:30:57 INFO 140644862207616] Epoch[67] Batch[0] avg_epoch_loss=2.934377\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:30:57 INFO 140644862207616] #quality_metric: host=algo-1, epoch=67, batch=0 train loss <loss>=2.9343769550323486\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:30:59 INFO 140644862207616] Epoch[67] Batch[5] avg_epoch_loss=2.802317\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:30:59 INFO 140644862207616] #quality_metric: host=algo-1, epoch=67, batch=5 train loss <loss>=2.8023173014322915\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:30:59 INFO 140644862207616] Epoch[67] Batch [5]#011Speed: 328.73 samples/sec#011loss=2.802317\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:31:01 INFO 140644862207616] processed a total of 1230 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1639107056.4785933, \"EndTime\": 1639107061.2423456, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4763.39054107666, \"count\": 1, \"min\": 4763.39054107666, \"max\": 4763.39054107666}}}\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:31:01 INFO 140644862207616] #throughput_metric: host=algo-1, train throughput=258.2135644415028 records/second\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:31:01 INFO 140644862207616] #progress_metric: host=algo-1, completed 34.0 % of epochs\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:31:01 INFO 140644862207616] #quality_metric: host=algo-1, epoch=67, train loss <loss>=2.8926497220993044\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:31:01 INFO 140644862207616] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:31:02 INFO 140644862207616] Epoch[68] Batch[0] avg_epoch_loss=2.737196\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:31:02 INFO 140644862207616] #quality_metric: host=algo-1, epoch=68, batch=0 train loss <loss>=2.737196445465088\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:31:04 INFO 140644862207616] Epoch[68] Batch[5] avg_epoch_loss=2.898950\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:31:04 INFO 140644862207616] #quality_metric: host=algo-1, epoch=68, batch=5 train loss <loss>=2.8989495038986206\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:31:04 INFO 140644862207616] Epoch[68] Batch [5]#011Speed: 308.52 samples/sec#011loss=2.898950\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:31:06 INFO 140644862207616] processed a total of 1249 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1639107061.24242, \"EndTime\": 1639107066.4365003, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5193.694353103638, \"count\": 1, \"min\": 5193.694353103638, \"max\": 5193.694353103638}}}\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:31:06 INFO 140644862207616] #throughput_metric: host=algo-1, train throughput=240.47859240293613 records/second\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:31:06 INFO 140644862207616] #progress_metric: host=algo-1, completed 34.5 % of epochs\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:31:06 INFO 140644862207616] #quality_metric: host=algo-1, epoch=68, train loss <loss>=2.81598801612854\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:31:06 INFO 140644862207616] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:31:07 INFO 140644862207616] Epoch[69] Batch[0] avg_epoch_loss=2.621134\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:31:07 INFO 140644862207616] #quality_metric: host=algo-1, epoch=69, batch=0 train loss <loss>=2.621133804321289\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:31:09 INFO 140644862207616] Epoch[69] Batch[5] avg_epoch_loss=2.742491\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:31:09 INFO 140644862207616] #quality_metric: host=algo-1, epoch=69, batch=5 train loss <loss>=2.7424910068511963\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:31:09 INFO 140644862207616] Epoch[69] Batch [5]#011Speed: 330.20 samples/sec#011loss=2.742491\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:31:11 INFO 140644862207616] processed a total of 1256 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1639107066.4365802, \"EndTime\": 1639107071.1330163, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4696.015357971191, \"count\": 1, \"min\": 4696.015357971191, \"max\": 4696.015357971191}}}\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:31:11 INFO 140644862207616] #throughput_metric: host=algo-1, train throughput=267.45446708536366 records/second\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:31:11 INFO 140644862207616] #progress_metric: host=algo-1, completed 35.0 % of epochs\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:31:11 INFO 140644862207616] #quality_metric: host=algo-1, epoch=69, train loss <loss>=2.805725407600403\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:31:11 INFO 140644862207616] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:31:12 INFO 140644862207616] Epoch[70] Batch[0] avg_epoch_loss=2.557006\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:31:12 INFO 140644862207616] #quality_metric: host=algo-1, epoch=70, batch=0 train loss <loss>=2.557006359100342\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:31:14 INFO 140644862207616] Epoch[70] Batch[5] avg_epoch_loss=2.795304\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:31:14 INFO 140644862207616] #quality_metric: host=algo-1, epoch=70, batch=5 train loss <loss>=2.7953035036722818\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:31:14 INFO 140644862207616] Epoch[70] Batch [5]#011Speed: 331.10 samples/sec#011loss=2.795304\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:31:16 INFO 140644862207616] Epoch[70] Batch[10] avg_epoch_loss=2.888870\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:31:16 INFO 140644862207616] #quality_metric: host=algo-1, epoch=70, batch=10 train loss <loss>=3.0011507511138915\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:31:16 INFO 140644862207616] Epoch[70] Batch [10]#011Speed: 308.18 samples/sec#011loss=3.001151\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:31:16 INFO 140644862207616] processed a total of 1285 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1639107071.1330943, \"EndTime\": 1639107076.375801, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5242.28048324585, \"count\": 1, \"min\": 5242.28048324585, \"max\": 5242.28048324585}}}\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:31:16 INFO 140644862207616] #throughput_metric: host=algo-1, train throughput=245.11754628527817 records/second\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:31:16 INFO 140644862207616] #progress_metric: host=algo-1, completed 35.5 % of epochs\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:31:16 INFO 140644862207616] #quality_metric: host=algo-1, epoch=70, train loss <loss>=2.888870434327559\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:31:16 INFO 140644862207616] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:31:17 INFO 140644862207616] Epoch[71] Batch[0] avg_epoch_loss=2.705978\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:31:17 INFO 140644862207616] #quality_metric: host=algo-1, epoch=71, batch=0 train loss <loss>=2.70597767829895\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:31:19 INFO 140644862207616] Epoch[71] Batch[5] avg_epoch_loss=2.681809\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:31:19 INFO 140644862207616] #quality_metric: host=algo-1, epoch=71, batch=5 train loss <loss>=2.681808869043986\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:31:19 INFO 140644862207616] Epoch[71] Batch [5]#011Speed: 329.59 samples/sec#011loss=2.681809\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:31:21 INFO 140644862207616] processed a total of 1221 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1639107076.3758714, \"EndTime\": 1639107081.1132798, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4737.056016921997, \"count\": 1, \"min\": 4737.056016921997, \"max\": 4737.056016921997}}}\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:31:21 INFO 140644862207616] #throughput_metric: host=algo-1, train throughput=257.7493705926829 records/second\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:31:21 INFO 140644862207616] #progress_metric: host=algo-1, completed 36.0 % of epochs\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:31:21 INFO 140644862207616] #quality_metric: host=algo-1, epoch=71, train loss <loss>=2.824466347694397\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:31:21 INFO 140644862207616] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:31:22 INFO 140644862207616] Epoch[72] Batch[0] avg_epoch_loss=2.593701\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:31:22 INFO 140644862207616] #quality_metric: host=algo-1, epoch=72, batch=0 train loss <loss>=2.593700647354126\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:31:24 INFO 140644862207616] Epoch[72] Batch[5] avg_epoch_loss=2.728459\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:31:24 INFO 140644862207616] #quality_metric: host=algo-1, epoch=72, batch=5 train loss <loss>=2.7284586429595947\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:31:24 INFO 140644862207616] Epoch[72] Batch [5]#011Speed: 326.75 samples/sec#011loss=2.728459\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:31:25 INFO 140644862207616] processed a total of 1240 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1639107081.1133516, \"EndTime\": 1639107085.8780274, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4764.22119140625, \"count\": 1, \"min\": 4764.22119140625, \"max\": 4764.22119140625}}}\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:31:25 INFO 140644862207616] #throughput_metric: host=algo-1, train throughput=260.26772420414585 records/second\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:31:25 INFO 140644862207616] #progress_metric: host=algo-1, completed 36.5 % of epochs\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:31:25 INFO 140644862207616] #quality_metric: host=algo-1, epoch=72, train loss <loss>=2.7771270513534545\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:31:25 INFO 140644862207616] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:31:27 INFO 140644862207616] Epoch[73] Batch[0] avg_epoch_loss=2.812073\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:31:27 INFO 140644862207616] #quality_metric: host=algo-1, epoch=73, batch=0 train loss <loss>=2.81207275390625\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:31:29 INFO 140644862207616] Epoch[73] Batch[5] avg_epoch_loss=2.874544\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:31:29 INFO 140644862207616] #quality_metric: host=algo-1, epoch=73, batch=5 train loss <loss>=2.8745442628860474\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:31:29 INFO 140644862207616] Epoch[73] Batch [5]#011Speed: 329.70 samples/sec#011loss=2.874544\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:31:31 INFO 140644862207616] Epoch[73] Batch[10] avg_epoch_loss=2.823672\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:31:31 INFO 140644862207616] #quality_metric: host=algo-1, epoch=73, batch=10 train loss <loss>=2.762624454498291\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:31:31 INFO 140644862207616] Epoch[73] Batch [10]#011Speed: 318.37 samples/sec#011loss=2.762624\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:31:31 INFO 140644862207616] processed a total of 1312 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1639107085.878098, \"EndTime\": 1639107091.0350094, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5156.441688537598, \"count\": 1, \"min\": 5156.441688537598, \"max\": 5156.441688537598}}}\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:31:31 INFO 140644862207616] #throughput_metric: host=algo-1, train throughput=254.43412012917824 records/second\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:31:31 INFO 140644862207616] #progress_metric: host=algo-1, completed 37.0 % of epochs\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:31:31 INFO 140644862207616] #quality_metric: host=algo-1, epoch=73, train loss <loss>=2.8236716227097944\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:31:31 INFO 140644862207616] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:31:32 INFO 140644862207616] Epoch[74] Batch[0] avg_epoch_loss=2.850723\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:31:32 INFO 140644862207616] #quality_metric: host=algo-1, epoch=74, batch=0 train loss <loss>=2.8507230281829834\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:31:34 INFO 140644862207616] Epoch[74] Batch[5] avg_epoch_loss=2.870986\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:31:34 INFO 140644862207616] #quality_metric: host=algo-1, epoch=74, batch=5 train loss <loss>=2.8709857066472373\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:31:34 INFO 140644862207616] Epoch[74] Batch [5]#011Speed: 333.51 samples/sec#011loss=2.870986\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:31:35 INFO 140644862207616] processed a total of 1269 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1639107091.0350769, \"EndTime\": 1639107095.7984903, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4763.070344924927, \"count\": 1, \"min\": 4763.070344924927, \"max\": 4763.070344924927}}}\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:31:35 INFO 140644862207616] #throughput_metric: host=algo-1, train throughput=266.4184787467243 records/second\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:31:35 INFO 140644862207616] #progress_metric: host=algo-1, completed 37.5 % of epochs\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:31:35 INFO 140644862207616] #quality_metric: host=algo-1, epoch=74, train loss <loss>=2.8423484802246093\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:31:35 INFO 140644862207616] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:31:37 INFO 140644862207616] Epoch[75] Batch[0] avg_epoch_loss=2.625130\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:31:37 INFO 140644862207616] #quality_metric: host=algo-1, epoch=75, batch=0 train loss <loss>=2.6251301765441895\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:31:38 INFO 140644862207616] Epoch[75] Batch[5] avg_epoch_loss=2.831625\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:31:38 INFO 140644862207616] #quality_metric: host=algo-1, epoch=75, batch=5 train loss <loss>=2.831625064214071\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:31:38 INFO 140644862207616] Epoch[75] Batch [5]#011Speed: 331.20 samples/sec#011loss=2.831625\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:31:40 INFO 140644862207616] processed a total of 1231 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1639107095.7985682, \"EndTime\": 1639107100.535436, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4736.366271972656, \"count\": 1, \"min\": 4736.366271972656, \"max\": 4736.366271972656}}}\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:31:40 INFO 140644862207616] #throughput_metric: host=algo-1, train throughput=259.8988884706452 records/second\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:31:40 INFO 140644862207616] #progress_metric: host=algo-1, completed 38.0 % of epochs\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:31:40 INFO 140644862207616] #quality_metric: host=algo-1, epoch=75, train loss <loss>=2.7538599014282226\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:31:40 INFO 140644862207616] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:31:41 INFO 140644862207616] Epoch[76] Batch[0] avg_epoch_loss=2.639849\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:31:41 INFO 140644862207616] #quality_metric: host=algo-1, epoch=76, batch=0 train loss <loss>=2.6398487091064453\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:31:43 INFO 140644862207616] Epoch[76] Batch[5] avg_epoch_loss=2.814563\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:31:43 INFO 140644862207616] #quality_metric: host=algo-1, epoch=76, batch=5 train loss <loss>=2.8145628372828164\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:31:43 INFO 140644862207616] Epoch[76] Batch [5]#011Speed: 332.58 samples/sec#011loss=2.814563\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:31:45 INFO 140644862207616] processed a total of 1253 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1639107100.5354993, \"EndTime\": 1639107105.257598, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4721.618890762329, \"count\": 1, \"min\": 4721.618890762329, \"max\": 4721.618890762329}}}\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:31:45 INFO 140644862207616] #throughput_metric: host=algo-1, train throughput=265.3692932007772 records/second\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:31:45 INFO 140644862207616] #progress_metric: host=algo-1, completed 38.5 % of epochs\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:31:45 INFO 140644862207616] #quality_metric: host=algo-1, epoch=76, train loss <loss>=2.849499535560608\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:31:45 INFO 140644862207616] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:31:46 INFO 140644862207616] Epoch[77] Batch[0] avg_epoch_loss=2.760602\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:31:46 INFO 140644862207616] #quality_metric: host=algo-1, epoch=77, batch=0 train loss <loss>=2.7606019973754883\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:31:48 INFO 140644862207616] Epoch[77] Batch[5] avg_epoch_loss=2.684934\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:31:48 INFO 140644862207616] #quality_metric: host=algo-1, epoch=77, batch=5 train loss <loss>=2.684934417406718\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:31:48 INFO 140644862207616] Epoch[77] Batch [5]#011Speed: 334.84 samples/sec#011loss=2.684934\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:31:50 INFO 140644862207616] processed a total of 1272 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1639107105.2576666, \"EndTime\": 1639107110.032067, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4773.945569992065, \"count\": 1, \"min\": 4773.945569992065, \"max\": 4773.945569992065}}}\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:31:50 INFO 140644862207616] #throughput_metric: host=algo-1, train throughput=266.4401510619417 records/second\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:31:50 INFO 140644862207616] #progress_metric: host=algo-1, completed 39.0 % of epochs\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:31:50 INFO 140644862207616] #quality_metric: host=algo-1, epoch=77, train loss <loss>=2.7308326244354246\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:31:50 INFO 140644862207616] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:31:51 INFO 140644862207616] Epoch[78] Batch[0] avg_epoch_loss=2.747983\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:31:51 INFO 140644862207616] #quality_metric: host=algo-1, epoch=78, batch=0 train loss <loss>=2.747983455657959\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:31:53 INFO 140644862207616] Epoch[78] Batch[5] avg_epoch_loss=2.806692\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:31:53 INFO 140644862207616] #quality_metric: host=algo-1, epoch=78, batch=5 train loss <loss>=2.8066916465759277\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:31:53 INFO 140644862207616] Epoch[78] Batch [5]#011Speed: 335.67 samples/sec#011loss=2.806692\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:31:55 INFO 140644862207616] Epoch[78] Batch[10] avg_epoch_loss=2.744085\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:31:55 INFO 140644862207616] #quality_metric: host=algo-1, epoch=78, batch=10 train loss <loss>=2.6689568519592286\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:31:55 INFO 140644862207616] Epoch[78] Batch [10]#011Speed: 319.50 samples/sec#011loss=2.668957\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:31:55 INFO 140644862207616] processed a total of 1312 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1639107110.0321438, \"EndTime\": 1639107115.1738646, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5141.341209411621, \"count\": 1, \"min\": 5141.341209411621, \"max\": 5141.341209411621}}}\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:31:55 INFO 140644862207616] #throughput_metric: host=algo-1, train throughput=255.18179883625723 records/second\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:31:55 INFO 140644862207616] #progress_metric: host=algo-1, completed 39.5 % of epochs\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:31:55 INFO 140644862207616] #quality_metric: host=algo-1, epoch=78, train loss <loss>=2.7440849217501553\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:31:55 INFO 140644862207616] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:31:56 INFO 140644862207616] Epoch[79] Batch[0] avg_epoch_loss=2.903333\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:31:56 INFO 140644862207616] #quality_metric: host=algo-1, epoch=79, batch=0 train loss <loss>=2.9033327102661133\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:31:58 INFO 140644862207616] Epoch[79] Batch[5] avg_epoch_loss=2.786919\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:31:58 INFO 140644862207616] #quality_metric: host=algo-1, epoch=79, batch=5 train loss <loss>=2.786919355392456\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:31:58 INFO 140644862207616] Epoch[79] Batch [5]#011Speed: 334.17 samples/sec#011loss=2.786919\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:32:00 INFO 140644862207616] Epoch[79] Batch[10] avg_epoch_loss=2.718936\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:32:00 INFO 140644862207616] #quality_metric: host=algo-1, epoch=79, batch=10 train loss <loss>=2.6373568773269653\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:32:00 INFO 140644862207616] Epoch[79] Batch [10]#011Speed: 321.58 samples/sec#011loss=2.637357\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:32:00 INFO 140644862207616] processed a total of 1313 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1639107115.1739264, \"EndTime\": 1639107120.2808213, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5106.544494628906, \"count\": 1, \"min\": 5106.544494628906, \"max\": 5106.544494628906}}}\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:32:00 INFO 140644862207616] #throughput_metric: host=algo-1, train throughput=257.1163161248162 records/second\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:32:00 INFO 140644862207616] #progress_metric: host=algo-1, completed 40.0 % of epochs\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:32:00 INFO 140644862207616] #quality_metric: host=algo-1, epoch=79, train loss <loss>=2.718936410817233\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:32:00 INFO 140644862207616] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:32:01 INFO 140644862207616] Epoch[80] Batch[0] avg_epoch_loss=2.716987\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:32:01 INFO 140644862207616] #quality_metric: host=algo-1, epoch=80, batch=0 train loss <loss>=2.716986656188965\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:32:03 INFO 140644862207616] Epoch[80] Batch[5] avg_epoch_loss=2.769178\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:32:03 INFO 140644862207616] #quality_metric: host=algo-1, epoch=80, batch=5 train loss <loss>=2.7691781520843506\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:32:03 INFO 140644862207616] Epoch[80] Batch [5]#011Speed: 327.81 samples/sec#011loss=2.769178\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:32:05 INFO 140644862207616] processed a total of 1272 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1639107120.280885, \"EndTime\": 1639107125.2360818, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4954.789638519287, \"count\": 1, \"min\": 4954.789638519287, \"max\": 4954.789638519287}}}\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:32:05 INFO 140644862207616] #throughput_metric: host=algo-1, train throughput=256.71510372120247 records/second\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:32:05 INFO 140644862207616] #progress_metric: host=algo-1, completed 40.5 % of epochs\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:32:05 INFO 140644862207616] #quality_metric: host=algo-1, epoch=80, train loss <loss>=2.733363628387451\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:32:05 INFO 140644862207616] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:32:06 INFO 140644862207616] Epoch[81] Batch[0] avg_epoch_loss=2.767691\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:32:06 INFO 140644862207616] #quality_metric: host=algo-1, epoch=81, batch=0 train loss <loss>=2.767690658569336\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:32:08 INFO 140644862207616] Epoch[81] Batch[5] avg_epoch_loss=2.751695\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:32:08 INFO 140644862207616] #quality_metric: host=algo-1, epoch=81, batch=5 train loss <loss>=2.751695195833842\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:32:08 INFO 140644862207616] Epoch[81] Batch [5]#011Speed: 334.03 samples/sec#011loss=2.751695\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:32:10 INFO 140644862207616] Epoch[81] Batch[10] avg_epoch_loss=2.759399\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:32:10 INFO 140644862207616] #quality_metric: host=algo-1, epoch=81, batch=10 train loss <loss>=2.768643045425415\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:32:10 INFO 140644862207616] Epoch[81] Batch [10]#011Speed: 320.70 samples/sec#011loss=2.768643\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:32:10 INFO 140644862207616] processed a total of 1302 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1639107125.236165, \"EndTime\": 1639107130.4438038, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5207.113265991211, \"count\": 1, \"min\": 5207.113265991211, \"max\": 5207.113265991211}}}\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:32:10 INFO 140644862207616] #throughput_metric: host=algo-1, train throughput=250.0376045584724 records/second\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:32:10 INFO 140644862207616] #progress_metric: host=algo-1, completed 41.0 % of epochs\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:32:10 INFO 140644862207616] #quality_metric: host=algo-1, epoch=81, train loss <loss>=2.7593987638300117\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:32:10 INFO 140644862207616] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:32:11 INFO 140644862207616] Epoch[82] Batch[0] avg_epoch_loss=2.666842\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:32:11 INFO 140644862207616] #quality_metric: host=algo-1, epoch=82, batch=0 train loss <loss>=2.666842222213745\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:32:13 INFO 140644862207616] Epoch[82] Batch[5] avg_epoch_loss=2.711282\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:32:13 INFO 140644862207616] #quality_metric: host=algo-1, epoch=82, batch=5 train loss <loss>=2.711281736691793\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:32:13 INFO 140644862207616] Epoch[82] Batch [5]#011Speed: 331.95 samples/sec#011loss=2.711282\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:32:15 INFO 140644862207616] processed a total of 1278 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1639107130.4438756, \"EndTime\": 1639107135.18628, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4742.01226234436, \"count\": 1, \"min\": 4742.01226234436, \"max\": 4742.01226234436}}}\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:32:15 INFO 140644862207616] #throughput_metric: host=algo-1, train throughput=269.49936357206883 records/second\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:32:15 INFO 140644862207616] #progress_metric: host=algo-1, completed 41.5 % of epochs\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:32:15 INFO 140644862207616] #quality_metric: host=algo-1, epoch=82, train loss <loss>=2.82944769859314\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:32:15 INFO 140644862207616] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:32:16 INFO 140644862207616] Epoch[83] Batch[0] avg_epoch_loss=2.809121\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:32:16 INFO 140644862207616] #quality_metric: host=algo-1, epoch=83, batch=0 train loss <loss>=2.8091206550598145\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:32:18 INFO 140644862207616] Epoch[83] Batch[5] avg_epoch_loss=2.658113\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:32:18 INFO 140644862207616] #quality_metric: host=algo-1, epoch=83, batch=5 train loss <loss>=2.6581132809321084\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:32:18 INFO 140644862207616] Epoch[83] Batch [5]#011Speed: 325.11 samples/sec#011loss=2.658113\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:32:20 INFO 140644862207616] Epoch[83] Batch[10] avg_epoch_loss=2.634468\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:32:20 INFO 140644862207616] #quality_metric: host=algo-1, epoch=83, batch=10 train loss <loss>=2.606092643737793\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:32:20 INFO 140644862207616] Epoch[83] Batch [10]#011Speed: 319.74 samples/sec#011loss=2.606093\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:32:20 INFO 140644862207616] processed a total of 1326 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1639107135.186359, \"EndTime\": 1639107140.4118214, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5224.998235702515, \"count\": 1, \"min\": 5224.998235702515, \"max\": 5224.998235702515}}}\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:32:20 INFO 140644862207616] #throughput_metric: host=algo-1, train throughput=253.77499908512917 records/second\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:32:20 INFO 140644862207616] #progress_metric: host=algo-1, completed 42.0 % of epochs\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:32:20 INFO 140644862207616] #quality_metric: host=algo-1, epoch=83, train loss <loss>=2.634467536752874\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:32:20 INFO 140644862207616] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:32:20 INFO 140644862207616] Saved checkpoint to \"/opt/ml/model/state_c4dc340e-38c6-4f58-ad73-020199164db5-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1639107140.411892, \"EndTime\": 1639107140.4536684, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 41.318655014038086, \"count\": 1, \"min\": 41.318655014038086, \"max\": 41.318655014038086}}}\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:32:21 INFO 140644862207616] Epoch[84] Batch[0] avg_epoch_loss=2.869727\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:32:21 INFO 140644862207616] #quality_metric: host=algo-1, epoch=84, batch=0 train loss <loss>=2.869727373123169\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:32:23 INFO 140644862207616] Epoch[84] Batch[5] avg_epoch_loss=2.959188\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:32:23 INFO 140644862207616] #quality_metric: host=algo-1, epoch=84, batch=5 train loss <loss>=2.959188461303711\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:32:23 INFO 140644862207616] Epoch[84] Batch [5]#011Speed: 331.14 samples/sec#011loss=2.959188\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:32:25 INFO 140644862207616] processed a total of 1242 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1639107140.453735, \"EndTime\": 1639107145.1508956, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4697.089433670044, \"count\": 1, \"min\": 4697.089433670044, \"max\": 4697.089433670044}}}\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:32:25 INFO 140644862207616] #throughput_metric: host=algo-1, train throughput=264.4126772701587 records/second\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:32:25 INFO 140644862207616] #progress_metric: host=algo-1, completed 42.5 % of epochs\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:32:25 INFO 140644862207616] #quality_metric: host=algo-1, epoch=84, train loss <loss>=2.8050058841705323\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:32:25 INFO 140644862207616] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:32:26 INFO 140644862207616] Epoch[85] Batch[0] avg_epoch_loss=2.899262\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:32:26 INFO 140644862207616] #quality_metric: host=algo-1, epoch=85, batch=0 train loss <loss>=2.8992624282836914\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:32:28 INFO 140644862207616] Epoch[85] Batch[5] avg_epoch_loss=2.865106\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:32:28 INFO 140644862207616] #quality_metric: host=algo-1, epoch=85, batch=5 train loss <loss>=2.8651064236958823\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:32:28 INFO 140644862207616] Epoch[85] Batch [5]#011Speed: 330.69 samples/sec#011loss=2.865106\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:32:30 INFO 140644862207616] Epoch[85] Batch[10] avg_epoch_loss=2.755439\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:32:30 INFO 140644862207616] #quality_metric: host=algo-1, epoch=85, batch=10 train loss <loss>=2.6238377571105955\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:32:30 INFO 140644862207616] Epoch[85] Batch [10]#011Speed: 325.55 samples/sec#011loss=2.623838\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:32:30 INFO 140644862207616] processed a total of 1312 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1639107145.1509743, \"EndTime\": 1639107150.2840245, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5132.579326629639, \"count\": 1, \"min\": 5132.579326629639, \"max\": 5132.579326629639}}}\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:32:30 INFO 140644862207616] #throughput_metric: host=algo-1, train throughput=255.6168448119426 records/second\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:32:30 INFO 140644862207616] #progress_metric: host=algo-1, completed 43.0 % of epochs\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:32:30 INFO 140644862207616] #quality_metric: host=algo-1, epoch=85, train loss <loss>=2.7554388479752974\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:32:30 INFO 140644862207616] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:32:31 INFO 140644862207616] Epoch[86] Batch[0] avg_epoch_loss=2.786154\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:32:31 INFO 140644862207616] #quality_metric: host=algo-1, epoch=86, batch=0 train loss <loss>=2.786153554916382\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:32:33 INFO 140644862207616] Epoch[86] Batch[5] avg_epoch_loss=2.694216\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:32:33 INFO 140644862207616] #quality_metric: host=algo-1, epoch=86, batch=5 train loss <loss>=2.694216251373291\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:32:33 INFO 140644862207616] Epoch[86] Batch [5]#011Speed: 332.68 samples/sec#011loss=2.694216\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:32:34 INFO 140644862207616] processed a total of 1239 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1639107150.2840946, \"EndTime\": 1639107154.9881597, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4703.6614418029785, \"count\": 1, \"min\": 4703.6614418029785, \"max\": 4703.6614418029785}}}\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:32:34 INFO 140644862207616] #throughput_metric: host=algo-1, train throughput=263.40542017576604 records/second\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:32:34 INFO 140644862207616] #progress_metric: host=algo-1, completed 43.5 % of epochs\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:32:34 INFO 140644862207616] #quality_metric: host=algo-1, epoch=86, train loss <loss>=2.7621659994125367\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:32:34 INFO 140644862207616] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:32:36 INFO 140644862207616] Epoch[87] Batch[0] avg_epoch_loss=2.853541\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:32:36 INFO 140644862207616] #quality_metric: host=algo-1, epoch=87, batch=0 train loss <loss>=2.853541135787964\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:32:38 INFO 140644862207616] Epoch[87] Batch[5] avg_epoch_loss=2.711554\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:32:38 INFO 140644862207616] #quality_metric: host=algo-1, epoch=87, batch=5 train loss <loss>=2.7115538517634072\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:32:38 INFO 140644862207616] Epoch[87] Batch [5]#011Speed: 332.67 samples/sec#011loss=2.711554\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:32:39 INFO 140644862207616] processed a total of 1275 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1639107154.9882402, \"EndTime\": 1639107159.7423153, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4753.676652908325, \"count\": 1, \"min\": 4753.676652908325, \"max\": 4753.676652908325}}}\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:32:39 INFO 140644862207616] #throughput_metric: host=algo-1, train throughput=268.2072589533235 records/second\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:32:39 INFO 140644862207616] #progress_metric: host=algo-1, completed 44.0 % of epochs\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:32:39 INFO 140644862207616] #quality_metric: host=algo-1, epoch=87, train loss <loss>=2.7801746129989624\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:32:39 INFO 140644862207616] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:32:40 INFO 140644862207616] Epoch[88] Batch[0] avg_epoch_loss=2.676217\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:32:40 INFO 140644862207616] #quality_metric: host=algo-1, epoch=88, batch=0 train loss <loss>=2.6762170791625977\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:32:42 INFO 140644862207616] Epoch[88] Batch[5] avg_epoch_loss=2.660083\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:32:42 INFO 140644862207616] #quality_metric: host=algo-1, epoch=88, batch=5 train loss <loss>=2.6600834925969443\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:32:42 INFO 140644862207616] Epoch[88] Batch [5]#011Speed: 327.57 samples/sec#011loss=2.660083\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:32:44 INFO 140644862207616] Epoch[88] Batch[10] avg_epoch_loss=2.658177\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:32:44 INFO 140644862207616] #quality_metric: host=algo-1, epoch=88, batch=10 train loss <loss>=2.6558882713317873\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:32:44 INFO 140644862207616] Epoch[88] Batch [10]#011Speed: 328.37 samples/sec#011loss=2.655888\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:32:44 INFO 140644862207616] processed a total of 1293 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1639107159.7423916, \"EndTime\": 1639107164.8746777, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5131.897926330566, \"count\": 1, \"min\": 5131.897926330566, \"max\": 5131.897926330566}}}\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:32:44 INFO 140644862207616] #throughput_metric: host=algo-1, train throughput=251.9489926449228 records/second\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:32:44 INFO 140644862207616] #progress_metric: host=algo-1, completed 44.5 % of epochs\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:32:44 INFO 140644862207616] #quality_metric: host=algo-1, epoch=88, train loss <loss>=2.6581765738400547\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:32:44 INFO 140644862207616] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:32:46 INFO 140644862207616] Epoch[89] Batch[0] avg_epoch_loss=3.413495\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:32:46 INFO 140644862207616] #quality_metric: host=algo-1, epoch=89, batch=0 train loss <loss>=3.413494825363159\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:32:48 INFO 140644862207616] Epoch[89] Batch[5] avg_epoch_loss=3.175514\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:32:48 INFO 140644862207616] #quality_metric: host=algo-1, epoch=89, batch=5 train loss <loss>=3.1755142211914062\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:32:48 INFO 140644862207616] Epoch[89] Batch [5]#011Speed: 323.03 samples/sec#011loss=3.175514\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:32:50 INFO 140644862207616] Epoch[89] Batch[10] avg_epoch_loss=3.199692\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:32:50 INFO 140644862207616] #quality_metric: host=algo-1, epoch=89, batch=10 train loss <loss>=3.228706169128418\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:32:50 INFO 140644862207616] Epoch[89] Batch [10]#011Speed: 319.94 samples/sec#011loss=3.228706\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:32:50 INFO 140644862207616] processed a total of 1350 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1639107164.8747404, \"EndTime\": 1639107170.0640004, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5188.875913619995, \"count\": 1, \"min\": 5188.875913619995, \"max\": 5188.875913619995}}}\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:32:50 INFO 140644862207616] #throughput_metric: host=algo-1, train throughput=260.1669719088372 records/second\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:32:50 INFO 140644862207616] #progress_metric: host=algo-1, completed 45.0 % of epochs\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:32:50 INFO 140644862207616] #quality_metric: host=algo-1, epoch=89, train loss <loss>=3.1996923793445933\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:32:50 INFO 140644862207616] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:32:51 INFO 140644862207616] Epoch[90] Batch[0] avg_epoch_loss=3.620697\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:32:51 INFO 140644862207616] #quality_metric: host=algo-1, epoch=90, batch=0 train loss <loss>=3.620697021484375\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:32:53 INFO 140644862207616] Epoch[90] Batch[5] avg_epoch_loss=3.347401\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:32:53 INFO 140644862207616] #quality_metric: host=algo-1, epoch=90, batch=5 train loss <loss>=3.3474010626475015\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:32:53 INFO 140644862207616] Epoch[90] Batch [5]#011Speed: 317.55 samples/sec#011loss=3.347401\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:32:54 INFO 140644862207616] processed a total of 1257 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1639107170.064069, \"EndTime\": 1639107174.9398715, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4875.4425048828125, \"count\": 1, \"min\": 4875.4425048828125, \"max\": 4875.4425048828125}}}\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:32:54 INFO 140644862207616] #throughput_metric: host=algo-1, train throughput=257.8168381838296 records/second\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:32:54 INFO 140644862207616] #progress_metric: host=algo-1, completed 45.5 % of epochs\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:32:54 INFO 140644862207616] #quality_metric: host=algo-1, epoch=90, train loss <loss>=3.2488863468170166\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:32:54 INFO 140644862207616] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:32:56 INFO 140644862207616] Epoch[91] Batch[0] avg_epoch_loss=3.027820\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:32:56 INFO 140644862207616] #quality_metric: host=algo-1, epoch=91, batch=0 train loss <loss>=3.027820110321045\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:32:58 INFO 140644862207616] Epoch[91] Batch[5] avg_epoch_loss=2.972692\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:32:58 INFO 140644862207616] #quality_metric: host=algo-1, epoch=91, batch=5 train loss <loss>=2.972692330678304\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:32:58 INFO 140644862207616] Epoch[91] Batch [5]#011Speed: 331.70 samples/sec#011loss=2.972692\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:33:00 INFO 140644862207616] Epoch[91] Batch[10] avg_epoch_loss=2.775974\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:33:00 INFO 140644862207616] #quality_metric: host=algo-1, epoch=91, batch=10 train loss <loss>=2.539910912513733\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:33:00 INFO 140644862207616] Epoch[91] Batch [10]#011Speed: 328.08 samples/sec#011loss=2.539911\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:33:00 INFO 140644862207616] processed a total of 1285 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1639107174.9399493, \"EndTime\": 1639107180.1035168, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5163.175344467163, \"count\": 1, \"min\": 5163.175344467163, \"max\": 5163.175344467163}}}\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:33:00 INFO 140644862207616] #throughput_metric: host=algo-1, train throughput=248.87310782893138 records/second\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:33:00 INFO 140644862207616] #progress_metric: host=algo-1, completed 46.0 % of epochs\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:33:00 INFO 140644862207616] #quality_metric: host=algo-1, epoch=91, train loss <loss>=2.7759735042398628\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:33:00 INFO 140644862207616] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:33:01 INFO 140644862207616] Epoch[92] Batch[0] avg_epoch_loss=3.391119\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:33:01 INFO 140644862207616] #quality_metric: host=algo-1, epoch=92, batch=0 train loss <loss>=3.3911185264587402\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:33:03 INFO 140644862207616] Epoch[92] Batch[5] avg_epoch_loss=2.969689\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:33:03 INFO 140644862207616] #quality_metric: host=algo-1, epoch=92, batch=5 train loss <loss>=2.9696888526280723\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:33:03 INFO 140644862207616] Epoch[92] Batch [5]#011Speed: 321.64 samples/sec#011loss=2.969689\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:33:05 INFO 140644862207616] Epoch[92] Batch[10] avg_epoch_loss=2.762696\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:33:05 INFO 140644862207616] #quality_metric: host=algo-1, epoch=92, batch=10 train loss <loss>=2.5143049478530886\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:33:05 INFO 140644862207616] Epoch[92] Batch [10]#011Speed: 287.64 samples/sec#011loss=2.514305\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:33:05 INFO 140644862207616] processed a total of 1289 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1639107180.103585, \"EndTime\": 1639107185.5690558, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5465.083122253418, \"count\": 1, \"min\": 5465.083122253418, \"max\": 5465.083122253418}}}\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:33:05 INFO 140644862207616] #throughput_metric: host=algo-1, train throughput=235.8569308145283 records/second\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:33:05 INFO 140644862207616] #progress_metric: host=algo-1, completed 46.5 % of epochs\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:33:05 INFO 140644862207616] #quality_metric: host=algo-1, epoch=92, train loss <loss>=2.7626961686394433\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:33:05 INFO 140644862207616] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:33:06 INFO 140644862207616] Epoch[93] Batch[0] avg_epoch_loss=2.929134\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:33:06 INFO 140644862207616] #quality_metric: host=algo-1, epoch=93, batch=0 train loss <loss>=2.9291341304779053\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:33:08 INFO 140644862207616] Epoch[93] Batch[5] avg_epoch_loss=2.814907\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:33:08 INFO 140644862207616] #quality_metric: host=algo-1, epoch=93, batch=5 train loss <loss>=2.81490687529246\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:33:08 INFO 140644862207616] Epoch[93] Batch [5]#011Speed: 334.21 samples/sec#011loss=2.814907\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:33:10 INFO 140644862207616] processed a total of 1262 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1639107185.569118, \"EndTime\": 1639107190.361029, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4791.484594345093, \"count\": 1, \"min\": 4791.484594345093, \"max\": 4791.484594345093}}}\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:33:10 INFO 140644862207616] #throughput_metric: host=algo-1, train throughput=263.37803671242517 records/second\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:33:10 INFO 140644862207616] #progress_metric: host=algo-1, completed 47.0 % of epochs\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:33:10 INFO 140644862207616] #quality_metric: host=algo-1, epoch=93, train loss <loss>=2.793295407295227\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:33:10 INFO 140644862207616] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:33:11 INFO 140644862207616] Epoch[94] Batch[0] avg_epoch_loss=2.720610\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:33:11 INFO 140644862207616] #quality_metric: host=algo-1, epoch=94, batch=0 train loss <loss>=2.7206099033355713\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:33:13 INFO 140644862207616] Epoch[94] Batch[5] avg_epoch_loss=2.778036\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:33:13 INFO 140644862207616] #quality_metric: host=algo-1, epoch=94, batch=5 train loss <loss>=2.778036117553711\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:33:13 INFO 140644862207616] Epoch[94] Batch [5]#011Speed: 335.32 samples/sec#011loss=2.778036\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:33:15 INFO 140644862207616] Epoch[94] Batch[10] avg_epoch_loss=2.749465\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:33:15 INFO 140644862207616] #quality_metric: host=algo-1, epoch=94, batch=10 train loss <loss>=2.7151806354522705\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:33:15 INFO 140644862207616] Epoch[94] Batch [10]#011Speed: 323.85 samples/sec#011loss=2.715181\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:33:15 INFO 140644862207616] processed a total of 1316 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1639107190.361104, \"EndTime\": 1639107195.5149145, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5153.392553329468, \"count\": 1, \"min\": 5153.392553329468, \"max\": 5153.392553329468}}}\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:33:15 INFO 140644862207616] #throughput_metric: host=algo-1, train throughput=255.36112770559095 records/second\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:33:15 INFO 140644862207616] #progress_metric: host=algo-1, completed 47.5 % of epochs\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:33:15 INFO 140644862207616] #quality_metric: host=algo-1, epoch=94, train loss <loss>=2.749465443871238\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:33:15 INFO 140644862207616] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:33:16 INFO 140644862207616] Epoch[95] Batch[0] avg_epoch_loss=3.028486\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:33:16 INFO 140644862207616] #quality_metric: host=algo-1, epoch=95, batch=0 train loss <loss>=3.0284855365753174\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:33:18 INFO 140644862207616] Epoch[95] Batch[5] avg_epoch_loss=2.844692\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:33:18 INFO 140644862207616] #quality_metric: host=algo-1, epoch=95, batch=5 train loss <loss>=2.844692349433899\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:33:18 INFO 140644862207616] Epoch[95] Batch [5]#011Speed: 316.12 samples/sec#011loss=2.844692\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:33:20 INFO 140644862207616] Epoch[95] Batch[10] avg_epoch_loss=2.736302\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:33:20 INFO 140644862207616] #quality_metric: host=algo-1, epoch=95, batch=10 train loss <loss>=2.6062337875366213\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:33:20 INFO 140644862207616] Epoch[95] Batch [10]#011Speed: 325.32 samples/sec#011loss=2.606234\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:33:20 INFO 140644862207616] processed a total of 1297 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1639107195.5149794, \"EndTime\": 1639107200.7325509, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5217.15235710144, \"count\": 1, \"min\": 5217.15235710144, \"max\": 5217.15235710144}}}\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:33:20 INFO 140644862207616] #throughput_metric: host=algo-1, train throughput=248.59846252251833 records/second\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:33:20 INFO 140644862207616] #progress_metric: host=algo-1, completed 48.0 % of epochs\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:33:20 INFO 140644862207616] #quality_metric: host=algo-1, epoch=95, train loss <loss>=2.7363020940260454\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:33:20 INFO 140644862207616] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:33:21 INFO 140644862207616] Epoch[96] Batch[0] avg_epoch_loss=3.085433\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:33:21 INFO 140644862207616] #quality_metric: host=algo-1, epoch=96, batch=0 train loss <loss>=3.0854332447052\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:33:23 INFO 140644862207616] Epoch[96] Batch[5] avg_epoch_loss=2.752390\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:33:23 INFO 140644862207616] #quality_metric: host=algo-1, epoch=96, batch=5 train loss <loss>=2.7523899475733438\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:33:23 INFO 140644862207616] Epoch[96] Batch [5]#011Speed: 330.10 samples/sec#011loss=2.752390\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:33:25 INFO 140644862207616] processed a total of 1278 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1639107200.732618, \"EndTime\": 1639107205.4465277, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4713.490009307861, \"count\": 1, \"min\": 4713.490009307861, \"max\": 4713.490009307861}}}\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:33:25 INFO 140644862207616] #throughput_metric: host=algo-1, train throughput=271.13088566689555 records/second\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:33:25 INFO 140644862207616] #progress_metric: host=algo-1, completed 48.5 % of epochs\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:33:25 INFO 140644862207616] #quality_metric: host=algo-1, epoch=96, train loss <loss>=2.754698657989502\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:33:25 INFO 140644862207616] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:33:26 INFO 140644862207616] Epoch[97] Batch[0] avg_epoch_loss=2.913795\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:33:26 INFO 140644862207616] #quality_metric: host=algo-1, epoch=97, batch=0 train loss <loss>=2.913794755935669\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:33:28 INFO 140644862207616] Epoch[97] Batch[5] avg_epoch_loss=2.813533\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:33:28 INFO 140644862207616] #quality_metric: host=algo-1, epoch=97, batch=5 train loss <loss>=2.813533306121826\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:33:28 INFO 140644862207616] Epoch[97] Batch [5]#011Speed: 334.00 samples/sec#011loss=2.813533\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:33:30 INFO 140644862207616] Epoch[97] Batch[10] avg_epoch_loss=2.662296\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:33:30 INFO 140644862207616] #quality_metric: host=algo-1, epoch=97, batch=10 train loss <loss>=2.480810356140137\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:33:30 INFO 140644862207616] Epoch[97] Batch [10]#011Speed: 327.69 samples/sec#011loss=2.480810\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:33:30 INFO 140644862207616] processed a total of 1307 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1639107205.4465973, \"EndTime\": 1639107210.5646682, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5117.638111114502, \"count\": 1, \"min\": 5117.638111114502, \"max\": 5117.638111114502}}}\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:33:30 INFO 140644862207616] #throughput_metric: host=algo-1, train throughput=255.3860993638915 records/second\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:33:30 INFO 140644862207616] #progress_metric: host=algo-1, completed 49.0 % of epochs\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:33:30 INFO 140644862207616] #quality_metric: host=algo-1, epoch=97, train loss <loss>=2.662295601584695\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:33:30 INFO 140644862207616] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:33:31 INFO 140644862207616] Epoch[98] Batch[0] avg_epoch_loss=3.080527\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:33:31 INFO 140644862207616] #quality_metric: host=algo-1, epoch=98, batch=0 train loss <loss>=3.08052659034729\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:33:33 INFO 140644862207616] Epoch[98] Batch[5] avg_epoch_loss=2.976202\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:33:33 INFO 140644862207616] #quality_metric: host=algo-1, epoch=98, batch=5 train loss <loss>=2.976202368736267\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:33:33 INFO 140644862207616] Epoch[98] Batch [5]#011Speed: 332.40 samples/sec#011loss=2.976202\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:33:35 INFO 140644862207616] processed a total of 1240 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1639107210.5647385, \"EndTime\": 1639107215.3004615, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4735.366344451904, \"count\": 1, \"min\": 4735.366344451904, \"max\": 4735.366344451904}}}\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:33:35 INFO 140644862207616] #throughput_metric: host=algo-1, train throughput=261.85279435208747 records/second\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:33:35 INFO 140644862207616] #progress_metric: host=algo-1, completed 49.5 % of epochs\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:33:35 INFO 140644862207616] #quality_metric: host=algo-1, epoch=98, train loss <loss>=2.9456247329711913\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:33:35 INFO 140644862207616] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:33:36 INFO 140644862207616] Epoch[99] Batch[0] avg_epoch_loss=2.914779\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:33:36 INFO 140644862207616] #quality_metric: host=algo-1, epoch=99, batch=0 train loss <loss>=2.9147794246673584\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:33:38 INFO 140644862207616] Epoch[99] Batch[5] avg_epoch_loss=2.831086\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:33:38 INFO 140644862207616] #quality_metric: host=algo-1, epoch=99, batch=5 train loss <loss>=2.831085721651713\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:33:38 INFO 140644862207616] Epoch[99] Batch [5]#011Speed: 321.98 samples/sec#011loss=2.831086\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:33:40 INFO 140644862207616] Epoch[99] Batch[10] avg_epoch_loss=2.756495\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:33:40 INFO 140644862207616] #quality_metric: host=algo-1, epoch=99, batch=10 train loss <loss>=2.6669859886169434\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:33:40 INFO 140644862207616] Epoch[99] Batch [10]#011Speed: 327.13 samples/sec#011loss=2.666986\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:33:40 INFO 140644862207616] processed a total of 1294 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1639107215.3005443, \"EndTime\": 1639107220.4679606, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5166.976451873779, \"count\": 1, \"min\": 5166.976451873779, \"max\": 5166.976451873779}}}\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:33:40 INFO 140644862207616] #throughput_metric: host=algo-1, train throughput=250.43159354130478 records/second\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:33:40 INFO 140644862207616] #progress_metric: host=algo-1, completed 50.0 % of epochs\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:33:40 INFO 140644862207616] #quality_metric: host=algo-1, epoch=99, train loss <loss>=2.7564949339086358\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:33:40 INFO 140644862207616] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:33:41 INFO 140644862207616] Epoch[100] Batch[0] avg_epoch_loss=2.482410\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:33:41 INFO 140644862207616] #quality_metric: host=algo-1, epoch=100, batch=0 train loss <loss>=2.482409715652466\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:33:43 INFO 140644862207616] Epoch[100] Batch[5] avg_epoch_loss=2.803416\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:33:43 INFO 140644862207616] #quality_metric: host=algo-1, epoch=100, batch=5 train loss <loss>=2.8034159342447915\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:33:43 INFO 140644862207616] Epoch[100] Batch [5]#011Speed: 325.54 samples/sec#011loss=2.803416\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:33:45 INFO 140644862207616] Epoch[100] Batch[10] avg_epoch_loss=2.773609\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:33:45 INFO 140644862207616] #quality_metric: host=algo-1, epoch=100, batch=10 train loss <loss>=2.7378398895263674\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:33:45 INFO 140644862207616] Epoch[100] Batch [10]#011Speed: 327.51 samples/sec#011loss=2.737840\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:33:45 INFO 140644862207616] processed a total of 1294 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1639107220.4680312, \"EndTime\": 1639107225.678899, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5210.5255126953125, \"count\": 1, \"min\": 5210.5255126953125, \"max\": 5210.5255126953125}}}\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:33:45 INFO 140644862207616] #throughput_metric: host=algo-1, train throughput=248.33860914659357 records/second\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:33:45 INFO 140644862207616] #progress_metric: host=algo-1, completed 50.5 % of epochs\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:33:45 INFO 140644862207616] #quality_metric: host=algo-1, epoch=100, train loss <loss>=2.7736086411909624\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:33:45 INFO 140644862207616] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:33:46 INFO 140644862207616] Epoch[101] Batch[0] avg_epoch_loss=2.936065\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:33:46 INFO 140644862207616] #quality_metric: host=algo-1, epoch=101, batch=0 train loss <loss>=2.936065435409546\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:33:48 INFO 140644862207616] Epoch[101] Batch[5] avg_epoch_loss=2.732016\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:33:48 INFO 140644862207616] #quality_metric: host=algo-1, epoch=101, batch=5 train loss <loss>=2.7320159673690796\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:33:48 INFO 140644862207616] Epoch[101] Batch [5]#011Speed: 316.21 samples/sec#011loss=2.732016\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:33:50 INFO 140644862207616] processed a total of 1202 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1639107225.6789691, \"EndTime\": 1639107230.5114238, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4832.106113433838, \"count\": 1, \"min\": 4832.106113433838, \"max\": 4832.106113433838}}}\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:33:50 INFO 140644862207616] #throughput_metric: host=algo-1, train throughput=248.7468873911112 records/second\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:33:50 INFO 140644862207616] #progress_metric: host=algo-1, completed 51.0 % of epochs\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:33:50 INFO 140644862207616] #quality_metric: host=algo-1, epoch=101, train loss <loss>=2.8834211349487306\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:33:50 INFO 140644862207616] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:33:51 INFO 140644862207616] Epoch[102] Batch[0] avg_epoch_loss=3.890839\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:33:51 INFO 140644862207616] #quality_metric: host=algo-1, epoch=102, batch=0 train loss <loss>=3.890838861465454\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:33:53 INFO 140644862207616] Epoch[102] Batch[5] avg_epoch_loss=3.252888\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:33:53 INFO 140644862207616] #quality_metric: host=algo-1, epoch=102, batch=5 train loss <loss>=3.2528875271479287\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:33:53 INFO 140644862207616] Epoch[102] Batch [5]#011Speed: 328.89 samples/sec#011loss=3.252888\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:33:55 INFO 140644862207616] Epoch[102] Batch[10] avg_epoch_loss=3.094444\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:33:55 INFO 140644862207616] #quality_metric: host=algo-1, epoch=102, batch=10 train loss <loss>=2.9043107986450196\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:33:55 INFO 140644862207616] Epoch[102] Batch [10]#011Speed: 321.06 samples/sec#011loss=2.904311\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:33:55 INFO 140644862207616] processed a total of 1327 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1639107230.511504, \"EndTime\": 1639107235.6454194, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5133.458137512207, \"count\": 1, \"min\": 5133.458137512207, \"max\": 5133.458137512207}}}\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:33:55 INFO 140644862207616] #throughput_metric: host=algo-1, train throughput=258.4947702082744 records/second\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:33:55 INFO 140644862207616] #progress_metric: host=algo-1, completed 51.5 % of epochs\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:33:55 INFO 140644862207616] #quality_metric: host=algo-1, epoch=102, train loss <loss>=3.0944435596466064\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:33:55 INFO 140644862207616] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:33:56 INFO 140644862207616] Epoch[103] Batch[0] avg_epoch_loss=3.225651\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:33:56 INFO 140644862207616] #quality_metric: host=algo-1, epoch=103, batch=0 train loss <loss>=3.2256510257720947\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:33:58 INFO 140644862207616] Epoch[103] Batch[5] avg_epoch_loss=3.060373\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:33:58 INFO 140644862207616] #quality_metric: host=algo-1, epoch=103, batch=5 train loss <loss>=3.060372749964396\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:33:58 INFO 140644862207616] Epoch[103] Batch [5]#011Speed: 328.56 samples/sec#011loss=3.060373\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:34:00 INFO 140644862207616] Epoch[103] Batch[10] avg_epoch_loss=3.364928\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:34:00 INFO 140644862207616] #quality_metric: host=algo-1, epoch=103, batch=10 train loss <loss>=3.7303948402404785\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:34:00 INFO 140644862207616] Epoch[103] Batch [10]#011Speed: 318.05 samples/sec#011loss=3.730395\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:34:00 INFO 140644862207616] processed a total of 1290 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1639107235.6454945, \"EndTime\": 1639107240.8154676, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5169.543266296387, \"count\": 1, \"min\": 5169.543266296387, \"max\": 5169.543266296387}}}\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:34:00 INFO 140644862207616] #throughput_metric: host=algo-1, train throughput=249.53353736966903 records/second\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:34:00 INFO 140644862207616] #progress_metric: host=algo-1, completed 52.0 % of epochs\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:34:00 INFO 140644862207616] #quality_metric: host=algo-1, epoch=103, train loss <loss>=3.3649282455444336\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:34:00 INFO 140644862207616] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:34:02 INFO 140644862207616] Epoch[104] Batch[0] avg_epoch_loss=2.748524\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:34:02 INFO 140644862207616] #quality_metric: host=algo-1, epoch=104, batch=0 train loss <loss>=2.7485244274139404\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:34:04 INFO 140644862207616] Epoch[104] Batch[5] avg_epoch_loss=2.767014\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:34:04 INFO 140644862207616] #quality_metric: host=algo-1, epoch=104, batch=5 train loss <loss>=2.767013748486837\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:34:04 INFO 140644862207616] Epoch[104] Batch [5]#011Speed: 327.72 samples/sec#011loss=2.767014\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:34:05 INFO 140644862207616] processed a total of 1274 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1639107240.8155375, \"EndTime\": 1639107245.9706976, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5154.803037643433, \"count\": 1, \"min\": 5154.803037643433, \"max\": 5154.803037643433}}}\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:34:05 INFO 140644862207616] #throughput_metric: host=algo-1, train throughput=247.14282801810586 records/second\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:34:05 INFO 140644862207616] #progress_metric: host=algo-1, completed 52.5 % of epochs\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:34:05 INFO 140644862207616] #quality_metric: host=algo-1, epoch=104, train loss <loss>=5.274103450775146\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:34:05 INFO 140644862207616] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:34:07 INFO 140644862207616] Epoch[105] Batch[0] avg_epoch_loss=3.049465\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:34:07 INFO 140644862207616] #quality_metric: host=algo-1, epoch=105, batch=0 train loss <loss>=3.049464702606201\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:34:09 INFO 140644862207616] Epoch[105] Batch[5] avg_epoch_loss=2.948177\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:34:09 INFO 140644862207616] #quality_metric: host=algo-1, epoch=105, batch=5 train loss <loss>=2.9481765031814575\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:34:09 INFO 140644862207616] Epoch[105] Batch [5]#011Speed: 333.17 samples/sec#011loss=2.948177\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:34:11 INFO 140644862207616] Epoch[105] Batch[10] avg_epoch_loss=2.888513\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:34:11 INFO 140644862207616] #quality_metric: host=algo-1, epoch=105, batch=10 train loss <loss>=2.816917657852173\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:34:11 INFO 140644862207616] Epoch[105] Batch [10]#011Speed: 325.59 samples/sec#011loss=2.816918\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:34:11 INFO 140644862207616] processed a total of 1297 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1639107245.9707751, \"EndTime\": 1639107251.1708663, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5199.702978134155, \"count\": 1, \"min\": 5199.702978134155, \"max\": 5199.702978134155}}}\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:34:11 INFO 140644862207616] #throughput_metric: host=algo-1, train throughput=249.432486802346 records/second\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:34:11 INFO 140644862207616] #progress_metric: host=algo-1, completed 53.0 % of epochs\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:34:11 INFO 140644862207616] #quality_metric: host=algo-1, epoch=105, train loss <loss>=2.8885133916681465\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:34:11 INFO 140644862207616] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:34:12 INFO 140644862207616] Epoch[106] Batch[0] avg_epoch_loss=3.094745\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:34:12 INFO 140644862207616] #quality_metric: host=algo-1, epoch=106, batch=0 train loss <loss>=3.0947446823120117\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:34:14 INFO 140644862207616] Epoch[106] Batch[5] avg_epoch_loss=2.949245\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:34:14 INFO 140644862207616] #quality_metric: host=algo-1, epoch=106, batch=5 train loss <loss>=2.9492445389429727\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:34:14 INFO 140644862207616] Epoch[106] Batch [5]#011Speed: 333.20 samples/sec#011loss=2.949245\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:34:15 INFO 140644862207616] processed a total of 1225 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1639107251.1709354, \"EndTime\": 1639107255.9249833, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4753.690242767334, \"count\": 1, \"min\": 4753.690242767334, \"max\": 4753.690242767334}}}\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:34:15 INFO 140644862207616] #throughput_metric: host=algo-1, train throughput=257.6883837812902 records/second\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:34:15 INFO 140644862207616] #progress_metric: host=algo-1, completed 53.5 % of epochs\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:34:15 INFO 140644862207616] #quality_metric: host=algo-1, epoch=106, train loss <loss>=2.890812802314758\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:34:15 INFO 140644862207616] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:34:17 INFO 140644862207616] Epoch[107] Batch[0] avg_epoch_loss=2.844050\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:34:17 INFO 140644862207616] #quality_metric: host=algo-1, epoch=107, batch=0 train loss <loss>=2.844050168991089\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:34:19 INFO 140644862207616] Epoch[107] Batch[5] avg_epoch_loss=2.915309\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:34:19 INFO 140644862207616] #quality_metric: host=algo-1, epoch=107, batch=5 train loss <loss>=2.9153093894322715\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:34:19 INFO 140644862207616] Epoch[107] Batch [5]#011Speed: 315.07 samples/sec#011loss=2.915309\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:34:21 INFO 140644862207616] Epoch[107] Batch[10] avg_epoch_loss=3.049372\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:34:21 INFO 140644862207616] #quality_metric: host=algo-1, epoch=107, batch=10 train loss <loss>=3.210246515274048\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:34:21 INFO 140644862207616] Epoch[107] Batch [10]#011Speed: 317.89 samples/sec#011loss=3.210247\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:34:21 INFO 140644862207616] processed a total of 1318 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1639107255.9250634, \"EndTime\": 1639107261.1823037, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5256.821393966675, \"count\": 1, \"min\": 5256.821393966675, \"max\": 5256.821393966675}}}\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:34:21 INFO 140644862207616] #throughput_metric: host=algo-1, train throughput=250.71702015299286 records/second\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:34:21 INFO 140644862207616] #progress_metric: host=algo-1, completed 54.0 % of epochs\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:34:21 INFO 140644862207616] #quality_metric: host=algo-1, epoch=107, train loss <loss>=3.0493717193603516\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:34:21 INFO 140644862207616] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:34:22 INFO 140644862207616] Epoch[108] Batch[0] avg_epoch_loss=2.686034\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:34:22 INFO 140644862207616] #quality_metric: host=algo-1, epoch=108, batch=0 train loss <loss>=2.6860337257385254\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:34:24 INFO 140644862207616] Epoch[108] Batch[5] avg_epoch_loss=2.791876\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:34:24 INFO 140644862207616] #quality_metric: host=algo-1, epoch=108, batch=5 train loss <loss>=2.7918761571248374\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:34:24 INFO 140644862207616] Epoch[108] Batch [5]#011Speed: 331.83 samples/sec#011loss=2.791876\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:34:26 INFO 140644862207616] Epoch[108] Batch[10] avg_epoch_loss=2.646383\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:34:26 INFO 140644862207616] #quality_metric: host=algo-1, epoch=108, batch=10 train loss <loss>=2.4717904567718505\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:34:26 INFO 140644862207616] Epoch[108] Batch [10]#011Speed: 330.30 samples/sec#011loss=2.471790\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:34:26 INFO 140644862207616] processed a total of 1284 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1639107261.1823735, \"EndTime\": 1639107266.3800778, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5197.328805923462, \"count\": 1, \"min\": 5197.328805923462, \"max\": 5197.328805923462}}}\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:34:26 INFO 140644862207616] #throughput_metric: host=algo-1, train throughput=247.04521321906944 records/second\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:34:26 INFO 140644862207616] #progress_metric: host=algo-1, completed 54.5 % of epochs\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:34:26 INFO 140644862207616] #quality_metric: host=algo-1, epoch=108, train loss <loss>=2.6463826569643887\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:34:26 INFO 140644862207616] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:34:27 INFO 140644862207616] Epoch[109] Batch[0] avg_epoch_loss=3.263321\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:34:27 INFO 140644862207616] #quality_metric: host=algo-1, epoch=109, batch=0 train loss <loss>=3.2633209228515625\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:34:29 INFO 140644862207616] Epoch[109] Batch[5] avg_epoch_loss=3.107060\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:34:29 INFO 140644862207616] #quality_metric: host=algo-1, epoch=109, batch=5 train loss <loss>=3.1070597569147744\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:34:29 INFO 140644862207616] Epoch[109] Batch [5]#011Speed: 333.33 samples/sec#011loss=3.107060\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:34:31 INFO 140644862207616] processed a total of 1209 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1639107266.3801465, \"EndTime\": 1639107271.090699, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4710.216760635376, \"count\": 1, \"min\": 4710.216760635376, \"max\": 4710.216760635376}}}\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:34:31 INFO 140644862207616] #throughput_metric: host=algo-1, train throughput=256.6699013395907 records/second\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:34:31 INFO 140644862207616] #progress_metric: host=algo-1, completed 55.0 % of epochs\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:34:31 INFO 140644862207616] #quality_metric: host=algo-1, epoch=109, train loss <loss>=2.9669568300247193\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:34:31 INFO 140644862207616] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:34:32 INFO 140644862207616] Epoch[110] Batch[0] avg_epoch_loss=2.776186\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:34:32 INFO 140644862207616] #quality_metric: host=algo-1, epoch=110, batch=0 train loss <loss>=2.7761857509613037\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:34:34 INFO 140644862207616] Epoch[110] Batch[5] avg_epoch_loss=2.973709\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:34:34 INFO 140644862207616] #quality_metric: host=algo-1, epoch=110, batch=5 train loss <loss>=2.9737091064453125\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:34:34 INFO 140644862207616] Epoch[110] Batch [5]#011Speed: 329.49 samples/sec#011loss=2.973709\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:34:35 INFO 140644862207616] processed a total of 1279 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1639107271.0907779, \"EndTime\": 1639107275.8261747, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4735.008716583252, \"count\": 1, \"min\": 4735.008716583252, \"max\": 4735.008716583252}}}\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:34:35 INFO 140644862207616] #throughput_metric: host=algo-1, train throughput=270.10918513704206 records/second\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:34:35 INFO 140644862207616] #progress_metric: host=algo-1, completed 55.5 % of epochs\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:34:35 INFO 140644862207616] #quality_metric: host=algo-1, epoch=110, train loss <loss>=2.952894115447998\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:34:35 INFO 140644862207616] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:34:37 INFO 140644862207616] Epoch[111] Batch[0] avg_epoch_loss=2.704993\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:34:37 INFO 140644862207616] #quality_metric: host=algo-1, epoch=111, batch=0 train loss <loss>=2.7049927711486816\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:34:39 INFO 140644862207616] Epoch[111] Batch[5] avg_epoch_loss=2.913870\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:34:39 INFO 140644862207616] #quality_metric: host=algo-1, epoch=111, batch=5 train loss <loss>=2.913869857788086\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:34:39 INFO 140644862207616] Epoch[111] Batch [5]#011Speed: 329.00 samples/sec#011loss=2.913870\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:34:41 INFO 140644862207616] Epoch[111] Batch[10] avg_epoch_loss=3.088775\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:34:41 INFO 140644862207616] #quality_metric: host=algo-1, epoch=111, batch=10 train loss <loss>=3.298660230636597\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:34:41 INFO 140644862207616] Epoch[111] Batch [10]#011Speed: 319.18 samples/sec#011loss=3.298660\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:34:41 INFO 140644862207616] processed a total of 1310 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1639107275.8262541, \"EndTime\": 1639107281.021882, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5195.192813873291, \"count\": 1, \"min\": 5195.192813873291, \"max\": 5195.192813873291}}}\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:34:41 INFO 140644862207616] #throughput_metric: host=algo-1, train throughput=252.15132480288094 records/second\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:34:41 INFO 140644862207616] #progress_metric: host=algo-1, completed 56.0 % of epochs\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:34:41 INFO 140644862207616] #quality_metric: host=algo-1, epoch=111, train loss <loss>=3.088774572719227\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:34:41 INFO 140644862207616] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:34:42 INFO 140644862207616] Epoch[112] Batch[0] avg_epoch_loss=2.953823\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:34:42 INFO 140644862207616] #quality_metric: host=algo-1, epoch=112, batch=0 train loss <loss>=2.953822612762451\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:34:44 INFO 140644862207616] Epoch[112] Batch[5] avg_epoch_loss=2.735963\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:34:44 INFO 140644862207616] #quality_metric: host=algo-1, epoch=112, batch=5 train loss <loss>=2.7359628677368164\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:34:44 INFO 140644862207616] Epoch[112] Batch [5]#011Speed: 332.66 samples/sec#011loss=2.735963\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:34:45 INFO 140644862207616] processed a total of 1252 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1639107281.0219514, \"EndTime\": 1639107285.7378223, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4715.522527694702, \"count\": 1, \"min\": 4715.522527694702, \"max\": 4715.522527694702}}}\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:34:45 INFO 140644862207616] #throughput_metric: host=algo-1, train throughput=265.49969775737793 records/second\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:34:45 INFO 140644862207616] #progress_metric: host=algo-1, completed 56.5 % of epochs\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:34:45 INFO 140644862207616] #quality_metric: host=algo-1, epoch=112, train loss <loss>=2.66442232131958\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:34:45 INFO 140644862207616] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:34:46 INFO 140644862207616] Epoch[113] Batch[0] avg_epoch_loss=2.610072\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:34:46 INFO 140644862207616] #quality_metric: host=algo-1, epoch=113, batch=0 train loss <loss>=2.6100716590881348\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:34:48 INFO 140644862207616] Epoch[113] Batch[5] avg_epoch_loss=2.604694\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:34:48 INFO 140644862207616] #quality_metric: host=algo-1, epoch=113, batch=5 train loss <loss>=2.6046942869822183\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:34:48 INFO 140644862207616] Epoch[113] Batch [5]#011Speed: 330.74 samples/sec#011loss=2.604694\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:34:50 INFO 140644862207616] Epoch[113] Batch[10] avg_epoch_loss=2.533402\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:34:50 INFO 140644862207616] #quality_metric: host=algo-1, epoch=113, batch=10 train loss <loss>=2.4478505849838257\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:34:50 INFO 140644862207616] Epoch[113] Batch [10]#011Speed: 311.68 samples/sec#011loss=2.447851\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:34:50 INFO 140644862207616] processed a total of 1287 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1639107285.7379034, \"EndTime\": 1639107290.9766896, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5238.381624221802, \"count\": 1, \"min\": 5238.381624221802, \"max\": 5238.381624221802}}}\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:34:50 INFO 140644862207616] #throughput_metric: host=algo-1, train throughput=245.68181509107816 records/second\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:34:50 INFO 140644862207616] #progress_metric: host=algo-1, completed 57.0 % of epochs\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:34:50 INFO 140644862207616] #quality_metric: host=algo-1, epoch=113, train loss <loss>=2.533401695164767\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:34:50 INFO 140644862207616] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:34:51 INFO 140644862207616] Saved checkpoint to \"/opt/ml/model/state_d89b2f25-fe2c-44dd-819c-f6224a18c0aa-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1639107290.9767604, \"EndTime\": 1639107291.0406651, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 63.52996826171875, \"count\": 1, \"min\": 63.52996826171875, \"max\": 63.52996826171875}}}\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:34:52 INFO 140644862207616] Epoch[114] Batch[0] avg_epoch_loss=3.171883\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:34:52 INFO 140644862207616] #quality_metric: host=algo-1, epoch=114, batch=0 train loss <loss>=3.1718833446502686\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:34:54 INFO 140644862207616] Epoch[114] Batch[5] avg_epoch_loss=2.755267\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:34:54 INFO 140644862207616] #quality_metric: host=algo-1, epoch=114, batch=5 train loss <loss>=2.755266547203064\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:34:54 INFO 140644862207616] Epoch[114] Batch [5]#011Speed: 332.22 samples/sec#011loss=2.755267\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:34:56 INFO 140644862207616] Epoch[114] Batch[10] avg_epoch_loss=2.599861\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:34:56 INFO 140644862207616] #quality_metric: host=algo-1, epoch=114, batch=10 train loss <loss>=2.4133745193481446\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:34:56 INFO 140644862207616] Epoch[114] Batch [10]#011Speed: 329.46 samples/sec#011loss=2.413375\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:34:56 INFO 140644862207616] processed a total of 1287 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1639107291.0407348, \"EndTime\": 1639107296.1536427, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5112.844228744507, \"count\": 1, \"min\": 5112.844228744507, \"max\": 5112.844228744507}}}\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:34:56 INFO 140644862207616] #throughput_metric: host=algo-1, train throughput=251.71405128335215 records/second\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:34:56 INFO 140644862207616] #progress_metric: host=algo-1, completed 57.5 % of epochs\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:34:56 INFO 140644862207616] #quality_metric: host=algo-1, epoch=114, train loss <loss>=2.5998610799962822\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:34:56 INFO 140644862207616] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:34:57 INFO 140644862207616] Epoch[115] Batch[0] avg_epoch_loss=2.783505\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:34:57 INFO 140644862207616] #quality_metric: host=algo-1, epoch=115, batch=0 train loss <loss>=2.783505439758301\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:34:59 INFO 140644862207616] Epoch[115] Batch[5] avg_epoch_loss=3.223409\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:34:59 INFO 140644862207616] #quality_metric: host=algo-1, epoch=115, batch=5 train loss <loss>=3.223409136136373\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:34:59 INFO 140644862207616] Epoch[115] Batch [5]#011Speed: 330.23 samples/sec#011loss=3.223409\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:35:01 INFO 140644862207616] Epoch[115] Batch[10] avg_epoch_loss=3.212340\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:35:01 INFO 140644862207616] #quality_metric: host=algo-1, epoch=115, batch=10 train loss <loss>=3.1990561485290527\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:35:01 INFO 140644862207616] Epoch[115] Batch [10]#011Speed: 321.11 samples/sec#011loss=3.199056\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:35:01 INFO 140644862207616] processed a total of 1332 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1639107296.1537118, \"EndTime\": 1639107301.2911263, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5137.0689868927, \"count\": 1, \"min\": 5137.0689868927, \"max\": 5137.0689868927}}}\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:35:01 INFO 140644862207616] #throughput_metric: host=algo-1, train throughput=259.287193364179 records/second\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:35:01 INFO 140644862207616] #progress_metric: host=algo-1, completed 58.0 % of epochs\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:35:01 INFO 140644862207616] #quality_metric: host=algo-1, epoch=115, train loss <loss>=3.2123395963148638\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:35:01 INFO 140644862207616] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:35:02 INFO 140644862207616] Epoch[116] Batch[0] avg_epoch_loss=3.376832\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:35:02 INFO 140644862207616] #quality_metric: host=algo-1, epoch=116, batch=0 train loss <loss>=3.3768317699432373\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:35:04 INFO 140644862207616] Epoch[116] Batch[5] avg_epoch_loss=3.198588\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:35:04 INFO 140644862207616] #quality_metric: host=algo-1, epoch=116, batch=5 train loss <loss>=3.1985884507497153\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:35:04 INFO 140644862207616] Epoch[116] Batch [5]#011Speed: 310.86 samples/sec#011loss=3.198588\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:35:06 INFO 140644862207616] processed a total of 1271 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1639107301.2911882, \"EndTime\": 1639107306.4567502, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5165.1716232299805, \"count\": 1, \"min\": 5165.1716232299805, \"max\": 5165.1716232299805}}}\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:35:06 INFO 140644862207616] #throughput_metric: host=algo-1, train throughput=246.06620637958267 records/second\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:35:06 INFO 140644862207616] #progress_metric: host=algo-1, completed 58.5 % of epochs\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:35:06 INFO 140644862207616] #quality_metric: host=algo-1, epoch=116, train loss <loss>=3.1564441442489626\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:35:06 INFO 140644862207616] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:35:07 INFO 140644862207616] Epoch[117] Batch[0] avg_epoch_loss=2.948191\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:35:07 INFO 140644862207616] #quality_metric: host=algo-1, epoch=117, batch=0 train loss <loss>=2.9481914043426514\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:35:09 INFO 140644862207616] Epoch[117] Batch[5] avg_epoch_loss=3.055216\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:35:09 INFO 140644862207616] #quality_metric: host=algo-1, epoch=117, batch=5 train loss <loss>=3.05521567662557\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:35:09 INFO 140644862207616] Epoch[117] Batch [5]#011Speed: 331.63 samples/sec#011loss=3.055216\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:35:11 INFO 140644862207616] processed a total of 1274 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1639107306.4568214, \"EndTime\": 1639107311.1636493, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4706.251621246338, \"count\": 1, \"min\": 4706.251621246338, \"max\": 4706.251621246338}}}\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:35:11 INFO 140644862207616] #throughput_metric: host=algo-1, train throughput=270.6972989405581 records/second\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:35:11 INFO 140644862207616] #progress_metric: host=algo-1, completed 59.0 % of epochs\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:35:11 INFO 140644862207616] #quality_metric: host=algo-1, epoch=117, train loss <loss>=2.9949764013290405\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:35:11 INFO 140644862207616] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:35:12 INFO 140644862207616] Epoch[118] Batch[0] avg_epoch_loss=2.834434\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:35:12 INFO 140644862207616] #quality_metric: host=algo-1, epoch=118, batch=0 train loss <loss>=2.8344342708587646\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:35:14 INFO 140644862207616] Epoch[118] Batch[5] avg_epoch_loss=2.931955\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:35:14 INFO 140644862207616] #quality_metric: host=algo-1, epoch=118, batch=5 train loss <loss>=2.9319549401601157\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:35:14 INFO 140644862207616] Epoch[118] Batch [5]#011Speed: 325.62 samples/sec#011loss=2.931955\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:35:15 INFO 140644862207616] processed a total of 1276 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1639107311.163727, \"EndTime\": 1639107315.9271574, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4762.983560562134, \"count\": 1, \"min\": 4762.983560562134, \"max\": 4762.983560562134}}}\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:35:15 INFO 140644862207616] #throughput_metric: host=algo-1, train throughput=267.89307246329514 records/second\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:35:15 INFO 140644862207616] #progress_metric: host=algo-1, completed 59.5 % of epochs\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:35:15 INFO 140644862207616] #quality_metric: host=algo-1, epoch=118, train loss <loss>=2.9435084581375124\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:35:15 INFO 140644862207616] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:35:17 INFO 140644862207616] Epoch[119] Batch[0] avg_epoch_loss=3.054718\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:35:17 INFO 140644862207616] #quality_metric: host=algo-1, epoch=119, batch=0 train loss <loss>=3.054718255996704\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:35:19 INFO 140644862207616] Epoch[119] Batch[5] avg_epoch_loss=2.927440\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:35:19 INFO 140644862207616] #quality_metric: host=algo-1, epoch=119, batch=5 train loss <loss>=2.9274396498998008\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:35:19 INFO 140644862207616] Epoch[119] Batch [5]#011Speed: 327.40 samples/sec#011loss=2.927440\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:35:21 INFO 140644862207616] Epoch[119] Batch[10] avg_epoch_loss=3.314783\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:35:21 INFO 140644862207616] #quality_metric: host=algo-1, epoch=119, batch=10 train loss <loss>=3.7795951843261717\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:35:21 INFO 140644862207616] Epoch[119] Batch [10]#011Speed: 298.86 samples/sec#011loss=3.779595\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:35:21 INFO 140644862207616] processed a total of 1337 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1639107315.9272363, \"EndTime\": 1639107321.2369676, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5309.345483779907, \"count\": 1, \"min\": 5309.345483779907, \"max\": 5309.345483779907}}}\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:35:21 INFO 140644862207616] #throughput_metric: host=algo-1, train throughput=251.8156615154883 records/second\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:35:21 INFO 140644862207616] #progress_metric: host=algo-1, completed 60.0 % of epochs\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:35:21 INFO 140644862207616] #quality_metric: host=algo-1, epoch=119, train loss <loss>=3.31478307463906\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:35:21 INFO 140644862207616] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:35:22 INFO 140644862207616] Epoch[120] Batch[0] avg_epoch_loss=3.353432\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:35:22 INFO 140644862207616] #quality_metric: host=algo-1, epoch=120, batch=0 train loss <loss>=3.3534317016601562\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:35:24 INFO 140644862207616] Epoch[120] Batch[5] avg_epoch_loss=3.333481\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:35:24 INFO 140644862207616] #quality_metric: host=algo-1, epoch=120, batch=5 train loss <loss>=3.333481192588806\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:35:24 INFO 140644862207616] Epoch[120] Batch [5]#011Speed: 331.71 samples/sec#011loss=3.333481\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:35:25 INFO 140644862207616] processed a total of 1191 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1639107321.2370343, \"EndTime\": 1639107325.936453, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4698.971509933472, \"count\": 1, \"min\": 4698.971509933472, \"max\": 4698.971509933472}}}\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:35:25 INFO 140644862207616] #throughput_metric: host=algo-1, train throughput=253.45453678305864 records/second\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:35:25 INFO 140644862207616] #progress_metric: host=algo-1, completed 60.5 % of epochs\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:35:25 INFO 140644862207616] #quality_metric: host=algo-1, epoch=120, train loss <loss>=3.2175976037979126\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:35:25 INFO 140644862207616] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:35:27 INFO 140644862207616] Epoch[121] Batch[0] avg_epoch_loss=3.021156\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:35:27 INFO 140644862207616] #quality_metric: host=algo-1, epoch=121, batch=0 train loss <loss>=3.021156072616577\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:35:29 INFO 140644862207616] Epoch[121] Batch[5] avg_epoch_loss=3.068038\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:35:29 INFO 140644862207616] #quality_metric: host=algo-1, epoch=121, batch=5 train loss <loss>=3.0680379470189414\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:35:29 INFO 140644862207616] Epoch[121] Batch [5]#011Speed: 325.81 samples/sec#011loss=3.068038\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:35:30 INFO 140644862207616] processed a total of 1234 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1639107325.9365191, \"EndTime\": 1639107330.6756706, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4738.7237548828125, \"count\": 1, \"min\": 4738.7237548828125, \"max\": 4738.7237548828125}}}\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:35:30 INFO 140644862207616] #throughput_metric: host=algo-1, train throughput=260.40240067820315 records/second\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:35:30 INFO 140644862207616] #progress_metric: host=algo-1, completed 61.0 % of epochs\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:35:30 INFO 140644862207616] #quality_metric: host=algo-1, epoch=121, train loss <loss>=3.2148804664611816\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:35:30 INFO 140644862207616] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:35:31 INFO 140644862207616] Epoch[122] Batch[0] avg_epoch_loss=2.972091\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:35:31 INFO 140644862207616] #quality_metric: host=algo-1, epoch=122, batch=0 train loss <loss>=2.97209095954895\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:35:33 INFO 140644862207616] Epoch[122] Batch[5] avg_epoch_loss=3.333302\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:35:33 INFO 140644862207616] #quality_metric: host=algo-1, epoch=122, batch=5 train loss <loss>=3.333302100499471\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:35:33 INFO 140644862207616] Epoch[122] Batch [5]#011Speed: 325.19 samples/sec#011loss=3.333302\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:35:35 INFO 140644862207616] Epoch[122] Batch[10] avg_epoch_loss=3.303699\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:35:35 INFO 140644862207616] #quality_metric: host=algo-1, epoch=122, batch=10 train loss <loss>=3.2681742191314695\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:35:35 INFO 140644862207616] Epoch[122] Batch [10]#011Speed: 325.91 samples/sec#011loss=3.268174\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:35:35 INFO 140644862207616] processed a total of 1298 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1639107330.6757379, \"EndTime\": 1639107335.812976, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5136.741399765015, \"count\": 1, \"min\": 5136.741399765015, \"max\": 5136.741399765015}}}\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:35:35 INFO 140644862207616] #throughput_metric: host=algo-1, train throughput=252.68445425315525 records/second\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:35:35 INFO 140644862207616] #progress_metric: host=algo-1, completed 61.5 % of epochs\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:35:35 INFO 140644862207616] #quality_metric: host=algo-1, epoch=122, train loss <loss>=3.3036985180594702\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:35:35 INFO 140644862207616] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:35:37 INFO 140644862207616] Epoch[123] Batch[0] avg_epoch_loss=3.356280\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:35:37 INFO 140644862207616] #quality_metric: host=algo-1, epoch=123, batch=0 train loss <loss>=3.3562800884246826\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:35:38 INFO 140644862207616] Epoch[123] Batch[5] avg_epoch_loss=3.237987\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:35:38 INFO 140644862207616] #quality_metric: host=algo-1, epoch=123, batch=5 train loss <loss>=3.237986922264099\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:35:38 INFO 140644862207616] Epoch[123] Batch [5]#011Speed: 324.67 samples/sec#011loss=3.237987\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:35:40 INFO 140644862207616] processed a total of 1252 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1639107335.8130474, \"EndTime\": 1639107340.5614314, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4747.94340133667, \"count\": 1, \"min\": 4747.94340133667, \"max\": 4747.94340133667}}}\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:35:40 INFO 140644862207616] #throughput_metric: host=algo-1, train throughput=263.68701395079506 records/second\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:35:40 INFO 140644862207616] #progress_metric: host=algo-1, completed 62.0 % of epochs\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:35:40 INFO 140644862207616] #quality_metric: host=algo-1, epoch=123, train loss <loss>=3.143481230735779\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:35:40 INFO 140644862207616] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:35:41 INFO 140644862207616] Epoch[124] Batch[0] avg_epoch_loss=2.996372\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:35:41 INFO 140644862207616] #quality_metric: host=algo-1, epoch=124, batch=0 train loss <loss>=2.9963722229003906\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:35:43 INFO 140644862207616] Epoch[124] Batch[5] avg_epoch_loss=2.976672\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:35:43 INFO 140644862207616] #quality_metric: host=algo-1, epoch=124, batch=5 train loss <loss>=2.9766722520192466\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:35:43 INFO 140644862207616] Epoch[124] Batch [5]#011Speed: 328.20 samples/sec#011loss=2.976672\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:35:45 INFO 140644862207616] Epoch[124] Batch[10] avg_epoch_loss=2.916576\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:35:45 INFO 140644862207616] #quality_metric: host=algo-1, epoch=124, batch=10 train loss <loss>=2.844460916519165\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:35:45 INFO 140644862207616] Epoch[124] Batch [10]#011Speed: 320.70 samples/sec#011loss=2.844461\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:35:45 INFO 140644862207616] processed a total of 1329 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1639107340.5615084, \"EndTime\": 1639107345.7121818, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5150.280237197876, \"count\": 1, \"min\": 5150.280237197876, \"max\": 5150.280237197876}}}\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:35:45 INFO 140644862207616] #throughput_metric: host=algo-1, train throughput=258.03956424869097 records/second\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:35:45 INFO 140644862207616] #progress_metric: host=algo-1, completed 62.5 % of epochs\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:35:45 INFO 140644862207616] #quality_metric: host=algo-1, epoch=124, train loss <loss>=2.9165761904283003\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:35:45 INFO 140644862207616] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:35:46 INFO 140644862207616] Epoch[125] Batch[0] avg_epoch_loss=3.296925\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:35:46 INFO 140644862207616] #quality_metric: host=algo-1, epoch=125, batch=0 train loss <loss>=3.2969250679016113\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:35:48 INFO 140644862207616] Epoch[125] Batch[5] avg_epoch_loss=3.357741\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:35:48 INFO 140644862207616] #quality_metric: host=algo-1, epoch=125, batch=5 train loss <loss>=3.357741196950277\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:35:48 INFO 140644862207616] Epoch[125] Batch [5]#011Speed: 323.62 samples/sec#011loss=3.357741\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:35:50 INFO 140644862207616] processed a total of 1250 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1639107345.7122438, \"EndTime\": 1639107350.5391488, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4826.518535614014, \"count\": 1, \"min\": 4826.518535614014, \"max\": 4826.518535614014}}}\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:35:50 INFO 140644862207616] #throughput_metric: host=algo-1, train throughput=258.97984895048523 records/second\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:35:50 INFO 140644862207616] #progress_metric: host=algo-1, completed 63.0 % of epochs\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:35:50 INFO 140644862207616] #quality_metric: host=algo-1, epoch=125, train loss <loss>=3.202670693397522\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:35:50 INFO 140644862207616] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:35:51 INFO 140644862207616] Epoch[126] Batch[0] avg_epoch_loss=3.130537\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:35:51 INFO 140644862207616] #quality_metric: host=algo-1, epoch=126, batch=0 train loss <loss>=3.130537271499634\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:35:53 INFO 140644862207616] Epoch[126] Batch[5] avg_epoch_loss=2.993033\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:35:53 INFO 140644862207616] #quality_metric: host=algo-1, epoch=126, batch=5 train loss <loss>=2.9930325746536255\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:35:53 INFO 140644862207616] Epoch[126] Batch [5]#011Speed: 329.55 samples/sec#011loss=2.993033\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:35:55 INFO 140644862207616] processed a total of 1262 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1639107350.5392265, \"EndTime\": 1639107355.2751014, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4735.479831695557, \"count\": 1, \"min\": 4735.479831695557, \"max\": 4735.479831695557}}}\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:35:55 INFO 140644862207616] #throughput_metric: host=algo-1, train throughput=266.4925320128157 records/second\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:35:55 INFO 140644862207616] #progress_metric: host=algo-1, completed 63.5 % of epochs\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:35:55 INFO 140644862207616] #quality_metric: host=algo-1, epoch=126, train loss <loss>=2.9602885007858277\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:35:55 INFO 140644862207616] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:35:56 INFO 140644862207616] Epoch[127] Batch[0] avg_epoch_loss=2.823310\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:35:56 INFO 140644862207616] #quality_metric: host=algo-1, epoch=127, batch=0 train loss <loss>=2.823310375213623\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:35:58 INFO 140644862207616] Epoch[127] Batch[5] avg_epoch_loss=3.050417\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:35:58 INFO 140644862207616] #quality_metric: host=algo-1, epoch=127, batch=5 train loss <loss>=3.0504174629847207\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:35:58 INFO 140644862207616] Epoch[127] Batch [5]#011Speed: 329.27 samples/sec#011loss=3.050417\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:36:00 INFO 140644862207616] Epoch[127] Batch[10] avg_epoch_loss=2.982710\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:36:00 INFO 140644862207616] #quality_metric: host=algo-1, epoch=127, batch=10 train loss <loss>=2.901461458206177\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:36:00 INFO 140644862207616] Epoch[127] Batch [10]#011Speed: 311.92 samples/sec#011loss=2.901461\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:36:00 INFO 140644862207616] processed a total of 1348 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1639107355.27518, \"EndTime\": 1639107360.469967, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5194.393157958984, \"count\": 1, \"min\": 5194.393157958984, \"max\": 5194.393157958984}}}\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:36:00 INFO 140644862207616] #throughput_metric: host=algo-1, train throughput=259.5052951011662 records/second\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:36:00 INFO 140644862207616] #progress_metric: host=algo-1, completed 64.0 % of epochs\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:36:00 INFO 140644862207616] #quality_metric: host=algo-1, epoch=127, train loss <loss>=2.982710188085383\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:36:00 INFO 140644862207616] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:36:01 INFO 140644862207616] Epoch[128] Batch[0] avg_epoch_loss=3.134624\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:36:01 INFO 140644862207616] #quality_metric: host=algo-1, epoch=128, batch=0 train loss <loss>=3.1346237659454346\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:36:03 INFO 140644862207616] Epoch[128] Batch[5] avg_epoch_loss=2.997824\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:36:03 INFO 140644862207616] #quality_metric: host=algo-1, epoch=128, batch=5 train loss <loss>=2.9978243112564087\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:36:03 INFO 140644862207616] Epoch[128] Batch [5]#011Speed: 310.48 samples/sec#011loss=2.997824\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:36:06 INFO 140644862207616] Epoch[128] Batch[10] avg_epoch_loss=2.866343\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:36:06 INFO 140644862207616] #quality_metric: host=algo-1, epoch=128, batch=10 train loss <loss>=2.70856511592865\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:36:06 INFO 140644862207616] Epoch[128] Batch [10]#011Speed: 282.39 samples/sec#011loss=2.708565\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:36:06 INFO 140644862207616] processed a total of 1305 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1639107360.4700394, \"EndTime\": 1639107366.0482867, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5577.873945236206, \"count\": 1, \"min\": 5577.873945236206, \"max\": 5577.873945236206}}}\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:36:06 INFO 140644862207616] #throughput_metric: host=algo-1, train throughput=233.95522963172814 records/second\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:36:06 INFO 140644862207616] #progress_metric: host=algo-1, completed 64.5 % of epochs\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:36:06 INFO 140644862207616] #quality_metric: host=algo-1, epoch=128, train loss <loss>=2.8663428588347\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:36:06 INFO 140644862207616] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:36:07 INFO 140644862207616] Epoch[129] Batch[0] avg_epoch_loss=2.981374\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:36:07 INFO 140644862207616] #quality_metric: host=algo-1, epoch=129, batch=0 train loss <loss>=2.9813735485076904\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:36:09 INFO 140644862207616] Epoch[129] Batch[5] avg_epoch_loss=2.917010\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:36:09 INFO 140644862207616] #quality_metric: host=algo-1, epoch=129, batch=5 train loss <loss>=2.9170099099477134\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:36:09 INFO 140644862207616] Epoch[129] Batch [5]#011Speed: 327.54 samples/sec#011loss=2.917010\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:36:10 INFO 140644862207616] processed a total of 1263 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1639107366.0483658, \"EndTime\": 1639107370.873439, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4824.610710144043, \"count\": 1, \"min\": 4824.610710144043, \"max\": 4824.610710144043}}}\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:36:10 INFO 140644862207616] #throughput_metric: host=algo-1, train throughput=261.77651799967265 records/second\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:36:10 INFO 140644862207616] #progress_metric: host=algo-1, completed 65.0 % of epochs\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:36:10 INFO 140644862207616] #quality_metric: host=algo-1, epoch=129, train loss <loss>=2.907851552963257\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:36:10 INFO 140644862207616] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:36:12 INFO 140644862207616] Epoch[130] Batch[0] avg_epoch_loss=3.004289\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:36:12 INFO 140644862207616] #quality_metric: host=algo-1, epoch=130, batch=0 train loss <loss>=3.004288673400879\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:36:14 INFO 140644862207616] Epoch[130] Batch[5] avg_epoch_loss=2.941205\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:36:14 INFO 140644862207616] #quality_metric: host=algo-1, epoch=130, batch=5 train loss <loss>=2.94120458761851\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:36:14 INFO 140644862207616] Epoch[130] Batch [5]#011Speed: 327.87 samples/sec#011loss=2.941205\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:36:15 INFO 140644862207616] processed a total of 1232 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1639107370.8735197, \"EndTime\": 1639107375.6481287, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4774.216175079346, \"count\": 1, \"min\": 4774.216175079346, \"max\": 4774.216175079346}}}\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:36:15 INFO 140644862207616] #throughput_metric: host=algo-1, train throughput=258.0468388006799 records/second\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:36:15 INFO 140644862207616] #progress_metric: host=algo-1, completed 65.5 % of epochs\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:36:15 INFO 140644862207616] #quality_metric: host=algo-1, epoch=130, train loss <loss>=2.913015294075012\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:36:15 INFO 140644862207616] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:36:16 INFO 140644862207616] Epoch[131] Batch[0] avg_epoch_loss=3.009984\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:36:16 INFO 140644862207616] #quality_metric: host=algo-1, epoch=131, batch=0 train loss <loss>=3.009983539581299\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:36:18 INFO 140644862207616] Epoch[131] Batch[5] avg_epoch_loss=3.005446\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:36:18 INFO 140644862207616] #quality_metric: host=algo-1, epoch=131, batch=5 train loss <loss>=3.005445639292399\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:36:18 INFO 140644862207616] Epoch[131] Batch [5]#011Speed: 332.98 samples/sec#011loss=3.005446\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:36:20 INFO 140644862207616] processed a total of 1280 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1639107375.6482077, \"EndTime\": 1639107380.359341, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4710.688352584839, \"count\": 1, \"min\": 4710.688352584839, \"max\": 4710.688352584839}}}\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:36:20 INFO 140644862207616] #throughput_metric: host=algo-1, train throughput=271.71526301098993 records/second\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:36:20 INFO 140644862207616] #progress_metric: host=algo-1, completed 66.0 % of epochs\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:36:20 INFO 140644862207616] #quality_metric: host=algo-1, epoch=131, train loss <loss>=2.9565021276473997\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:36:20 INFO 140644862207616] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:36:21 INFO 140644862207616] Epoch[132] Batch[0] avg_epoch_loss=2.718902\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:36:21 INFO 140644862207616] #quality_metric: host=algo-1, epoch=132, batch=0 train loss <loss>=2.718902349472046\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:36:23 INFO 140644862207616] Epoch[132] Batch[5] avg_epoch_loss=2.901373\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:36:23 INFO 140644862207616] #quality_metric: host=algo-1, epoch=132, batch=5 train loss <loss>=2.9013728698094687\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:36:23 INFO 140644862207616] Epoch[132] Batch [5]#011Speed: 335.19 samples/sec#011loss=2.901373\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:36:25 INFO 140644862207616] processed a total of 1244 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1639107380.3594294, \"EndTime\": 1639107385.0753965, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4715.533256530762, \"count\": 1, \"min\": 4715.533256530762, \"max\": 4715.533256530762}}}\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:36:25 INFO 140644862207616] #throughput_metric: host=algo-1, train throughput=263.80301400077974 records/second\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:36:25 INFO 140644862207616] #progress_metric: host=algo-1, completed 66.5 % of epochs\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:36:25 INFO 140644862207616] #quality_metric: host=algo-1, epoch=132, train loss <loss>=2.9488252878189085\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:36:25 INFO 140644862207616] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:36:26 INFO 140644862207616] Epoch[133] Batch[0] avg_epoch_loss=3.044527\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:36:26 INFO 140644862207616] #quality_metric: host=algo-1, epoch=133, batch=0 train loss <loss>=3.044527053833008\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:36:28 INFO 140644862207616] Epoch[133] Batch[5] avg_epoch_loss=2.876125\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:36:28 INFO 140644862207616] #quality_metric: host=algo-1, epoch=133, batch=5 train loss <loss>=2.87612521648407\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:36:28 INFO 140644862207616] Epoch[133] Batch [5]#011Speed: 329.04 samples/sec#011loss=2.876125\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:36:30 INFO 140644862207616] Epoch[133] Batch[10] avg_epoch_loss=3.170043\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:36:30 INFO 140644862207616] #quality_metric: host=algo-1, epoch=133, batch=10 train loss <loss>=3.522743558883667\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:36:30 INFO 140644862207616] Epoch[133] Batch [10]#011Speed: 317.20 samples/sec#011loss=3.522744\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:36:30 INFO 140644862207616] processed a total of 1307 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1639107385.0754716, \"EndTime\": 1639107390.2701173, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5194.208860397339, \"count\": 1, \"min\": 5194.208860397339, \"max\": 5194.208860397339}}}\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:36:30 INFO 140644862207616] #throughput_metric: host=algo-1, train throughput=251.6215344439502 records/second\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:36:30 INFO 140644862207616] #progress_metric: host=algo-1, completed 67.0 % of epochs\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:36:30 INFO 140644862207616] #quality_metric: host=algo-1, epoch=133, train loss <loss>=3.170042644847523\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:36:30 INFO 140644862207616] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:36:31 INFO 140644862207616] Epoch[134] Batch[0] avg_epoch_loss=2.819415\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:36:31 INFO 140644862207616] #quality_metric: host=algo-1, epoch=134, batch=0 train loss <loss>=2.8194148540496826\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:36:33 INFO 140644862207616] Epoch[134] Batch[5] avg_epoch_loss=2.698845\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:36:33 INFO 140644862207616] #quality_metric: host=algo-1, epoch=134, batch=5 train loss <loss>=2.698844869931539\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:36:33 INFO 140644862207616] Epoch[134] Batch [5]#011Speed: 330.60 samples/sec#011loss=2.698845\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:36:35 INFO 140644862207616] Epoch[134] Batch[10] avg_epoch_loss=2.722037\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:36:35 INFO 140644862207616] #quality_metric: host=algo-1, epoch=134, batch=10 train loss <loss>=2.749867820739746\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:36:35 INFO 140644862207616] Epoch[134] Batch [10]#011Speed: 308.59 samples/sec#011loss=2.749868\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:36:35 INFO 140644862207616] processed a total of 1340 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1639107390.2701876, \"EndTime\": 1639107395.4966993, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5226.16171836853, \"count\": 1, \"min\": 5226.16171836853, \"max\": 5226.16171836853}}}\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:36:35 INFO 140644862207616] #throughput_metric: host=algo-1, train throughput=256.3973625643717 records/second\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:36:35 INFO 140644862207616] #progress_metric: host=algo-1, completed 67.5 % of epochs\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:36:35 INFO 140644862207616] #quality_metric: host=algo-1, epoch=134, train loss <loss>=2.7220371202989058\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:36:35 INFO 140644862207616] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:36:36 INFO 140644862207616] Epoch[135] Batch[0] avg_epoch_loss=3.825130\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:36:36 INFO 140644862207616] #quality_metric: host=algo-1, epoch=135, batch=0 train loss <loss>=3.8251304626464844\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:36:38 INFO 140644862207616] Epoch[135] Batch[5] avg_epoch_loss=3.005607\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:36:38 INFO 140644862207616] #quality_metric: host=algo-1, epoch=135, batch=5 train loss <loss>=3.0056069691975913\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:36:38 INFO 140644862207616] Epoch[135] Batch [5]#011Speed: 330.23 samples/sec#011loss=3.005607\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:36:40 INFO 140644862207616] Epoch[135] Batch[10] avg_epoch_loss=3.192570\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:36:40 INFO 140644862207616] #quality_metric: host=algo-1, epoch=135, batch=10 train loss <loss>=3.41692533493042\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:36:40 INFO 140644862207616] Epoch[135] Batch [10]#011Speed: 317.84 samples/sec#011loss=3.416925\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:36:40 INFO 140644862207616] processed a total of 1297 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1639107395.4967704, \"EndTime\": 1639107400.7009532, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5203.822135925293, \"count\": 1, \"min\": 5203.822135925293, \"max\": 5203.822135925293}}}\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:36:40 INFO 140644862207616] #throughput_metric: host=algo-1, train throughput=249.23527724956665 records/second\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:36:40 INFO 140644862207616] #progress_metric: host=algo-1, completed 68.0 % of epochs\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:36:40 INFO 140644862207616] #quality_metric: host=algo-1, epoch=135, train loss <loss>=3.192569862712513\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:36:40 INFO 140644862207616] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:36:41 INFO 140644862207616] Epoch[136] Batch[0] avg_epoch_loss=3.029318\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:36:41 INFO 140644862207616] #quality_metric: host=algo-1, epoch=136, batch=0 train loss <loss>=3.029318332672119\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:36:43 INFO 140644862207616] Epoch[136] Batch[5] avg_epoch_loss=3.002533\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:36:43 INFO 140644862207616] #quality_metric: host=algo-1, epoch=136, batch=5 train loss <loss>=3.002533038457235\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:36:43 INFO 140644862207616] Epoch[136] Batch [5]#011Speed: 330.97 samples/sec#011loss=3.002533\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:36:45 INFO 140644862207616] Epoch[136] Batch[10] avg_epoch_loss=3.338679\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:36:45 INFO 140644862207616] #quality_metric: host=algo-1, epoch=136, batch=10 train loss <loss>=3.7420534610748293\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:36:45 INFO 140644862207616] Epoch[136] Batch [10]#011Speed: 324.02 samples/sec#011loss=3.742053\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:36:45 INFO 140644862207616] processed a total of 1293 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1639107400.7010198, \"EndTime\": 1639107405.8186724, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5117.218971252441, \"count\": 1, \"min\": 5117.218971252441, \"max\": 5117.218971252441}}}\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:36:45 INFO 140644862207616] #throughput_metric: host=algo-1, train throughput=252.67129360138034 records/second\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:36:45 INFO 140644862207616] #progress_metric: host=algo-1, completed 68.5 % of epochs\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:36:45 INFO 140644862207616] #quality_metric: host=algo-1, epoch=136, train loss <loss>=3.3386786851015957\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:36:45 INFO 140644862207616] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:36:47 INFO 140644862207616] Epoch[137] Batch[0] avg_epoch_loss=2.700975\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:36:47 INFO 140644862207616] #quality_metric: host=algo-1, epoch=137, batch=0 train loss <loss>=2.700975179672241\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:36:48 INFO 140644862207616] Epoch[137] Batch[5] avg_epoch_loss=2.970887\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:36:48 INFO 140644862207616] #quality_metric: host=algo-1, epoch=137, batch=5 train loss <loss>=2.9708871046702066\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:36:48 INFO 140644862207616] Epoch[137] Batch [5]#011Speed: 330.85 samples/sec#011loss=2.970887\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:36:51 INFO 140644862207616] Epoch[137] Batch[10] avg_epoch_loss=2.959926\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:36:51 INFO 140644862207616] #quality_metric: host=algo-1, epoch=137, batch=10 train loss <loss>=2.9467719554901124\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:36:51 INFO 140644862207616] Epoch[137] Batch [10]#011Speed: 317.74 samples/sec#011loss=2.946772\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:36:51 INFO 140644862207616] processed a total of 1316 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1639107405.8187408, \"EndTime\": 1639107411.0101187, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5191.012859344482, \"count\": 1, \"min\": 5191.012859344482, \"max\": 5191.012859344482}}}\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:36:51 INFO 140644862207616] #throughput_metric: host=algo-1, train throughput=253.5101819648515 records/second\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:36:51 INFO 140644862207616] #progress_metric: host=algo-1, completed 69.0 % of epochs\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:36:51 INFO 140644862207616] #quality_metric: host=algo-1, epoch=137, train loss <loss>=2.9599256732247095\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:36:51 INFO 140644862207616] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:36:52 INFO 140644862207616] Epoch[138] Batch[0] avg_epoch_loss=3.094720\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:36:52 INFO 140644862207616] #quality_metric: host=algo-1, epoch=138, batch=0 train loss <loss>=3.0947203636169434\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:36:54 INFO 140644862207616] Epoch[138] Batch[5] avg_epoch_loss=2.992779\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:36:54 INFO 140644862207616] #quality_metric: host=algo-1, epoch=138, batch=5 train loss <loss>=2.9927791357040405\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:36:54 INFO 140644862207616] Epoch[138] Batch [5]#011Speed: 330.31 samples/sec#011loss=2.992779\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:36:56 INFO 140644862207616] Epoch[138] Batch[10] avg_epoch_loss=3.072001\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:36:56 INFO 140644862207616] #quality_metric: host=algo-1, epoch=138, batch=10 train loss <loss>=3.1670673370361326\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:36:56 INFO 140644862207616] Epoch[138] Batch [10]#011Speed: 308.20 samples/sec#011loss=3.167067\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:36:56 INFO 140644862207616] processed a total of 1317 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1639107411.0101871, \"EndTime\": 1639107416.2797017, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5269.171237945557, \"count\": 1, \"min\": 5269.171237945557, \"max\": 5269.171237945557}}}\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:36:56 INFO 140644862207616] #throughput_metric: host=algo-1, train throughput=249.93963457646353 records/second\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:36:56 INFO 140644862207616] #progress_metric: host=algo-1, completed 69.5 % of epochs\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:36:56 INFO 140644862207616] #quality_metric: host=algo-1, epoch=138, train loss <loss>=3.0720010454004463\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:36:56 INFO 140644862207616] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:36:57 INFO 140644862207616] Epoch[139] Batch[0] avg_epoch_loss=2.834832\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:36:57 INFO 140644862207616] #quality_metric: host=algo-1, epoch=139, batch=0 train loss <loss>=2.834831953048706\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:36:59 INFO 140644862207616] Epoch[139] Batch[5] avg_epoch_loss=3.079494\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:36:59 INFO 140644862207616] #quality_metric: host=algo-1, epoch=139, batch=5 train loss <loss>=3.0794938802719116\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:36:59 INFO 140644862207616] Epoch[139] Batch [5]#011Speed: 330.54 samples/sec#011loss=3.079494\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:37:01 INFO 140644862207616] processed a total of 1264 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1639107416.2797732, \"EndTime\": 1639107421.034053, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4753.834009170532, \"count\": 1, \"min\": 4753.834009170532, \"max\": 4753.834009170532}}}\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:37:01 INFO 140644862207616] #throughput_metric: host=algo-1, train throughput=265.88331275867216 records/second\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:37:01 INFO 140644862207616] #progress_metric: host=algo-1, completed 70.0 % of epochs\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:37:01 INFO 140644862207616] #quality_metric: host=algo-1, epoch=139, train loss <loss>=3.013380193710327\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:37:01 INFO 140644862207616] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:37:02 INFO 140644862207616] Epoch[140] Batch[0] avg_epoch_loss=2.993211\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:37:02 INFO 140644862207616] #quality_metric: host=algo-1, epoch=140, batch=0 train loss <loss>=2.993211269378662\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:37:04 INFO 140644862207616] Epoch[140] Batch[5] avg_epoch_loss=3.035747\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:37:04 INFO 140644862207616] #quality_metric: host=algo-1, epoch=140, batch=5 train loss <loss>=3.0357468525568643\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:37:04 INFO 140644862207616] Epoch[140] Batch [5]#011Speed: 321.97 samples/sec#011loss=3.035747\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:37:06 INFO 140644862207616] processed a total of 1239 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1639107421.0341458, \"EndTime\": 1639107426.149796, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5115.169286727905, \"count\": 1, \"min\": 5115.169286727905, \"max\": 5115.169286727905}}}\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:37:06 INFO 140644862207616] #throughput_metric: host=algo-1, train throughput=242.21557433312495 records/second\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:37:06 INFO 140644862207616] #progress_metric: host=algo-1, completed 70.5 % of epochs\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:37:06 INFO 140644862207616] #quality_metric: host=algo-1, epoch=140, train loss <loss>=3.029146146774292\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:37:06 INFO 140644862207616] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:37:07 INFO 140644862207616] Epoch[141] Batch[0] avg_epoch_loss=2.933106\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:37:07 INFO 140644862207616] #quality_metric: host=algo-1, epoch=141, batch=0 train loss <loss>=2.9331064224243164\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:37:09 INFO 140644862207616] Epoch[141] Batch[5] avg_epoch_loss=3.067146\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:37:09 INFO 140644862207616] #quality_metric: host=algo-1, epoch=141, batch=5 train loss <loss>=3.0671459833780923\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:37:09 INFO 140644862207616] Epoch[141] Batch [5]#011Speed: 330.03 samples/sec#011loss=3.067146\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:37:10 INFO 140644862207616] processed a total of 1225 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1639107426.149872, \"EndTime\": 1639107430.9481845, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4797.934055328369, \"count\": 1, \"min\": 4797.934055328369, \"max\": 4797.934055328369}}}\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:37:10 INFO 140644862207616] #throughput_metric: host=algo-1, train throughput=255.3131741818585 records/second\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:37:10 INFO 140644862207616] #progress_metric: host=algo-1, completed 71.0 % of epochs\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:37:10 INFO 140644862207616] #quality_metric: host=algo-1, epoch=141, train loss <loss>=2.9193368911743165\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:37:10 INFO 140644862207616] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:37:12 INFO 140644862207616] Epoch[142] Batch[0] avg_epoch_loss=2.875507\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:37:12 INFO 140644862207616] #quality_metric: host=algo-1, epoch=142, batch=0 train loss <loss>=2.875507354736328\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:37:14 INFO 140644862207616] Epoch[142] Batch[5] avg_epoch_loss=2.760133\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:37:14 INFO 140644862207616] #quality_metric: host=algo-1, epoch=142, batch=5 train loss <loss>=2.7601331075032554\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:37:14 INFO 140644862207616] Epoch[142] Batch [5]#011Speed: 329.87 samples/sec#011loss=2.760133\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:37:15 INFO 140644862207616] processed a total of 1244 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1639107430.9482467, \"EndTime\": 1639107435.672412, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4723.706007003784, \"count\": 1, \"min\": 4723.706007003784, \"max\": 4723.706007003784}}}\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:37:15 INFO 140644862207616] #throughput_metric: host=algo-1, train throughput=263.3463645611868 records/second\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:37:15 INFO 140644862207616] #progress_metric: host=algo-1, completed 71.5 % of epochs\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:37:15 INFO 140644862207616] #quality_metric: host=algo-1, epoch=142, train loss <loss>=2.7678807735443116\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:37:15 INFO 140644862207616] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:37:16 INFO 140644862207616] Epoch[143] Batch[0] avg_epoch_loss=2.589182\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:37:16 INFO 140644862207616] #quality_metric: host=algo-1, epoch=143, batch=0 train loss <loss>=2.589181661605835\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:37:18 INFO 140644862207616] Epoch[143] Batch[5] avg_epoch_loss=2.615627\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:37:18 INFO 140644862207616] #quality_metric: host=algo-1, epoch=143, batch=5 train loss <loss>=2.61562712987264\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:37:18 INFO 140644862207616] Epoch[143] Batch [5]#011Speed: 328.52 samples/sec#011loss=2.615627\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:37:20 INFO 140644862207616] Epoch[143] Batch[10] avg_epoch_loss=2.791286\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:37:20 INFO 140644862207616] #quality_metric: host=algo-1, epoch=143, batch=10 train loss <loss>=3.002077007293701\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:37:20 INFO 140644862207616] Epoch[143] Batch [10]#011Speed: 312.33 samples/sec#011loss=3.002077\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:37:20 INFO 140644862207616] processed a total of 1335 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1639107435.6724885, \"EndTime\": 1639107440.8897426, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5216.864347457886, \"count\": 1, \"min\": 5216.864347457886, \"max\": 5216.864347457886}}}\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:37:20 INFO 140644862207616] #throughput_metric: host=algo-1, train throughput=255.8961212884718 records/second\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:37:20 INFO 140644862207616] #progress_metric: host=algo-1, completed 72.0 % of epochs\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:37:20 INFO 140644862207616] #quality_metric: host=algo-1, epoch=143, train loss <loss>=2.7912861650640313\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:37:20 INFO 140644862207616] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:37:22 INFO 140644862207616] Epoch[144] Batch[0] avg_epoch_loss=3.127941\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:37:22 INFO 140644862207616] #quality_metric: host=algo-1, epoch=144, batch=0 train loss <loss>=3.127941370010376\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:37:24 INFO 140644862207616] Epoch[144] Batch[5] avg_epoch_loss=3.028687\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:37:24 INFO 140644862207616] #quality_metric: host=algo-1, epoch=144, batch=5 train loss <loss>=3.028687000274658\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:37:24 INFO 140644862207616] Epoch[144] Batch [5]#011Speed: 327.85 samples/sec#011loss=3.028687\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:37:25 INFO 140644862207616] processed a total of 1246 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1639107440.8898094, \"EndTime\": 1639107445.6627913, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4772.599220275879, \"count\": 1, \"min\": 4772.599220275879, \"max\": 4772.599220275879}}}\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:37:25 INFO 140644862207616] #throughput_metric: host=algo-1, train throughput=261.06816762424745 records/second\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:37:25 INFO 140644862207616] #progress_metric: host=algo-1, completed 72.5 % of epochs\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:37:25 INFO 140644862207616] #quality_metric: host=algo-1, epoch=144, train loss <loss>=3.1070830106735228\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:37:25 INFO 140644862207616] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:37:26 INFO 140644862207616] Epoch[145] Batch[0] avg_epoch_loss=2.946172\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:37:26 INFO 140644862207616] #quality_metric: host=algo-1, epoch=145, batch=0 train loss <loss>=2.9461724758148193\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:37:28 INFO 140644862207616] Epoch[145] Batch[5] avg_epoch_loss=3.168560\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:37:28 INFO 140644862207616] #quality_metric: host=algo-1, epoch=145, batch=5 train loss <loss>=3.1685596704483032\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:37:28 INFO 140644862207616] Epoch[145] Batch [5]#011Speed: 329.39 samples/sec#011loss=3.168560\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:37:30 INFO 140644862207616] processed a total of 1266 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1639107445.6628606, \"EndTime\": 1639107450.393176, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4729.879856109619, \"count\": 1, \"min\": 4729.879856109619, \"max\": 4729.879856109619}}}\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:37:30 INFO 140644862207616] #throughput_metric: host=algo-1, train throughput=267.6533163499975 records/second\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:37:30 INFO 140644862207616] #progress_metric: host=algo-1, completed 73.0 % of epochs\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:37:30 INFO 140644862207616] #quality_metric: host=algo-1, epoch=145, train loss <loss>=3.117513346672058\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:37:30 INFO 140644862207616] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:37:31 INFO 140644862207616] Epoch[146] Batch[0] avg_epoch_loss=2.942955\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:37:31 INFO 140644862207616] #quality_metric: host=algo-1, epoch=146, batch=0 train loss <loss>=2.942955493927002\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:37:33 INFO 140644862207616] Epoch[146] Batch[5] avg_epoch_loss=3.074986\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:37:33 INFO 140644862207616] #quality_metric: host=algo-1, epoch=146, batch=5 train loss <loss>=3.074985901514689\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:37:33 INFO 140644862207616] Epoch[146] Batch [5]#011Speed: 328.39 samples/sec#011loss=3.074986\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:37:35 INFO 140644862207616] processed a total of 1280 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1639107450.393258, \"EndTime\": 1639107455.1173446, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4723.6926555633545, \"count\": 1, \"min\": 4723.6926555633545, \"max\": 4723.6926555633545}}}\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:37:35 INFO 140644862207616] #throughput_metric: host=algo-1, train throughput=270.96926247649145 records/second\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:37:35 INFO 140644862207616] #progress_metric: host=algo-1, completed 73.5 % of epochs\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:37:35 INFO 140644862207616] #quality_metric: host=algo-1, epoch=146, train loss <loss>=3.008519411087036\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:37:35 INFO 140644862207616] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:37:36 INFO 140644862207616] Epoch[147] Batch[0] avg_epoch_loss=2.903773\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:37:36 INFO 140644862207616] #quality_metric: host=algo-1, epoch=147, batch=0 train loss <loss>=2.903773307800293\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:37:38 INFO 140644862207616] Epoch[147] Batch[5] avg_epoch_loss=2.982198\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:37:38 INFO 140644862207616] #quality_metric: host=algo-1, epoch=147, batch=5 train loss <loss>=2.9821982781092324\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:37:38 INFO 140644862207616] Epoch[147] Batch [5]#011Speed: 335.27 samples/sec#011loss=2.982198\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:37:40 INFO 140644862207616] Epoch[147] Batch[10] avg_epoch_loss=2.956362\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:37:40 INFO 140644862207616] #quality_metric: host=algo-1, epoch=147, batch=10 train loss <loss>=2.9253583431243895\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:37:40 INFO 140644862207616] Epoch[147] Batch [10]#011Speed: 313.46 samples/sec#011loss=2.925358\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:37:40 INFO 140644862207616] processed a total of 1327 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1639107455.1174088, \"EndTime\": 1639107460.2833354, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5165.530443191528, \"count\": 1, \"min\": 5165.530443191528, \"max\": 5165.530443191528}}}\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:37:40 INFO 140644862207616] #throughput_metric: host=algo-1, train throughput=256.89034323715515 records/second\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:37:40 INFO 140644862207616] #progress_metric: host=algo-1, completed 74.0 % of epochs\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:37:40 INFO 140644862207616] #quality_metric: host=algo-1, epoch=147, train loss <loss>=2.956361944025213\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:37:40 INFO 140644862207616] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:37:41 INFO 140644862207616] Epoch[148] Batch[0] avg_epoch_loss=2.982017\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:37:41 INFO 140644862207616] #quality_metric: host=algo-1, epoch=148, batch=0 train loss <loss>=2.9820165634155273\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:37:43 INFO 140644862207616] Epoch[148] Batch[5] avg_epoch_loss=3.037598\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:37:43 INFO 140644862207616] #quality_metric: host=algo-1, epoch=148, batch=5 train loss <loss>=3.03759757677714\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:37:43 INFO 140644862207616] Epoch[148] Batch [5]#011Speed: 334.12 samples/sec#011loss=3.037598\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:37:44 INFO 140644862207616] processed a total of 1277 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1639107460.2834013, \"EndTime\": 1639107464.977922, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4694.124698638916, \"count\": 1, \"min\": 4694.124698638916, \"max\": 4694.124698638916}}}\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:37:44 INFO 140644862207616] #throughput_metric: host=algo-1, train throughput=272.0363394604401 records/second\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:37:44 INFO 140644862207616] #progress_metric: host=algo-1, completed 74.5 % of epochs\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:37:44 INFO 140644862207616] #quality_metric: host=algo-1, epoch=148, train loss <loss>=3.0956330060958863\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:37:44 INFO 140644862207616] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:37:46 INFO 140644862207616] Epoch[149] Batch[0] avg_epoch_loss=2.985615\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:37:46 INFO 140644862207616] #quality_metric: host=algo-1, epoch=149, batch=0 train loss <loss>=2.9856150150299072\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:37:48 INFO 140644862207616] Epoch[149] Batch[5] avg_epoch_loss=2.857872\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:37:48 INFO 140644862207616] #quality_metric: host=algo-1, epoch=149, batch=5 train loss <loss>=2.857871929804484\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:37:48 INFO 140644862207616] Epoch[149] Batch [5]#011Speed: 335.31 samples/sec#011loss=2.857872\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:37:50 INFO 140644862207616] Epoch[149] Batch[10] avg_epoch_loss=2.768114\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:37:50 INFO 140644862207616] #quality_metric: host=algo-1, epoch=149, batch=10 train loss <loss>=2.66040358543396\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:37:50 INFO 140644862207616] Epoch[149] Batch [10]#011Speed: 319.17 samples/sec#011loss=2.660404\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:37:50 INFO 140644862207616] processed a total of 1343 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1639107464.9779906, \"EndTime\": 1639107470.0950737, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5116.666078567505, \"count\": 1, \"min\": 5116.666078567505, \"max\": 5116.666078567505}}}\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:37:50 INFO 140644862207616] #throughput_metric: host=algo-1, train throughput=262.47042682830227 records/second\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:37:50 INFO 140644862207616] #progress_metric: host=algo-1, completed 75.0 % of epochs\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:37:50 INFO 140644862207616] #quality_metric: host=algo-1, epoch=149, train loss <loss>=2.7681135914542456\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:37:50 INFO 140644862207616] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:37:51 INFO 140644862207616] Epoch[150] Batch[0] avg_epoch_loss=2.961574\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:37:51 INFO 140644862207616] #quality_metric: host=algo-1, epoch=150, batch=0 train loss <loss>=2.961573839187622\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:37:53 INFO 140644862207616] Epoch[150] Batch[5] avg_epoch_loss=2.934654\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:37:53 INFO 140644862207616] #quality_metric: host=algo-1, epoch=150, batch=5 train loss <loss>=2.9346538384755454\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:37:53 INFO 140644862207616] Epoch[150] Batch [5]#011Speed: 314.21 samples/sec#011loss=2.934654\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:37:54 INFO 140644862207616] processed a total of 1228 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1639107470.0951421, \"EndTime\": 1639107474.9247792, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4829.286336898804, \"count\": 1, \"min\": 4829.286336898804, \"max\": 4829.286336898804}}}\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:37:54 INFO 140644862207616] #throughput_metric: host=algo-1, train throughput=254.2760910420619 records/second\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:37:54 INFO 140644862207616] #progress_metric: host=algo-1, completed 75.5 % of epochs\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:37:54 INFO 140644862207616] #quality_metric: host=algo-1, epoch=150, train loss <loss>=2.9110851287841797\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:37:54 INFO 140644862207616] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:37:56 INFO 140644862207616] Epoch[151] Batch[0] avg_epoch_loss=2.872623\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:37:56 INFO 140644862207616] #quality_metric: host=algo-1, epoch=151, batch=0 train loss <loss>=2.8726232051849365\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:37:58 INFO 140644862207616] Epoch[151] Batch[5] avg_epoch_loss=2.960506\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:37:58 INFO 140644862207616] #quality_metric: host=algo-1, epoch=151, batch=5 train loss <loss>=2.9605058431625366\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:37:58 INFO 140644862207616] Epoch[151] Batch [5]#011Speed: 331.03 samples/sec#011loss=2.960506\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:37:59 INFO 140644862207616] processed a total of 1250 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1639107474.9248552, \"EndTime\": 1639107479.6681862, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4742.922306060791, \"count\": 1, \"min\": 4742.922306060791, \"max\": 4742.922306060791}}}\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:37:59 INFO 140644862207616] #throughput_metric: host=algo-1, train throughput=263.54325656641373 records/second\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:37:59 INFO 140644862207616] #progress_metric: host=algo-1, completed 76.0 % of epochs\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:37:59 INFO 140644862207616] #quality_metric: host=algo-1, epoch=151, train loss <loss>=2.9284998178482056\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:37:59 INFO 140644862207616] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:38:00 INFO 140644862207616] Epoch[152] Batch[0] avg_epoch_loss=2.679168\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:38:00 INFO 140644862207616] #quality_metric: host=algo-1, epoch=152, batch=0 train loss <loss>=2.679168224334717\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:38:02 INFO 140644862207616] Epoch[152] Batch[5] avg_epoch_loss=2.970692\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:38:02 INFO 140644862207616] #quality_metric: host=algo-1, epoch=152, batch=5 train loss <loss>=2.9706920782725015\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:38:03 INFO 140644862207616] Epoch[152] Batch [5]#011Speed: 311.58 samples/sec#011loss=2.970692\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:38:05 INFO 140644862207616] Epoch[152] Batch[10] avg_epoch_loss=2.931147\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:38:05 INFO 140644862207616] #quality_metric: host=algo-1, epoch=152, batch=10 train loss <loss>=2.8836931705474855\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:38:05 INFO 140644862207616] Epoch[152] Batch [10]#011Speed: 294.48 samples/sec#011loss=2.883693\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:38:05 INFO 140644862207616] processed a total of 1326 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1639107479.6682642, \"EndTime\": 1639107485.1737032, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 5505.024433135986, \"count\": 1, \"min\": 5505.024433135986, \"max\": 5505.024433135986}}}\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:38:05 INFO 140644862207616] #throughput_metric: host=algo-1, train throughput=240.8665154940656 records/second\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:38:05 INFO 140644862207616] #progress_metric: host=algo-1, completed 76.5 % of epochs\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:38:05 INFO 140644862207616] #quality_metric: host=algo-1, epoch=152, train loss <loss>=2.9311471202156762\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:38:05 INFO 140644862207616] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:38:06 INFO 140644862207616] Epoch[153] Batch[0] avg_epoch_loss=3.039109\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:38:06 INFO 140644862207616] #quality_metric: host=algo-1, epoch=153, batch=0 train loss <loss>=3.039109230041504\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:38:08 INFO 140644862207616] Epoch[153] Batch[5] avg_epoch_loss=2.891587\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:38:08 INFO 140644862207616] #quality_metric: host=algo-1, epoch=153, batch=5 train loss <loss>=2.8915865421295166\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:38:08 INFO 140644862207616] Epoch[153] Batch [5]#011Speed: 328.50 samples/sec#011loss=2.891587\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:38:10 INFO 140644862207616] processed a total of 1272 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1639107485.173773, \"EndTime\": 1639107490.087863, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 4913.742780685425, \"count\": 1, \"min\": 4913.742780685425, \"max\": 4913.742780685425}}}\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:38:10 INFO 140644862207616] #throughput_metric: host=algo-1, train throughput=258.8601066899691 records/second\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:38:10 INFO 140644862207616] #progress_metric: host=algo-1, completed 77.0 % of epochs\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:38:10 INFO 140644862207616] #quality_metric: host=algo-1, epoch=153, train loss <loss>=2.8998151302337645\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:38:10 INFO 140644862207616] loss did not improve\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:38:10 INFO 140644862207616] Loading parameters from best epoch (113)\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1639107490.0879393, \"EndTime\": 1639107490.1175685, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.deserialize.time\": {\"sum\": 29.155254364013672, \"count\": 1, \"min\": 29.155254364013672, \"max\": 29.155254364013672}}}\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:38:10 INFO 140644862207616] stopping training now\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:38:10 INFO 140644862207616] #progress_metric: host=algo-1, completed 100 % of epochs\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:38:10 INFO 140644862207616] Final loss: 2.533401695164767 (occurred at epoch 113)\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:38:10 INFO 140644862207616] #quality_metric: host=algo-1, train final_loss <loss>=2.533401695164767\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:38:10 INFO 140644862207616] Worker algo-1 finished training.\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:38:10 WARNING 140644862207616] wait_for_all_workers will not sync workers since the kv store is not running distributed\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:38:10 INFO 140644862207616] All workers finished. Serializing model for prediction.\u001b[0m\n",
      "\n",
      "2021-12-10 03:38:21 Uploading - Uploading generated training model\u001b[34m#metrics {\"StartTime\": 1639107490.1176486, \"EndTime\": 1639107490.6633017, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"get_graph.time\": {\"sum\": 545.0174808502197, \"count\": 1, \"min\": 545.0174808502197, \"max\": 545.0174808502197}}}\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:38:10 INFO 140644862207616] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1639107490.6633894, \"EndTime\": 1639107490.8685677, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"finalize.time\": {\"sum\": 750.3211498260498, \"count\": 1, \"min\": 750.3211498260498, \"max\": 750.3211498260498}}}\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:38:10 INFO 140644862207616] Serializing to /opt/ml/model/model_algo-1\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:38:10 INFO 140644862207616] Saved checkpoint to \"/opt/ml/model/model_algo-1-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1639107490.8686314, \"EndTime\": 1639107490.8999434, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"model.serialize.time\": {\"sum\": 31.275033950805664, \"count\": 1, \"min\": 31.275033950805664, \"max\": 31.275033950805664}}}\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:38:10 INFO 140644862207616] Successfully serialized the model for prediction.\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:38:10 INFO 140644862207616] #memory_usage::<batchbuffer> = 49.627723693847656 mb\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:38:10 INFO 140644862207616] Evaluating model accuracy on testset using 100 samples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1639107490.8999977, \"EndTime\": 1639107490.9008427, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"model.bind.time\": {\"sum\": 0.027418136596679688, \"count\": 1, \"min\": 0.027418136596679688, \"max\": 0.027418136596679688}}}\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1639107490.900894, \"EndTime\": 1639107497.3175647, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"model.score.time\": {\"sum\": 6416.742324829102, \"count\": 1, \"min\": 6416.742324829102, \"max\": 6416.742324829102}}}\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:38:17 INFO 140644862207616] #test_score (algo-1, RMSE): 119.8892999161889\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:38:17 INFO 140644862207616] #test_score (algo-1, mean_absolute_QuantileLoss): 99548.56127726237\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:38:17 INFO 140644862207616] #test_score (algo-1, mean_wQuantileLoss): 0.09804903608772522\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:38:17 INFO 140644862207616] #test_score (algo-1, wQuantileLoss[0.1]): 0.05365336126904226\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:38:17 INFO 140644862207616] #test_score (algo-1, wQuantileLoss[0.2]): 0.07962518894969078\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:38:17 INFO 140644862207616] #test_score (algo-1, wQuantileLoss[0.3]): 0.0978341207619879\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:38:17 INFO 140644862207616] #test_score (algo-1, wQuantileLoss[0.4]): 0.1111665602193352\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:38:17 INFO 140644862207616] #test_score (algo-1, wQuantileLoss[0.5]): 0.11853464654778847\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:38:17 INFO 140644862207616] #test_score (algo-1, wQuantileLoss[0.6]): 0.12025201385250629\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:38:17 INFO 140644862207616] #test_score (algo-1, wQuantileLoss[0.7]): 0.11594748226855238\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:38:17 INFO 140644862207616] #test_score (algo-1, wQuantileLoss[0.8]): 0.10452125436666031\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:38:17 INFO 140644862207616] #test_score (algo-1, wQuantileLoss[0.9]): 0.08090669655396347\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:38:17 INFO 140644862207616] #quality_metric: host=algo-1, test RMSE <loss>=119.8892999161889\u001b[0m\n",
      "\u001b[34m[12/10/2021 03:38:17 INFO 140644862207616] #quality_metric: host=algo-1, test mean_wQuantileLoss <loss>=0.09804903608772522\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1639107497.3176534, \"EndTime\": 1639107497.3724382, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"setuptime\": {\"sum\": 5.867719650268555, \"count\": 1, \"min\": 5.867719650268555, \"max\": 5.867719650268555}, \"totaltime\": {\"sum\": 778335.7126712799, \"count\": 1, \"min\": 778335.7126712799, \"max\": 778335.7126712799}}}\u001b[0m\n",
      "\n",
      "2021-12-10 03:38:28 Completed - Training job completed\n",
      "Training seconds: 867\n",
      "Billable seconds: 867\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "estimator.fit(inputs=data_channels, wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a1c0bb6",
   "metadata": {},
   "source": [
    "### 模型部署\n",
    "\n",
    "同样的，在模型训练成功后，我们需要将训练的结果部署到 SageMaker 终端节点上进行使用。首先我们定义一个 DeepAR Predictor 类："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f442a464",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepARPredictor(sagemaker.predictor.RealTimePredictor):\n",
    "    \n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, content_type=sagemaker.content_types.CONTENT_TYPE_JSON, **kwargs)\n",
    "        \n",
    "    def predict(self, ts, prediction_index, cat=None, dynamic_feat=None, \n",
    "                num_samples=100, return_samples=False, quantiles=[\"0.1\", \"0.5\", \"0.9\"]):\n",
    "        \"\"\"Requests the prediction of for the time series listed in `ts`, each with the (optional)\n",
    "        corresponding category listed in `cat`.\n",
    "        \n",
    "        ts -- `pandas.Series` object, the time series to predict\n",
    "        prediction_index -- list, time of the prediction steps\n",
    "        cat -- integer, the group associated to the time series (default: None)\n",
    "        num_samples -- integer, number of samples to compute at prediction time (default: 100)\n",
    "        return_samples -- boolean indicating whether to include samples in the response (default: False)\n",
    "        quantiles -- list of strings specifying the quantiles to compute (default: [\"0.1\", \"0.5\", \"0.9\"])\n",
    "        \n",
    "        Return value: list of `pandas.DataFrame` objects, each containing the predictions\n",
    "        \"\"\"\n",
    "        quantiles = [str(q) for q in quantiles]\n",
    "        req = self.__encode_request(ts, cat, dynamic_feat, num_samples, return_samples, quantiles)\n",
    "        res = super(DeepARPredictor, self).predict(req)\n",
    "        return self.__decode_response(res, prediction_index, return_samples)\n",
    "    \n",
    "    def __encode_request(self, ts, cat, dynamic_feat, num_samples, return_samples, quantiles):\n",
    "        \n",
    "        instance = series_to_dict(ts, cat if cat is not None else None, dynamic_feat if dynamic_feat else None)\n",
    "        configuration = {\n",
    "            \"num_samples\": num_samples,\n",
    "            \"output_types\": [\"quantiles\", \"samples\"] if return_samples else [\"quantiles\"],\n",
    "            \"quantiles\": quantiles\n",
    "        }\n",
    "        \n",
    "        http_request_data = {\n",
    "            \"instances\": [instance],\n",
    "            \"configuration\": configuration\n",
    "        }\n",
    "        \n",
    "        return json.dumps(http_request_data).encode('utf-8')\n",
    "    \n",
    "    def __decode_response(self, response, prediction_index, return_samples):\n",
    "        # we only sent one time series so we only receive one in return\n",
    "        # however, if possible one will pass multiple time series as predictions will then be faster\n",
    "        predictions = json.loads(response.decode('utf-8'))['predictions'][0]\n",
    "        if return_samples:\n",
    "            dict_of_samples = {'sample_' + str(i): s for i, s in enumerate(predictions['samples'])}\n",
    "        else:\n",
    "            dict_of_samples = {}\n",
    "        return pd.DataFrame(data={**predictions['quantiles'], **dict_of_samples}, index=prediction_index)\n",
    "\n",
    "    def set_frequency(self, freq):\n",
    "        self.freq = freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6694f526",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter image will be renamed to image_uri in SageMaker Python SDK v2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------!Pedictor attached to Endpoint: sagemaker-deepar-2021-12-10-03-21-36-689\n"
     ]
    }
   ],
   "source": [
    "predictor = estimator.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type='ml.m5.large',\n",
    "    predictor_cls=DeepARPredictor)\n",
    "print(\"Pedictor attached to Endpoint: {}\".format(predictor.endpoint))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d3c01dc8",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'predictor' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-223bc58bbad9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Pedictor attached to Endpoint: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendpoint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'predictor' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"Pedictor attached to Endpoint: {}\".format(predictor.endpoint))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41815952",
   "metadata": {},
   "source": [
    "### 使用 DeepAR 进行推理\n",
    "\n",
    "部署成功后，我们可以直接运行以下的代码查看模型预测的结果："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a999f060",
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator_job = estimator.latest_training_job.job_name\n",
    "predictor = DeepARPredictor(estimator_job)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "03f29d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def series_to_dict(ts, cat=None, dynamic_feat=None):\n",
    "    \"\"\"Given a pandas.Series object, returns a dictionary encoding the time series.\n",
    "    ts -- a pands.Series object with the target time series\n",
    "    cat -- an integer indicating the time series category\n",
    "    Return value: a dictionary\n",
    "    \"\"\"\n",
    "    obj = {\"start\": str(ts.index[0]), \"target\": [x if np.isfinite(x) else \"NaN\" for x in ts]}\n",
    "    \n",
    "    if cat is not None:\n",
    "        obj[\"cat\"] = cat\n",
    "    if dynamic_feat is not None:\n",
    "        obj[\"dynamic_feat\"] = dynamic_feat        \n",
    "        \n",
    "    return obj\n",
    "\n",
    "\n",
    "def query_for_stock(stock_to_predict, target_column, covariate_columns, data, prediction_length, start = None, end = None):\n",
    "    \n",
    "    if start is None:\n",
    "        start = data.index.values[0]\n",
    "    if end is None:\n",
    "        end = data.index.values[-1]\n",
    "    startloc = data.index.get_loc(start)\n",
    "    endloc = data.index.get_loc(end)\n",
    "    stockts = None\n",
    "    ts = None\n",
    "    dynamic_feat = []\n",
    "    \n",
    "    for i, col in enumerate(data.columns):\n",
    "        stock = col[:col.find('-')]\n",
    "        metric = col[col.find('-')+1:]\n",
    "        if stock == stock_to_predict: \n",
    "            if metric == target_column:\n",
    "                ts = data.iloc[:,i][startloc:endloc-prediction_length]\n",
    "                stockts = data.iloc[:,i][:]\n",
    "                print(\"Time series - {} for {} selected\".format(metric, stock))\n",
    "            elif metric in covariate_columns:\n",
    "                dynamic_feat.append(data.iloc[:,i][startloc:endloc].tolist())\n",
    "                print(\"Dynamic Feature - {} for {} selected\".format(metric, stock))\n",
    "            else:\n",
    "                pass\n",
    "            \n",
    "    return ts, dynamic_feat, stockts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "01247753",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time series - close for 600519 selected\n",
      "Dynamic Feature - high for 600519 selected\n",
      "Dynamic Feature - low for 600519 selected\n",
      "Dynamic Feature - open for 600519 selected\n",
      "Historical times: 2017-05-26 - 2021-02-18 total length 908\n",
      "Prediction times: 2021-02-19 - 2021-07-02 total length 91\n"
     ]
    }
   ],
   "source": [
    "ts, dynamic_feat, observed = query_for_stock('600519', target_column, covariate_columns, df, prediction_length)\n",
    "print('Historical times:', ts.index[0], '-', ts.index[-1], 'total length', len(ts.index))\n",
    "prediction_index = df.index.tolist()\n",
    "prediction_start = prediction_index.index(ts.index[-1]) + 1\n",
    "prediction_end = prediction_index.index(ts.index[-1]) + 1 + prediction_length\n",
    "prediction_index = prediction_index[prediction_start:prediction_end]\n",
    "print('Prediction times:', prediction_index[0], '-', prediction_index[-1], 'total length', len(prediction_index))\n",
    "\n",
    "quantiles = [0.10, 0.50, 0.90]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b05d892b",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = predictor.predict(ts, prediction_index, dynamic_feat=dynamic_feat, quantiles=quantiles, return_samples=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5efb9016",
   "metadata": {},
   "source": [
    "可以通过画图对预测结果进行可视化。与普通 RNN 模型不同的是，DeepAR 产生的结果是基于概率的预测。图中的阴影部分代表 10% 和 90% quantile 覆盖的范围："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ba0fa8e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "\n",
    "matplotlib.rcParams['figure.figsize'] = (25, 17)\n",
    "\n",
    "def plot_predicted_observed_at_quantile(ts, observed, prediction, quantile, upper, lower):\n",
    "    \n",
    "    ax = None\n",
    "    ax = observed.plot(ax=ax, legend=True, label=\"Given\" )\n",
    "    predicted = ts.append(prediction['0.5'])\n",
    "    predicted.plot(ax=ax, legend=True, label=\"Predicted\")\n",
    "    predicted_upper = np.array(ts.append(prediction[upper]))\n",
    "    predicted_lower = np.array(ts.append(prediction[lower]))\n",
    "    x = range(0, len(predicted))\n",
    "    ax.fill_between(x, predicted_upper, predicted_lower, where=predicted_upper>predicted_lower, interpolate=True, color='lavender')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c88b9a8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_predicted_observed_at_quantile(ts, observed, prediction, quantile='0.5', upper='0.9', lower='0.1')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6d0fadb",
   "metadata": {},
   "source": [
    "在终端节点不再使用后一定将其删除，否则会一直产生费用："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "52d0631a",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ace6f9e6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
